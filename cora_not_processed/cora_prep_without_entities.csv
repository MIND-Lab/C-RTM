heuristic discovering protein sequence patterns several computer algorithms discovering patterns groups protein sequences use based fitting parameters statistical model group related sequences include hidden markov model hmm algorithms multiple sequence alignment gibbs sampler algorithms discovering algorithms sometimes producing models incorrect two patterns combined statistical model produced situation convex combination weighted average two different models paper presents solution problem convex combinations form heuristic based using extremely low variance dirichlet mixture priors part statistical model heuristic call heuristic increases strength decreases variance prior size sequence dataset causes final model strongly mean single component prior regardless size dataset describe cause convex combination problem analyze mathematically describe implementation heuristic show effectively eliminate problem convex combinations protein sequence pattern discovery
applications machine learning medical follow study paper describes preliminary work apply learning strategies medical study investigation application three machine learning foil identify risk factors rate made goal study induce description explanation classification attribute rate completely improved worse examples set rules described risk factors result differences rate results encouraging indicate machine learning play useful role large scale medical problem solving
submitted section applications preference presentation reinforcement learning dynamic channel allocation cellular systems important problem dynamically communication resource channels maximize service stochastic environment problem naturally formulated dynamic programming problem use reinforcement learning method find dynamic channel allocation policies better previous heuristic solutions policies obtained perform broad variety call traffic patterns present results large cellular system cellular communication systems important problem communication resource maximize service provided set mobile whose service changes stochastically given area cells cell calls within boundaries see figure total system channels channel around frequency channel used simultaneously different cells provided cells sufficiently separated spatially interference minimum separation distance simultaneous reuse channel called channel reuse constraint call service given cell either free channel one channel reuse constraint may assigned call call system will happen free channel found also mobile one cell another call cell new free channel provided call new cell channel available call must system one objective channel allocation policy available channels calls number calls additional objective minimize number calls cell two objectives must weighted appropriately reflect relative importance since existing calls generally new calls approximately states
planning partially observable stochastic domains problem robot paper techniques operations research bear problem choosing optimal actions partially observable stochastic domains begin introducing theory markov decision processes mdps partially observable mdps pomdps outline novel algorithm solving pomdps line show cases controller extracted solution conclude discussion approach previous work complexity finding exact solutions pomdps possibilities finding approximate solutions
variational methods inference estimation graphical models variational methodology probabilistic inference bayesian graphical models enhance representational power probability models qualitative characterization properties also leads greater efficiency terms computational algorithms representations increasing complexity models however quickly exact probabilistic calculations infeasible propose principled framework approximating graphical models based variational methods develop variational techniques perspective applicability graphical models methods allow recursive computation upper lower bounds quantities interest bounds yield considerably information approximations provide inherent error metric approximations cases considered desirable properties variational methods arise result deterministic stochastic approximations
experiments realtime decision algorithms realtime decision algorithms class incremental algorithms evaluating influence diagrams present test domain realtime decision algorithms results experiments several realtime decision algorithms domain results demonstrate high performance two algorithms variant incremental inference variant algorithm suggested discuss implications experimental results explore applicability algorithms
formal framework speedup learning problems solutions speedup learning seeks improve computational efficiency problem solving experience paper develop formal framework learning efficient problem solving random problems solutions apply framework two different representations learned knowledge namely control rules prove theorems identify sufficient conditions learning representation constructive learning algorithms framework captures empirical explanationbased speedup learning unified fashion illustrate framework implementations two domains symbolic integration work integrates many experimental theoretical work machine learning including empirical learning control rules learning
optimal linear space using cost functions extended abstract submitted previous paper showed finite automata used define objective functions quality alignment two sequences paper show results using cost functions also show extend linear space algorithm setting thus generalizing result
hidden markov models protein families
online learning versus learning present offline variant model learning just like studied online model learner offline model learn unknown concept sequence elements instance space makes test trials models aim learner make mistakes possible difference models online model set possible elements known offline model sequence elements elements order presented known learner advance give combinatorial characterization number mistakes offline model apply characterization solve several natural questions arise new model first compare bounds offline learner learner learning concept classes online show number mistakes online learning log factor offline learning length sequence addition show offline algorithm make constant number mistakes sequence online algorithm also make constant number mistakes second issue address effect ordering elements number mistakes offline learner turns sequences offline learner guarantee one yet permutation sequence forces many elements prove however gap offline bounds sequence elements larger multiplicative factor log present examples obtain gap
fitting smoothing spline anova models exponential families
genetic programming incremental approaches solve supervised learning problems paper presents evolutionary approach incremental approach find learning rules several supervised learning tasks evolutionary approach potential solutions represented variable length mathematical expressions thus similar genetic programming employs fixed set specific functions solve variety problems model tested three parity problems results indicate usefulness encoding schema discovering learning rules simple supervised learning problems however hard learning problems require special attention terms need larger size potential solutions ability generalisation testing set order find better solutions issues hill climbing strategy incremental coding potential solutions used discovering learning rules problems found strategy larger solutions easily coded less computational effort although better performance achieved training hard learning problems ability generalisation testing cases observed poor
estimating bayes factors via posterior simulation estimator key needed bayesian hypothesis testing model selection marginal likelihood model also known integrated likelihood marginal probability data paper describe way use posterior simulation output estimate marginal describe basic estimator models without random effects models random effects estimator introduced estimator applied data world survey shown give accurate results simulation output used assess uncertainty involved using estimator method allows test effects independent variables random effects model also test presence random effects
unifying empirical explanationbased learning modeling utility learned knowledge problem empirical learning utility problem explanationbased learning describe similar phenomenon performance due increase amount learned knowledge performance learned knowledge course learning performance response reveals common several learning methods modeling allows control system amount learned knowledge achieve peak performance avoid general utility problem experiments evaluate particular empirical model analysis learners derive several formal models evidence suggests general utility problem modeled using mechanisms different learning paradigms model paradigms one framework capable comparing selecting different learning methods based predicted performance
hidden markov models computational biology applications protein modeling keywords hidden markov models hidden markov models hmms applied problems statistical modeling database searching multiple sequence alignment protein families protein domains methods demonstrated family protein domain binding case parameters hmm estimated training set sequences hmm built used obtain multiple alignment training sequences also used search database sequences members given protein family contain given domain hmm produces multiple good quality agree closely produced programs incorporate structural information employed discrimination tests examining closely sequences database fit hmms hmm able distinguish members families high degree accuracy hmm technique used search relationships protein sequence sequences perform better tests sites patterns proteins hmm appears advantage
back propagation sensitive initial conditions paper explores effect initial weight selection feedforward networks learning simple functions backpropagation technique first demonstrate use monte carlo techniques magnitude initial condition vector weight space significant parameter convergence time variability order understand result additional deterministic experiments performed results experiments demonstrate sensitivity back propagation initial weight configuration
exploration active learning
neural network model memory forms memory rely system brain structures temporal includes recall recent events one task relies proper system event becomes less recent temporal becomes less critical recall event appears rely upon proposed process called responsible transfer memory temporal examine network model proposed designed incorporate known features propose several possible experiments intended help evaluate performance model realistic conditions finally implement extended version model accommodate varying assumptions number areas connections within brain memory capacity examine performance model original task
ocular dominance model exploring positive correlations map eye brain topographic points eye map points brain addition two eyes target structure two sets form ocular dominance experimental evidence suggests two phenomena may mechanisms present computational model addresses formation ocular dominance model based form competitive learning weight normalization rule inputs model distributed patterns activity presented simultaneously eyes important aspect model ocular dominance occur two eyes correlated whereas previous models assume zero negative correlations eyes allows investigation dependence pattern degree correlation eyes find increasing correlation leads experiments suggested test prediction
validation average error rate classifiers examine methods estimate average variance test error rates set classifiers begin process classifier random example given validation data average test error rate estimated single classifier given test example inputs variance computed exactly next consider process classifier random using examples expected test error rate single classifier however variance must estimated yields uncertain bounds
learning hybrid noise environments using statistical queries consider formal models learning noisy data specifically focus learning probability approximately correct model defined two widely studied models noise setting classification noise malicious errors however realistic model combining two types noise define learning environment based natural combination two noise models first show hypothesis testing possible model next describe simple technique learning model describe powerful technique based statistical query learning show noise improved technique roughly optimal respect desired learning accuracy provides smooth tradeoff amounts two types noise finally show statistical query simulation yields learning algorithms combinations noise models thus demonstrating statistical query specification important goal research machine learning determine tasks automated determine information computation requirements one way answer questions development investigation formal models machine learning capture task learning plausible assumptions work consider formal model learning examples called probably approximately correct pac learning defined setting learner attempts approximate unknown target concept simply positive negative examples concept adversary specified function class hidden target function defined specified domain examples probability distribution domain goal learner output polynomial time high probability hypothesis close target function respect distribution examples learner gains information target function distribution interacting example oracle learner oracle example randomly according hidden distribution labels according hidden target function returns example learner class functions pac learnable captures generic fault learning algorithm
decision tree function approximation reinforcement learning present decision tree based approach function approximation reinforcement learning compare approach table neural network function approximator three problems known car pole balance problems simulated car find decision tree provide better learning performance neural network function approximation solve large problems infeasible using table
discovering complex strategies evolutionary neural networks approach develop new game playing strategies based artificial evolution neural networks presented evolution directed discover strategies opponent later search program networks discovered first standard strategy subsequently strategy advanced strategy seen latter discovery demonstrates evolutionary neural networks develop novel solutions initial advantage changed environment
applications extensions mcmc multiple item types missing data responses technical report december
role transfer learning extended abstract
general bounds statistical query learning pac learning noise via hypothesis boosting derive general bounds complexity learning statistical query model pac model classification noise considering problem boosting accuracy weak learning algorithms fall within statistical query model new model introduced provide general framework efficient pac learning presence classification noise first show general scheme boosting accuracy weak learning algorithms proving weak learning equivalent strong learning boosting efficient used show main result first general upper bounds complexity strong learning specifically derive simultaneous upper bounds respect number queries olog vapnikchervonenkis dimension query space olog inverse minimum log addition show general upper bounds nearly optimal describing class learning problems simultaneously lower bound number queries log apply boosting results model learning pac model classification noise since nearly pac learning algorithms model apply boosting techniques pac algorithms highly efficient algorithms simulating efficient algorithms pac model classification noise show nearly pac algorithms highly efficient pac algorithms tolerate classification noise give upper bound sample complexity noisetolerant pac algorithms nearly optimal respect noise rate also give upper bounds space complexity hypothesis size show two measures fact independent noise rate note running times noisetolerant pac algorithms efficient sequence simulations also demonstrates possible accuracy nearly pac algorithms even presence noise provides partial answer open problem schapire first theoretical evidence empirical result schapire
neural network applicability classifying problem space current effort propose inspired methods computation forces real world application potential models paper applications classes particularly discusses features applications make efficiently neural network methods computational machines deterministic mappings inputs outputs many computational mechanisms proposed problem solutions neural network features include parallel execution adaptive learning generalization fault often much effort given model applications already implemented much efficient way alternate technology neural networks potentially powerful devices many classes applications however proposed class applications neural networks efficient large commonly occurring nature comparison supervised unsupervised generalizing systems also included
formal rules selecting prior distributions review become foundation bayesian inference yet practice bayesian analyses performed socalled priors priors constructed formal rule review techniques constructing priors discuss practical issues arise used give special emphasis rules discuss evolution point view interpretation priors away unique representation toward notion chosen conclude problems raised research priors chosen formal rules serious may sample sizes small relative number parameters estimated put default solution take rules variants reasonable choices also provide professor associate professor department statistics university work authors supported nsf grant grant authors thank useful comments discussion
damage model selection algorithm neural networks recurrent neural networks become popular models system identification time series prediction nonlinear autoregressive models inputs neural network models popular subclass recurrent networks used many applications though embedded memory found recurrent network models particularly models show using intelligent memory order selection pruning good initial heuristics significantly improves generalization predictive performance nonlinear systems problems diverse grammatical inference time series prediction
stochastically guided disjunctive version space learning paper presents incremental concept learning approach concepts high overall accuracy main idea address concept central problem learning multiple descriptions many traditional inductive algorithms disjunctive version space family considered face problem approach focuses combinations possibly overlapping concepts original stochastic complexity formula focusing organized simulated search experiments show approach especially suitable developing incremental learning algorithms following advantages generates highly accurate concepts second certain degree sensitivity order examples third noisy examples
towards creative casebased design systems casebased reasoning cbr great deal offer supporting creative design particularly processes rely heavily previous design experience problem evaluating design alternatives however existing cbr systems living potential tend adapt reuse old solutions ways producing robust results little research effort directed towards kinds situation assessment evaluation processes facilitate exploration ideas problems crucial creative design also typically control structures facilitate kinds control inherent creative reasoning paper describe types behavior like casebased design systems support based study designers working engineering problem show standard cbr framework extended describe architecture developing experiment ideas
models paper present framework building probabilistic automata parameterized probabilities gibbs distributions used model state transitions output generation parameter estimation carried using algorithm uses generalized iterative scaling procedure discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology
design interactive exploration using memorybased techniques one characteristics design designers rely extensively past experience order create new designs memorybased techniques artificial intelligence help store retrieve reuse knowledge held memory good candidates designers another characteristic design phenomenon exploration early stages design configuration begins partially defined problem specification process exploration gradually refines modifies understanding problem improves paper describe interactive design system employs memorybased techniques help users explore design problems pose system order acquire better understanding requirements problems applied domain structural design
learning generative models algorithm algorithm inverting learning neural network generative models sensory input inverting model generates patterns hidden variables using topdown connections process iterative utilizing negative feedback loop depends error signal bottomup connections error signal also used learn generative model examples algorithm principal component analysis inference argued formed interaction bottomup sensory data topdown according one interpretation perception procedure sequential hypothesis testing propose new algorithm called interpretation neural networks uses topdown connections generate hypotheses bottomup connections revise important understand difference backpropagation backpropagation learning algorithm recognition models shown figure bottomup connections recognize patterns topdown connections error signal used learn recognition model contrast algorithm inverting learning generative models shown figure topdown connections generate patterns set hidden variables sensory input inverting generative model recovering hidden variables generated sensory data operation called either pattern recognition pattern analysis depending meaning hidden variables generative model done iteratively negative feedback loop driven error signal bottomup connections error signal also used learning connections experiments images handwritten digits
using case base speedup reinforcement learning paper demonstrates exploitation certain vision processing techniques index case base result reinforcement learning represent optimum choice actions achieve goal state space paper shows strong features occur interaction system environment detected early learning process features allow system identify identical similar task solved previously retrieve relevant surface results orders magnitude increase learning rate
teaching strategy memorybased control combining different machine learning algorithms system produce benefits beyond either method achieve alone paper demonstrates genetic algorithms used conjunction lazy learning solve examples difficult class delayed reinforcement learning problems better either method alone class class differential games includes numerous important control problems arise robotics planning game playing areas solutions differential games suggest solution strategies general class planning control problems conducted series experiments applying three learning qlearning knearest neighbor knn genetic particular differential game called pursuit game experiments demonstrate knn great difficulty solving problem lazy version qlearning performed genetic algorithm performed even better results motivated next step experiments knn difficulty good common source difficulty lazy learning therefore used genetic algorithm method knn create system provide examples experiments demonstrate resulting joint system learned solve pursuit games high degree either method relatively small memory requirements
generative models discovering sparse distributed representations describe hierarchical generative model viewed nonlinear generalization factor analysis implemented neural network model uses bottomup topdown lateral connections perform bayesian perceptual inference correctly perceptual inference performed connection strengths updated using simple learning rule requires locally available information network learns extract sparse distributed hierarchical representations
hierarchical evolution neural networks applications neuroevolution individual population represents complete neural network recent work system however demonstrated evolving individual neurons often produces efficient genetic search paper demonstrates solve easy tasks quickly often larger problems hierarchical approach neuroevolution presented difficulties integrating exploratory search search robot arm manipulation task hierarchical approach outperforms search search
evolve autonomous robots different approaches evolutionary robotics
finding structure reinforcement learning reinforcement learning addresses problem learning select actions order maximize ones performance unknown environments scale reinforcement learning complex realworld tasks typically studied one must able discover structure world order abstract away details operate tractable problem spaces paper presents skills algorithm skills discovers skills partially defined action policies arise context multiple related tasks skills whole action sequences single operators learned minimizing action policies using description length argument representation empirical results simple grid navigation tasks illustrate successful discovery structure reinforcement learning
online learning linear loss constraints consider generalization model learning functions learner must satisfy general constraint number incorrect predictions number incorrect predictions describe generalpurpose optimal algorithm formulation problem describe several applications general results involving situations learner satisfy linear
markov chain monte carlo convergence comparative review critical issue users markov chain monte carlo mcmc methods applications determine sampling use samples estimate characteristics distribution interest research methods computing theoretical convergence bounds holds promise future currently relatively little practical use applied work consequently mcmc users address convergence problem applying diagnostic tools output produced running samplers giving brief overview area provide review convergence describing theoretical basis practical implementation compare performance two simple models conclude methods fail detect convergence failure designed identify thus combination strategies aimed evaluating mcmc sampler convergence including applying diagnostic procedures small number parallel chains monitoring modifying sampling algorithms appropriately however possible finite sample mcmc algorithm representative underlying stationary distribution assistant professor school health associate professor division school health university much work done first author graduate student university assistant professor section department university medical center work authors supported part national institute first award authors thank studied sharing insights experiences software helpful discussions suggestions greatly improved
evolutionary module acquisition
competitive learning although detection invariant structure given set input patterns many recognition tasks connectionist learning rules tend focus directions high variance principal components prediction paradigm often used suggest direct approach invariant learning based learning rule unsupervised network implementing method competitive setting learns extract coherent depth information
instancebased learning methods explicitly data receive usually training phase prediction time perform computation take query search database similar build online local model local average local regression predict output value paper review advantages instance based methods autonomous systems also note cost slow computation database grows large present evaluate new way database new algorithm maintains advantages instancebased learning earlier attempts combat cost instancebased learning explicit data applicable instancebased predictions based small number near neighbors explicit training phase form data structure approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously permits query database conventional linear search greatly reduced computational cost
uncertainty discrete bayesian models navigation discrete bayesian models used model uncertainty navigation question actions chosen remains largely paper presents optimal solution problem formulated partially observable markov decision process since solving optimal control policy intractable general explore variety heuristic control strategies control strategies compared experimentally simulation runs robot
system agents ieee transactions neural networks also available report
implementation experiment nested generalized algorithm technical note describes work nge recently implemented algorithm run case studies purpose note implementation note result using implementation nge available address
sampling multimodal distributions using transitions technical report department statistics university toronto abstract present new markov chain sampling method appropriate distributions isolated modes like method simulated transition method uses series distributions distribution interest distribution sampling easier new method advantage require approximate values constants distributions needed simulated estimate simulated performs random walk along series distributions used contrast transitions new method move systematically desired distribution distribution back desired distribution systematic movement avoids random walk advantage unfortunately increase number distributions required sampling efficiency transition method simple problems similar simulated complex distributions however simulated transitions may perform better depends ways distributions
abstract describe ongoing project develop adaptive training system dynamically models students learning processes provide specialized adapted students knowledge state learning style student modeling component uses machine learning techniques students transition learning methods student used reach current knowledge state comparing students solution trace expert solution generating plausible hypotheses errors student made casebased approach used generate hypotheses applying analogy student expert models use representation includes abstract concepts relationships strategies problem solving fuzzy methods used represent uncertainty student model paper describes design gives detailed example system model student typical session domain use example level
abstract addresses issues knowledge cognition cognition argue process improved growing experience therefore mental models needed facilitate reuse previous processes will satisfy requirement describing casebased approach planning previous experience obtained reasoning object level plans used approach support various tasks identified generation example planning behaviour system described
alternative markov property chain graphs graphical markov models use graphs either directed mixed represent possible among statistical variables applications graphs include models spatial dependence image analysis acyclic directed graphs especially statistical analysis arise fields genetics models expert systems bayesian belief networks introduced markov property chain graphs mixed graphs used represent simultaneously causal associative dependencies include special cases paper alternative markov property chain graphs introduced ways direct extension markov property property chain graph
theory revision fault hierarchies fault hierarchy representation widely used expert systems diagnosis complex devices paper describes theory revision algorithm fault hierarchies task presents several challenges typical training instances missing feature values pattern missing features significant rather effect noise quality candidate theory depends returns set tests uses reach paper describes addresses challenges reports experiments use improve performance two diagnostic systems extended version paper appeared proceedings international workshop principles diagnosis new york helpful comments current address robotics laboratory computer science department university email
distributed genetic algorithms partitioning uniform
competitive approach game learning machine learning game strategies often competitive methods continually develop new strategies capable previous ones use definition game consider framework within competitive algorithm makes repeated use strategy learning component learn strategies given set describe game learning terms sets first second player strategies model familiar models concept learning show importance ideas teaching set specification number new context performance several competitive algorithms investigated using worstcase randomized strategy learning algorithms central result theorem competitive algorithm solves games total number strategies polynomial use demonstrated including application concept learning new kind oracle conclude complexity analysis game learning list number new questions arising work
comparison selection schemes used genetic algorithms december version
self bounding learning algorithms work attempts give bounds generalization error hypothesis generated learning algorithm based methods theory uniform convergence bounds bounds hold distribution examples calculated data observed paper propose different approach bounding generalization error data observed learning algorithm algorithm addition hypothesis outputs outputs reliable upper bound generalization error hypothesis first explore idea statistical query learning framework give explicit self bounding algorithm learning algorithms based local search
markov decision processes large state spaces paper propose new framework studying markov decision processes mdps based ideas statistical mechanics goal learning mdps find policy yields maximum expected return time choosing policies agents must therefore shortterm versus longterm gains study simple mdp agent must decide exploratory local reward mining state space number policies choose grows exponentially size state space view expected returns defining energy landscape policy space methods statistical mechanics used analyze landscape limit calculate overall distribution expected returns distribution returns policies fixed distance optimal one briefly discuss problem learning optimal policies empirical estimates expected return first step relate findings entropy limit learning numerical simulations support theoretical results
neural networks quadratic dimension paper shows neural networks use continuous activation functions dimension least large square number weights result open question namely whether wellknown log bound known nets also held general sigmoidal nets implications number samples needed valid generalization discussed
neural networks blind separation sources novel online learning algorithms self adaptive learning rates parameters blind separation signals proposed main motivation development new learning rules improve convergence speed reduce especially nonstationary signals furthermore discovered conditions proposed neural network models associated learning algorithms exhibit random attention ability chaotic random switching crossover output signals way specified separated signal may appear various outputs different time validity performance dynamic properties proposed learning algorithms investigated computer simulation experiments
efficient learning multiple task sequences present modular network architecture learning algorithm based incremental dynamic programming allows single learning agent learn solve multiple markovian decision tasks significant transfer learning across tasks consider class called composite tasks formed temporally number simpler elemental architecture trained set composite elemental temporal structure composite task assumed unknown architecture learns produce temporal decomposition shown certain conditions solution composite constructed computationally modifications solutions constituent elemental
program synthesis transformation techniques optimization constraint satisfaction deductive synthesis numerical face problems constructing testing modifying numerical simulation programs process coding revising extremely almost always written conventional programming languages therefore benefit software facilitates construction programs simulating physical systems research adapts methodology deductive program synthesis problem constructing numerical simulation codes focused represented second order functional programs composed numerical integration root extraction routines developed system uses first order horn logic numerical built components approach based two ideas first relationship integration neither attempt require complete mathematical analysis second system uses representation functions objects function objects encoded expressions knowledge base includes term also includes defining semantics numerical integration root extraction routines use depth bounded resolution construct programs system successfully constructed numerical computational design engine among others results demonstrate deductive synthesis techniques used construct numerical simulation programs realistic applications automatic design optimization highly sensitive problem formulation choice objective function constraints design parameters dramatically impact computational cost optimization quality resulting design best formulation varies one application another design will usually know best formulation advance order address problem developed system supports interactive formulation testing reformulation design optimization strategies system includes language representing optimization strategies language allows define multiple stages optimization using different approximations objective constraints different abstractions design space also developed set transformations strategies represented language transformations approximate objective constraint functions abstract search spaces optimization process multiple stages system applicable principle design problem expressed terms constrained
independence bayesian networks bayesian networks provide language representing conditional independence properties distribution allows natural compact representation distribution knowledge acquisition supports effective inference algorithms wellknown however certain capture within bayesian network structure hold certain contexts given specific assignment values certain variables paper propose formal notion independence based regularities conditional probability tables node present technique analogous based determining independence holds given network focus particular qualitative representation suggest ways representation used support effective inference algorithms particular present structural decomposition resulting network improve performance clustering algorithms alternative algorithm based conditioning
machine learning reinforcement learning traces trace one basic mechanisms used reinforcement learning handle delayed reward paper introduce new kind trace trace analyze theoretically show results faster reliable learning conventional trace kinds trace assign credit prior events according recently conventional trace gives greater credit repeated events analysis conventional versions algorithm applied undiscounted markov chains first show methods converge repeated training set predictions two known monte carlo methods analyze relative efficiency two monte carlo methods show method corresponding conventional biased whereas method corresponding unbiased addition show method corresponding traces closely related maximum likelihood solution tasks mean squared error always lower long run computational results confirm analyses show applicable generally particular show traces significantly improve performance reduce parameter sensitivity task full reinforcementlearning problem continuous state space using function approximator
integrating creativity reading functional approach reading studied variety cognitive yet theories exist sufficiently describe explain people complete task reading realworld particular type knowledge reading known creative reading largely past research argue creative reading aspect reading experiences result theory will insufficient built results psychology artificial intelligence order produce functional theory complete reading process overall framework describes set tasks necessary reading performed within framework developed theory creative reading theory implemented integrated story analysis creativity system reading system science stories
error certain found system order document missing list made
casebased reasoning issues methodological variations system approaches casebased reasoning issues methodological casebased reasoning recent approach problem solving learning attention last years basic idea underlying theories now within period highly active research casebased reasoning paper gives overview issues related casebased reasoning describes leading methodological approaches within field current state systems initially general framework defined subsequent descriptions discussions will refer framework influenced recent knowledge level descriptions intelligent systems methods case retrieval reuse solution testing learning actual realization discussed light example systems represent different cbr approaches also discuss role casebased methods one type reasoning learning method within integrated system architecture
updates study problem combining updates special instance theory change counterfactual propositional intuitively update means world described changed opposed revisions another instance theory change knowledge static world changes counterfactual implication form case also case may current knowledge present logic called update counterfactual implication object language update operator generalization operators previously proposed studied literature show operator satisfies certain postulates set reasonable update logic extension logic counterfactual semantics multimodal propositional based possible worlds rule becomes derivation rule sound complete show theorem combine theory change counterfactual via rule hold logic thus seen theorem applies revision operators updates preliminary version paper presented second international conference principles knowledge representation reasoning cambridge massachusetts work partially performed author department computer science university toronto
discovering neural nets low complexity high generalization capability neural networks many neural net learning algorithms aim finding simple nets explain training data expectation simpler networks better generalization test data occams razor previous implementations however use measures simplicity lack power based complexity algorithmic probability previous approaches especially bayesian kind suffer problem choosing appropriate priors paper addresses issues first reviews basic concepts algorithmic complexity theory relevant machine learning distribution universal prior deals prior problem universal prior leads probabilistic method finding simple problem solutions high generalization capability method based complexity generalization complexity inspired optimal universal search algorithm given problem solution candidates computed efficient programs influence runtime storage size probabilistic search algorithm finds good programs ones quickly computing solutions fitting training data simulations focus task discovering simple neural networks low complexity high generalization capability demonstrated method least certain problems computationally feasible lead generalization results previous neural net algorithms much remains done however make large scale applications incremental learning feasible
casebased reasoning system generating expressive musical performances studied problem generating expressive musical performances context interpretations done several playing different different degrees including interpretation analyzed using spectral modeling techniques extract information related several expressive parameters set parameters set cases examples casebased system set cases system set possible expressive transformations given new applying similarity criteria based background musical knowledge new set cases finally applies inferred expressive transformations new using synthesis capabilities
boosting margin new explanation effectiveness voting methods one surprising phenomena observed experiments boosting test error generated classifier usually increase size becomes large often observed decrease even training error zero paper show phenomenon related distribution training examples respect generated voting classification rule margin example simply difference number correct maximum number received incorrect show techniques used analysis support vector classifiers neural networks small weights applied voting methods relate margin distribution test error also show theoretically experimentally boosting especially effective increasing training examples finally compare explanation based decomposition
supervised learning incomplete data via approach realworld learning tasks may involve highdimensional data sets arbitrary patterns missing data paper present framework based maximum likelihood density estimation learning data sets use mixture models density estimates make two distinct expectationmaximization principle deriving learning used estimation mixture components missing data resulting algorithm applicable wide range supervised unsupervised learning problems results classification data presented
recognition hierarchical feature maps hierarchical feature map system input story instance particular classifying three levels role recognition taxonomy roles extracted automatically independently examples unsupervised selforganizing process process human learning frequently encountered become gradually detailed resulting structure feature maps hierarchy taxonomy maps topology level number input selforganization time considerably reduced compared ordinary feature mapping system recognize incomplete stories missing events taxonomy also memory organization memory maps assign unique memory location parts input data separated resources representing accurately
learning generate artificial trajectories target detection shown static neural approaches adaptive target detection efficient sequential alternative latter inspired observation biological systems employ sequential pattern recognition system described builds adaptive model timevarying inputs artificial controlled adaptive neural controller controller uses adaptive model learning sequential generation trajectories move target visual scene system also learns track moving targets teacher provides desired activations various times goal information shape target since task task involves complex temporal credit assignment problem implications adaptive systems general discussed
hierarchical mixtures experts algorithm present treestructured architecture supervised learning statistical model underlying architecture hierarchical mixture model mixture coefficients mixture components generalized linear models learning treated maximum likelihood problem particular present expectationmaximization algorithm parameters architecture also develop online learning algorithm parameters updated incrementally comparative simulation results presented robot dynamics domain report describes research done brain cognitive sciences center biological computational learning artificial intelligence laboratory massachusetts institute technology support provided part grant nsf asc9217041 support laboratorys artificial intelligence research provided part advanced research projects agency defense authors supported grant foundation grant atr human information processing research laboratories grant siemens corporation grant national science foundation grant office naval research nsf grant support intelligent control mit michael jordan nsf young
memory model case retrieval activation
view algorithm incremental sparse variants algorithm performs maximum likelihood estimation data variables present function negative free energy show step function respect model parameters step respect distribution variables perspective easy incremental variant algorithm distribution one variables step variant shown empirically give faster convergence mixture estimation problem variant algorithm exploits sparse conditional distributions also described wide range variant algorithms also seen possible
synchronization network locally coupled oscillators network oscillators constructed emergent properties synchronization investigated computer simulation formal analysis network twodimensional matrix oscillator coupled neighbors show analytically chain locally coupled oscillators linear approximation oscillator present technique rapidly finite numbers oscillators coupling strengths change fast time scale based hebbian rule global introduced receives input feedback oscillator matrix global used different oscillator groups unlike many models properties network emerge local connections preserve spatial relationships among components critical encoding principles feature grouping ability oscillator groups within network offers promising approach pattern segmentation based correlation
probabilistic networks new models new methods paper describe implementation probabilistic regression model program bayesian inference statistical problems using simulation technique known gibbs sampling possible implement surprisingly complex regression models environment demonstrate simultaneous inference noise level
hierarchical ensemble decision trees applied classifying data psychological experiment classifying hand complex data psychology experiments long difficult task data classify amount training may require one way problem use machine learning techniques built classifier based decision trees classifying process used two humans sample data learns classify unseen data automatic classifier proved accurate constant much faster classification hand
neural network implementation software estimation training methods neural network literature usually simple form gradient descent algorithm suitable implementation hardware using massively parallel computations ordinary computers massively parallel optimization algorithms several procedures usually far efficient shows fit neural networks using software
modification probability selecting right reference class right interval candidates possibility subset style dominance problem probability system various methods proposed solve problem way intuitively within framework scheme proposed paper leads stronger statistical without much intuitive proposal
reinforcement learning approach jobshop scheduling apply reinforcement learning methods learn heuristics job shop scheduling scheduler schedule incrementally constraint goal finding short schedule temporal difference algorithm applied train neural network learn heuristic evaluation function states evaluation function used lookahead search procedure find good solutions new scheduling problems evaluate approach synthetic problems problems space processing task evaluation function trained problems involving small number tested larger problems scheduler performs better best known existing algorithm iterative method based simulated annealing results suggest reinforcement learning provide new method constructing scheduling systems
neural network pole learns operates real robot real time neural network approach task presented task task keeping pole cart free fall plane roughly orientation moving cart plane keeping cart within maximum distance starting position task difficult control problem parameters system known precisely variable also forms basis even complex problem controller must learn proper actions successfully balancing pole given current state system failure signal pole becomes great cart one boundaries position approach presented demonstrated effective realtime control small task details learning scheme hardware results actual learning trials presented
approximate bayes factors model uncertainty generalized linear models technical report department statistics university washington august revised march
qlearning network modified reinforcementlearning paradigm existing hidden units rather adding new units units learn via backpropagation resulting algorithm tested qlearning network learns solve problem solutions found faster average algorithm without
split window paradigm exploiting parallelism propose new processing paradigm called split window paradigm exploiting parallelism paradigm considers window instructions possibly dependencies single unit exploits parallelism overlapping execution multiple basic idea multiple sequential processors manner achieve overall multiple issue processing paradigm number properties restricted machines derived sequential von architecture also present implementation split window execution model preliminary performance results
experimental comparison algorithms algorithms based nested generalized nge theory classify new data points computing distance nearest generalized either point axisparallel combine character nearest neighbor classifiers axisparallel representation employed many systems implementation nge compared knearest neighbor knn algorithm domains found significantly knn several modifications nge studied understand cause poor performance show performance substantially improved nge creating overlapping rectangles still allowing complete rectangles performance improved modifying distance metric allow weights features best results obtained study weights computed using mutual information features output class best version nge developed batch algorithm parameters performance comparable neighbor algorithm also incorporating feature weights however knearest neighbor algorithm still significantly superior domains conclude even improvements nge approach sensitive shape decision boundaries classification problems domains decision boundaries axisparallel nge approach produce excellent generalization hypotheses domains tested nge algorithms require much less memory store generalized required algorithms
model selection search classification function approximation selecting good model set input points cross validation computationally process especially number possible models number training points high techniques gradient descent helpful searching space models problems local minima lack distance metric various models reduce applicability search methods technique finding good model data quickly models computational effort better ones paper focuses special case leaveoneout cross validation applied memorybased learning algorithms also argue applicable class model selection problems
searches smallest possible feature sets subset set many learning problems learning system presented values features actually irrelevant concept trying learn focus algorithm due performs explicit search smallest possible input feature set permits consistent mapping features output feature focus algorithm also seen algorithm learning determinations functional dependencies suggested another algorithm learning determinations appears focus algorithm runtime open question underlying problem paper problem shown npcomplete also describe briefly experiments demonstrate benefits determination learning show finding determinations easier practice finding minimal define problem follows given set examples composed binary value value target feature vector binary values values features number determine whether exists feature set show npcomplete reducing problem may question given graph subset size edge connected least one may reduce instance instance mapping edge example one input feature every proof reported result reduction set covering proof therefore fails show
algorithm unsupervised neural networks unsupervised learning algorithm multilayer network stochastic neurons described bottomup recognition connections input representations successive hidden layers topdown generative connections representation one layer representation layer phase neurons driven recognition connections generative connections adapted increase probability correct activity vector layer phase neurons driven generative connections recognition connections adapted increase probability produce supervised learning algorithms multilayer neural networks face two problems require teacher specify desired output network require method error information connections algorithm avoids problems external teaching signal matched goal required force hidden units extract underlying structure algorithm goal learn representations describe allow input accurately quantify goal communication game vector raw sensory inputs first hidden representation difference input vector topdown reconstruction hidden representation aim learning minimize description length total number bits required input vectors way communication actually takes place minimizing description length required forces network learn representations capture underlying regularities data correct activity vector layer
ieee learning semantic similarity software components properly structured software libraries crucial success software reuse specifically structure software library functional similarity stored software components order facilitate retrieval process propose application artificial neural network technology achieve structured library detail utilize artificial neural network unsupervised learning paradigm feature model make semantic relationship stored software components explicit thus actual user software library notion semantic relationship components terms
learning analytically learning fundamental component intelligence key designing cognitive architectures chapter considers question appropriate generalpurpose learning mechanism interested mechanisms might explain rich variety learning capabilities humans ranging learning skills learning highly cognitive tasks play chess research learning fields cognitive science artificial intelligence statistics led identification two distinct classes learning methods inductive analytic inductive methods neural network backpropagation learn general laws finding statistical correlations regularities among large set training examples contrast analytical methods explanationbased learning acquire general laws many fewer training examples rely instead prior knowledge analyze individual training examples detail use analysis distinguish relevant example features irrelevant question considered chapter best combine inductive analytical learning architecture seeks cover range learning exhibited intelligent systems humans present specific learning mechanism explanation based neural network learning two types learning present experimental results demonstrating ability learn control strategies mobile robot using
gibbs sampling analysis large many loops
perfect simulation stochastic simulation plays important role stochastic related fields simplest random set models tend intractable analysis many simulation algorithms approximate samples random set models example simulating equilibrium distribution markov chain spatial process samples usually fail exact algorithm markov chain long finite time thus convergence equilibrium approximate work made important contribution simulation coupling method coupling past perfect exact simulations markov chains paper introduce new idea perfect simulation illustrate using two common models stochastic leaves model boolean model cover finite set points
bayesian detection clusters disease maps
lazy induction cbr recent years casebased reasoning demonstrated highly useful problem solving complex domains also mixed paradigm approaches combining cbr induction techniques knowledge andor building efficient case memory however complex domains induction whole problem space often possible time paper approach presented close interaction cbr part attempts induce rules particular context problem just solved system rules may used indexing purposes similarity assessment order support cbr process future
adaptive tuning numerical prediction models simultaneous estimation weighting smoothing physical parameters
planning learning robotic game paper demonstrates use finite automata learning algorithm utility planner robotic domain many applications robot agents need predict movement objects environment plan avoid robot reasoning model object machine learning techniques used generate one project learn dfa model robot use automaton predict next move adversary robot agent plans path avoid adversary predicted location goal requirements
bayesian forecasting time series gaussian dynamic models peter assistant professor professor institute statistics decision sciences university research performed partially supported nsf grant
using markov chains analyze theoretical understanding properties genetic algorithms gas used function optimization strong like traditional schema analysis provides first order insights capture nonlinear dynamics search process markov chain theory used primarily steady state analysis gas paper explore use markov chain analysis model understand behavior finite population observed transition steady states approach appears provide new insights circumstances will will perform preliminary results presented initial evaluation approach provided
adaptive noise input variables relevance determination paper consider application training noise multilayer perceptron input variables relevance determination noise modified order irrelevant features proposed algorithm attractive requires tuning single parameter parameter controls inputs together complexity model presentation method experimental given simulated data sets
multivariate versus univariate decision trees technical report january abstract paper present new multivariate decision tree algorithm combines linear machines decision trees constructs test decision tree training linear machine eliminating irrelevant noisy variables controlled manner examine ability find good generalizations present results variety domains compare empirically univariate decision tree algorithm observe multivariate tests appropriate bias given data set finds small accurate trees
reinforcement learning reinforcement learning tuning adaptation method control dynamic systems contrary supervised learning based usually gradient descent techniques require model sensitivity function process hence applied systems poorly understood uncertain nonlinear reasons conventional methods reinforcement learning overall controller performance evaluated measure called reinforcement depending type control task reinforcement may represent evaluation recent control action often entire sequence past control moves latter case system learns predict outcome individual control action prediction used parameters controller mathematical background closely related optimal control dynamic programming paper gives overview methods presents application control known applications literature reviewed
lateral interaction develops selforganizing feature map biologically motivated mechanism selforganizing neural network lateral connections presented weight modification rules purely unsupervised local lateral interaction weights initially random develop shape around neuron time external input weights form map input space algorithm demonstrates selforganization bootstrap using input information predictions algorithm agree experimental observations development lateral connections cortical feature maps
new challenge statistics causation main users statistical methods social discovering fields statistical causal foundations foundations years follows lack mathematical capable distinguishing causal relationships providing formal natural relations graphical methods potential statistics used applications response beginning causality concept clear mathematical paper surveys developments outlines future challenges
combining topdown bottomup techniques inductive logic programming paper describes new method inducing logic programs examples attempts integrate best aspects existing ilp methods single coherent framework particular combines bottomup method similar topdown method similar foil also includes method predicate invention similar solution noisy oracle problem allows system learn recursive programs without requiring complete set positive examples systematic experimental comparisons foil range problems used clearly demonstrate advantages approach
computing upper lower bounds intractable networks present deterministic techniques computing upper lower bounds marginal probabilities sigmoid networks techniques become useful size network clique size exact computations illustrate bounds numerical
recursive algorithms approximating probabilities graphical models mit computational cognitive science technical report abstract develop recursive formalism efficiently approximating large probabilistic networks constraints set network topologies yet formalism integrated exact methods applicable approximations use controlled maintain consistently upper lower bounds desired quantities times show boltzmann machines sigmoid belief networks combination chain graphs handled within framework accuracy methods verified
general lower bound number examples needed learning prove lower bound ffi number random examples required learning concept class vapnikchervonenkis dimension ffi accuracy confidence parameters improves previous best lower bound ffi comes close known general upper bound ffi consistent algorithms show many interesting concept classes including bound actually tight within constant factor
data exploration using selforganizing maps
net neural network modeling temporal variability ability handle temporal variation important dealing realworld dynamic signals many applications inputs sequences rather signals time scales vary one instance next thus modeling dynamic signals requires ability recognize sequences also ability handle temporal changes signal paper discusses net neural network modeling dynamic signals application speech net sequence learning accomplished using combination prediction recurrence connections temporal variability modeled time constants network respect prediction error adapting time constants changes time scale network adapted value networks time constant provides measure temporal variation signal net applied several simple signals sets frequency phase multidimensional signal representing energy simple speech net also shown work distinction task using synthetic speech data paper net applied two tasks recognition recognition using speech data taken database shown nets trained achieved performance networks without time constants trained rates performed better networks without time constants trained results demonstrate nets ability identify variable speech rates rates represented training set
neural networks bpsom backpropagation learning known serious limitations knowledge certain types learning material bpsom extension limitations bpsom combination feedforward network trained selforganising maps earlier reports shown bpsom improved generalisation performance whereas simultaneously number necessary hidden units without loss generalisation performance two effects use learning training paper focus two additional effects first show bpsom training activations hidden units tend among limited number discrete values second identify elements adequate instances task hand effects argue lead neural networks employed basis automatic rule extraction
pattern discrimination using feedforward networks benchmark study scaling behaviour discrimination multilayer perceptron mlp learning vector networks compared overlapping gaussian distributions shown analytically monte carlo studies mlp network high dimensional problems efficient way mainly due sigmoidal form mlp transfer function also fact mlp uses hyperplanes efficiently algorithms equally robust limited training sets learning curves fall like training set size compared theoretical predictions statistical estimates vapnikchervonenkis bounds
generalization
rate convergence gibbs sampler gaussian approximation summary article approximate rate convergence gibbs sampler normal approximation target distribution based approximation consider many issues gibbs sampler updating strategy give theoretical results approximation illustrate methods number realistic examples
instancebased learning methods explicitly data receive usually training phase prediction time perform computation take query search database similar build online local model local average local regression predict output value paper review advantages instance based methods autonomous systems also note cost slow computation database grows large present evaluate new way database new algorithm maintains advantages instancebased learning earlier attempts combat cost instancebased learning explicit data applicable instancebased predictions based small number near neighbors explicit training phase form data structure approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously permits query database conventional linear search greatly reduced computational cost
many clusters clustering method answers via modelbased cluster analysis
learning experiments simulated car implemented reinforcement learning architecture reactive component two layer control system simulated car found separating layers gradually improving competition interaction experiments test tuning decomposition coordination low level behaviors extended control system allow tested ability avoid best design used reinforcement learning separate networks behavior coarse coded input simple rule based coordination mechanism
feature reduction applied hybrid genetic algorithm study concerned whether possible detect information contained training data background knowledge relevant solving learning problem whether irrelevant information preprocessing starting learning process case study data preprocessing hybrid genetic algorithm shows elimination irrelevant features substantially improve efficiency learning addition feature elimination effective reducing costs induced hypotheses
genetic programming exploratory power discovery functions hierarchical genetic programming hgp approaches rely discovery modification use new functions accelerate evolution paper provides qualitative explanation improved behavior hgp based analysis evolution process dual perspective diversity causality static point view use hgp approach enables manipulation population higher diversity programs higher diversity increases exploratory ability genetic search process demonstrated theoretical experimental fitness distributions structural complexity individuals dynamic point view analysis causality crossover operator suggests hgp discovers exploits useful structures bottomup hierarchical manner diversity causality complementary exploration exploitation genetic search unlike machine learning techniques need extra control tradeoff hgp automatically exploration exploitation
learning complex extended sequences using principle history compression neural computation previous neural network learning algorithms sequence processing computationally expensive perform poorly comes long time paper first introduces simple principle reducing descriptions event sequences without loss information consequence principle inputs relevant insight leads construction neural architectures learn sequences describe two architectures first functions selforganizing hierarchy recurrent networks second involving two recurrent networks tries predictor hierarchy single recurrent net experiments show system require less computation per time step many fewer training sequences conventional training algorithms recurrent nets
ocular dominance lateral connections selforganizing model primary visual cortex neural network model selforganization ocular dominance lateral connections input presented selforganizing process results network afferent weights neuron smooth receptive fields primarily one neurons common eye preference form connected lateral connections primarily link regions eye preference similar selforganization cortical structures observed experimentally model shows lateral connections cortex may develop based correlated activity explains lateral connection patterns follow receptive field properties ocular dominance
fast numerical integration relaxation oscillator networks based limit solutions relaxation one time scale arise naturally many physical systems paper proposes method integrate large systems relaxation oscillators numerical technique called limit method derived analysis relaxation limit limit system evolution gives rise time fast dynamics takes place intervals slow dynamics takes place full description method given locally excitatory globally inhibitory oscillator networks fast dynamics characterized leads phase shifts captured method iterative operation slow dynamics entirely solved limit method evaluated computer experiments produces speedup compared methods integrating systems speedup makes possible simulate largescale oscillator networks
selforganization segmentation connected spiking neurons selforganizing model spiking neurons dynamic lateral excitatory inhibitory connections presented tested image segmentation task model integrates two previously separate research modeling visual cortex connected selforganizing maps used model afferent structures lateral connections hebbian adaptation spiking neurons synapses used model image segmentation binding synchronization neuronal activity although approaches differ model neuron overall connected twodimensional network paper shows selforganization segmentation achieved network thus presenting unified model development dynamics primary visual cortex
gaussian processes bayesian classification via hybrid monte carlo full bayesian method applying neural networks prediction problem set structure net perform necessary however tractable analytically markov chain monte carlo mcmc methods slow especially parameter space highdimensional using gaussian processes approximate weight space analytically small number hyperparameters need integrated mcmc methods applied idea classification problems obtaining results realworld problems investigated far
perfect simulation point processes user recently proposed algorithm called coupling past allows approximate perfect exact simulation stationary distribution certain finite state space markov chains perfect sampling using successfully extended context point processes authors gibbs sampling applied point process mixture model however general running time terms number transitions independent state sampled thus user long runs may introduce bias user bias introduced exact sampling algorithm finite state space markov chains contrast unbiased user fills algorithm form sampling similar requires sufficient properties transition kernel used show fills version sampling extended infinite state space context produce exact sample mixture process related models following use gibbs sampling make use partial order mixture model state space thus
selforganization functional role lateral connections receptive fields primary visual cortex cells visual cortex selective ocular dominance orientation input also size spatial frequency simulations reported paper show size develop hebbian selforganization receptive fields different sizes like orientation ocular dominance lateral connections network simultaneously receptive field sizes produce patterns lateral connectivity closely follow receptive field organization together previous work ocular dominance orientation results suggest single hebbian selforganizing process give rise major receptive field properties visual cortex also structured patterns lateral interactions verified experimentally others predicted model model also suggests functional role structures afferent receptive fields develop sparse coding visual input recurrent lateral interactions eliminate cortical activity patterns allowing cortex efficiently process amounts visual information
optimal control artificial neural networks study study described practical application artificial neural networks limit cycle control selected test case one sources limit cycle position dependent error observed reinforcement learning method selected able adapt controller cost function estimate cost function learned neural approach estimated cost function directly represented function parameters linear controller implemented network results simulations show method able find optimal parameters without behaviour particular case large measurements method shows clear improvement compared conventional approach error decreases approximately
evolving networks using genetic algorithm connectionist learning
paclearning prolog clauses without errors describe generic ilp problem following given set positive negative examples target predicate background knowledge world usually logic program including facts auxiliary predicates task find logic program hypothesis positive examples negative example paper review results achieved area discuss techniques used moreover prove following new results predicates described local clauses literals distribution generalizes previous result valid constrained clauses predicates described local clauses distribution generalizes previous result non valid class distributions finally introduce believe first theoretical framework learning prolog clauses presence errors purpose introduce new noise model call fixed attribute noise model learning propositional concepts boolean domain new noise model interest
expectationmaximization algorithm map estimation expectationmaximization algorithm given considerable solving map estimation problems note gives simple derivation algorithm due better illustrates convergence properties algorithm variants algorithm illustrated two examples data multiple noisy sources fitting mixture density
towards planning incremental adaptive robot control
prior knowledge creation virtual examples rbf networks consider problem incorporate prior knowledge supervised learning techniques set problem framework regularization theory consider case know approximated function radial symmetry problem solved two alternative ways use constraint regularization theory framework derive invariant version radial basis functions use radial symmetry create new virtual examples given data set show two different methods learning
gain adaptation least present computational results suggesting algorithms based part connectionist learning methods may improve least classical methods stochastic timevarying linear systems new algorithms evaluated respect classical methods along three dimensions asymptotic error computational complexity required prior knowledge system new algorithms order complexity methods dimensionality system whereas methods kalman filter new methods also improve kalman filter require complete statistical model system varies time simple computational experiment new methods shown produce asymptotic error levels near optimal kalman filter significantly methods new methods may perform better even kalman filter error filters model system varies time
efficient proposed procedure efficient memory use id3 decision tree learning algorithm however previous work shown may often lead decrease performance work try argue rule learning algorithms appropriate divideandconquer algorithms learn rules independently less changes class distributions particular will present new algorithm achieves additional gains efficiency exploiting property algorithms presented algorithm suitable redundant noisefree data sets will also briefly discuss problem noisy data present preliminary ideas might solved extension algorithm introduced paper
theory refinement combining analytical empirical methods article describes approach automatic theory revision given imperfect theory approach combines explanation attempts classified examples order identify portions theory theory fault correlated subsets examples used generate correction focused tend preserve structure original theory system approximate domain theory general fewer training examples required given level performance classification accuracy compared purely empirical system approach applies classification systems employing propositional theory system tested variety application domains results presented problems domains molecular biology disease diagnosis
auxiliary variable methods markov chain monte carlo applications one sample density using markov chain monte carlo mcmc auxiliary variable conditional distribution defined giving joint distribution mcmc scheme samples joint distribution lead substantial gains efficiency compared standard approaches algorithm one example addition algorithm generalizations paper introduces new auxiliary variable method called partial two applications bayesian image analysis considered first binary classification problem partial performs single site metropolis second reconstruction uses level prior generalized algorithm developed problem reduces computing time point mcmc method posterior exploration
convergence slice sampler markov chains paper analyse theoretical properties slice sampler find algorithm extremely robust geometric properties case just one auxiliary variable demonstrate algorithm stochastically monotone analytic bounds total variation distance method using condition methodology
investigating generality automatically defined functions paper studies combination simulated annealing solves genetic programming style program discovery problems suite composed problems analyses performance simulated annealing compared using contrast results suite simulated annealing run problem size increases advantage using standard program representation marginal performance simulated annealing compared algorithm using problem problem equal problem
exploiting irrelevant data learning algorithms work effectively training data contain completely specified labeled samples many diagnostic tasks however data will include values attributes model process values attributes learner remove values critical attributes learner paper instead focuses remove irrelevant attribute values values needed classify instance given values attributes first model demonstrate useful proving certain classes seem hard learn general pac model decision trees dnf formulae trivial learn setting also show model extended deal theory revision modifying existing formula include values required values training data
hierarchical selforganization genetic programming paper presents approach automatic discovery functions genetic programming approach based discovery useful building blocks analyzing evolution trace generalizing blocks define new functions finally adapting problem representation representation determines hierarchical organization extended function set enables search space solutions found easily measures complexity solution trees defined adaptive representation framework minimum description length principle applied feasibility approaches based hierarchy discovered functions suggest alternative ways defining problems fitness function preliminary empirical results presented
pattern recognition via linear programming theory application medical diagnosis decision problem associated fundamental model linearly pattern sets shown npcomplete another model employs norm instead norm solved polynomial time solving linear programs usually small dimensionality pattern space effective finite algorithm proposed solving latter model algorithm employed obtain function separating points representing measurements made taken human computer program trained samples correctly new samples encountered currently use university introduction fundamental problem address
perception musical meter many connectionist approaches musical music let question next equally important question next one latter question one temporal structure considering perception musical meter view perception metrical structure dynamic process temporal organization external musical events internal processing mechanisms article introduces novel connectionist unit based upon mathematical model capable phase components patterns networks units temporally structured responses patterns resulting network behavior perception metrical structure article concludes discussion implications approach theories metrical structure musical
apparent computational complexity physical systems
genetic algorithm research years several developed provide genetic algorithm research use model inspired model used unfortunately working problems routing scheduling paper describes developed specifically problems also works easily kinds problems offers easy use interface allows comparisons made genetic algorithms particular problem includes variety genetic operators reproduction crossover mutation makes easy use operators new ways particular applications develop include new operators finally offers unique new feature dynamic generation gap
memory analysis simulations human memory provides storage experiences retrieval system allows access experiences partial activation components system consist fast storage slow longterm storage within paper presents neural network model hippocampal memory inspired idea convergence model consists layer perceptual feature maps binding layer perceptual feature pattern coarse coded binding layer stored weights layers partial activation stored features binding pattern turn entire stored pattern many configurations model theoretical lower bound memory capacity derived order magnitude higher number units model several orders magnitude higher number units computational simulations indicate average capacity order magnitude larger theoretical lower bound making connectivity layers causes even increase capacity simulations also show binding patterns used errors tend plausible patterns similar patterns cost capacity memory therefore accounts storage associative retrieval capability large capacity hippocampal memory shows memory encoding areas much smaller perceptual maps consist rather coarse computational units connected perceptual maps
empirical learning results value empirical learning demonstrated results testing theory space search component empirical data shows approximations generated generic simplifying assumptions widely varying levels accuracy efficiency candidate theory space includes theories optimal combinations accuracy efficiency others empirical learning thus needed separate optimal theories ones works filter process generating approximations generic simplifying assumptions empirical tests serve additional purpose theory space search data precisely tradeoff accuracy efficiency among candidate approximate theories tradeoff data used select theory best competing objectives accuracy efficiency manner appropriate intended performance context feasibility empirical learning also addressed results testing theory space search component order empirical testing feasible candidate approximate theories must candidate theories generated shown experimental results theory space search phase learning run real machine producing results compared training examples feasibility also depends information computation costs empirical testing information costs result need system training examples computation costs result need execute candidate theories types costs grow numbers candidate theories tested experimental results show empirical testing limited computation costs executing candidate theories information costs obtaining many training examples respect traditional inductive learning systems feasibility empirical learning depends also intended performance context resources available context learning measurements theory space search phase indicate algorithms performing exhaustive search feasible domain although may feasible applications algorithms avoid exhaustive search hold considerably promise
multiagent reinforcement learning theoretical framework algorithm paper stochastic games framework multiagent reinforcement learning work extends previous work stochastic games framework design multiagent qlearning method framework prove converges equilibrium specified conditions algorithm useful finding optimal strategy exists unique equilibrium game exist multiple game algorithm combined learning techniques find optimal strategies
memories distributed case libraries memory modeling track operating costs structural transformations world focus knowledge management concerned making knowledge memories form important part knowledge management paper discuss memories distributed case libraries benefit existing techniques distributed casebased reasoning resource discovery exploitation previous expertise present two techniques developed context multiagent casebased reasoning exploiting past experience memory resources first approach called retrieval deals retrieving case different resources memory form good overall case second approach based learning deals two modes cooperation called let agent exploit experience expertise agents achieve local task first author like support national science foundation grant second authors research reported paper developed inside analog project grant content paper necessarily reflect position policy government government government inferred
using knowledge cognitive behavior learn failure learning reasoning failures knowledge system powerful system system needs learn number benefits arise systems possess knowledge operation knowledge abstract knowledge cognition used select diagnosis strategies among alternatives specific kinds used distinguish failure hypothesis candidates making explicit also facilitate use knowledge across domains provide principled way incorporate new learning strategies illustrate advantages learning provide implemented examples two different systems plan execution system called story understanding system called
domain theories constructive induction approach theory revision integrates inductive learning background knowledge combining training examples coarse domain theory produce accurate theory two challenges theory revision systems face first representation language appropriate initial theory may inappropriate improved theory original representation may express initial theory accurate theory use representation may difficult reach second theory structure suitable coarse domain theory may insufficient theory systems produce small local changes theory limited value complex structural may required consequently advanced learning systems require flexible representation flexible structure analysis various theory revision systems learning systems reveals specific strengths weaknesses terms two desired properties designed capture underlying system new system uses constructive induction experiments three domains show improvement previous systems leads study behavior limitations potential constructive induction
neural computing experiments experiment requires statistical analysis establish result one better experiment experiments one main characteristics abstract increasing research various aspects neural computing much progress theoretical advances empirical studies empirical side data experimental studies reported however clear best report neural computing experiments may interested researchers particular nature iterative learning initial architecture backpropagation training multilayer perceptron precise reported result impossible outcome experimental reported results scientific method researchers popular neural computing paper address issue experiments based backpropagation training multilayer perceptrons although many results will applicable characteristics first attempt produce complete abstract specification neural computing experiment specification identify full range parameters needed support maximum use show absolute practice propose statistical framework support demonstrate framework empirical studies respect experimental controls validity implementations backpropagation algorithm finally suggest degree neural computing experiment estimated precision empirical results reported
living partially structured environment limitations classical reinforcement techniques paper propose unsupervised neural network allowing robot learn delayed reward robot task learn meaning order first introduce new neural conditioning rule probabilistic conditioning rule allowing test hypotheses visual categories movements given time second describe real experiment mobile robot propose neural architecture solve problem discuss difficulty build visual categories dynamically movements third propose use algorithm simulation order test give results different kind compare system adapted version qlearning algorithm finally conclude showing limitations approaches take account intrinsic complexity based image recognition
datadriven modeling synthesis present framework analysis synthesis based datadriven probabilistic inference modeling time series boundary conditions nonlinear mapping control data space inferred using general inference framework modeling resulting model used realtime synthesis sequences new input data
inference modelbased cluster analysis technical report department statistics university washington march
structural regression trees many realworld domains task machine learning algorithms learn theory predicting numerical values particular several standard test domains used inductive logic programming ilp concerned predicting numerical values examples relational mostly background knowledge however far ilp algorithm except one predict numbers cope background knowledge exception covering algorithm called paper present structural regression trees new algorithm applied class problems integrating statistical method regression trees ilp constructs tree containing formula conjunction literals node assigns numerical value provides comprehensible results purely statistical methods applied class problems ilp systems handle experiments several realworld domains demonstrate approach competitive existing methods indicating advantages predictive accuracy
practical bayesian framework networks quantitative practical bayesian framework described learning mappings feedforward networks framework makes possible objective comparisons solutions using alternative network architectures objective stopping rules network pruning growing procedures objective choice magnitude type weight decay terms additive large weights etc measure effective number parameters model estimates error network parameters network output objective comparisons alternative learning models splines radial basis functions bayesian evidence automatically occams razor models bayesian approach helps detect poor underlying assumptions learning models learning models matched problem good correlation generalisation ability paper makes use bayesian framework regularisation model comparison described paper bayesian framework due bayesian evidence obtained
exploiting choice instruction issue simultaneous multithreading processor simultaneous multithreading technique permits multiple independent threads issue multiple instructions cycle previous work demonstrated performance potential simultaneous multithreading based somewhat model paper show gains simultaneous multithreading achieved without extensive changes conventional superscalar either hardware structures sizes present architecture simultaneous multithreading achieves three goals minimizes impact conventional superscalar design minimal performance impact single executing alone achieves significant gains running multiple threads simultaneous multithreading architecture achieves instructions per cycle improvement superscalar similar hardware resources speedup enhanced advantage multithreading previously architectures ability issue threads efficiently using processor cycle thereby providing best instructions processor
revision logical domain theories theory revision problem problem best revising domain theory using information contained examples paper present approach theory revision problem propositional domain theories approach described called uses probabilities associated domain theory elements track proof theory allows measure precise role clause allowing desired derivation given example information used efficiently elements theory proved converge theory correctly examples shown experimentally fast accurate even theories
evaluation gaussian processes methods nonlinear regression
bayesian analysis mixtures unknown number components summary new methodology fully bayesian mixture analysis developed making use reversible markov chain monte carlo methods capable parameter subspaces corresponding different numbers components mixture sample full joint distribution unknown variables thereby generated used basis presentation many aspects posterior distribution methodology applied analysis univariate normal mixtures using hierarchical prior model offers approach dealing weak prior information avoiding mathematical using priors mixture context
analysis incremental variants policy iteration first steps toward understanding learning systems university college computer science technical report substantial contributions effort provided original interest questions whose comments greatly recent discussions also helpful impact work special also rich sutton influenced subject numerous ways work supported grant national science foundation air force
evolutionary module acquisition highlevel representations artificial life genetic algorithms search optimization evolutionary algorithm constructs recurrent neural networks technical report submitted ieee transactions neural networks special issue evolutionary programming
improving generalization active learning active learning differs learning examples learning algorithm assumes least control part input domain receives information situations active learning provably powerful learning examples alone giving better generalization fixed number training examples paper consider problem learning binary concept absence noise describe formalism active concept learning called selective sampling show may approximately implemented neural network selective sampling learner receives distribution information environment queries oracle parts domain considers useful test implementation called three domains observe significant improvement generalization
evaluating effects predicated execution branch prediction high performance architectures always deal impact branch operations designs deal problem move towards pipelines support multiple instruction issue branch prediction schemes often used negative impact branch operations allowing speculative execution instructions branch another technique eliminate branch instructions predication remove forward branch instructions instructions following branch predicate form paper analyzes variety existing predication models eliminating branch operations effect elimination branch prediction schemes existing processors including single issue architectures simple prediction mechanisms designs sophisticated branch predictors effect branch prediction accuracy branch penalty basic block size studied
rules precedents complementary rules precedents classification paper describes model rules precedents classification task model precedents assist rulebased reasoning abstract rule rules assist casebased reasoning case process case facts order increase similarity cases term reformulation process term whose precedents weakly match case terms whose precedents strongly match case fully exploiting requires control strategy characterized absence arbitrary ordering restrictions use rules precedents control strategy implemented domain law preliminary evaluation performance found good slightly better performance law students task case classified particular category relating description criteria category membership relate case category vary widely generality example consider classifying case category rule action fails use reasonable care failure cause general terms reasonable care result specific terms failure types used classification systems relate cases categories classification systems used precedents help match rules cases match difficult terms significant uncertainty whether match specific facts problem results generality gap separating abstract terms specific facts precedents term past cases term applied used bridge gap unlike rule precedents level generality cases generality gap exists precedents new cases precedents therefore reduce problem matching specific case facts terms problem matching two sets specific facts example depends whether activity determining whether particular case classified therefore requires matching specific facts case john driving office term activity gap generality case description abstract term makes match however match may much easier precedents term activity driving work activity driving activity case driving office closely matches driving work
average reward reinforcement learning introduce modelbased average reward reinforcement learning method called hlearning compare discounted adaptive realtime dynamic programming simulated robot scheduling task also introduce extension hlearning automatically explores parts state space always choosing greedy actions respect current value function show hlearning performs better original hlearning previously studied exploration methods random exploration
dynamic control genetic algorithms using fuzzy logic techniques paper proposes using fuzzy logic techniques dynamically control parameter settings genetic algorithms gas describe dynamic parametric uses fuzzy knowledgebased system control parameters introduce technique automatically designing tuning fuzzy system using gas results initial experiments show performance improvement simple static one dynamic parametric system designed automatic method demonstrated improvement application included design phase may indicate general applicability dynamic parametric wide range
learning linear sparse factorial codes previous work field algorithm described learning linear sparse codes trained natural images produces set basis functions spatially localized oriented note shows algorithm may interpreted within framework several useful insights emerge connection makes explicit relation statistical independence factorial coding shows formal relationship algorithm bell sejnowski suggests adapt parameters previously fixed report describes research done within center biological computational learning department brain cognitive sciences massachusetts institute technology research sponsored individual national research service award grant national science foundation contract asc9217041 award includes provided program
large methods approximate probabilistic inference rates convergence free parameter study belief networks binary random variables conditional probabilities depend weighted parents networks give efficient algorithms computing rigorous bounds marginal probabilities evidence output layer methods apply generally computation upper lower bounds generic transfer function conditional probability tables sigmoid also prove rates convergence accuracy bounds function network size results derived applying theory large weighted parents node network bounds marginal probabilities computed two contributions one assuming weighted fall near mean values assuming gives rise interesting tradeoff explanations evidence mean networks parents gap upper lower bounds sum two terms one order addition providing rates convergence large networks methods also yield efficient algorithms approximate inference fixed networks
learnability classes functions
efficient feature selection conceptual clustering feature selection proven valuable technique supervised learning improving predictive accuracy reducing number attributes considered task investigate potential similar benefits unsupervised learning task conceptual clustering issues raised feature selection absence class labels discussed implementation sequential feature selection algorithm based existing conceptual clustering system described additionally present second implementation employs technique improving efficiency search optimal description compare performance algorithms
upper bound loss approximate functions
symbolic learning vision possibilities robust flexible sufficiently general vision systems recognition description complex dimensional objects require adequate representations learning mechanisms paper briefly analyzes strengths weaknesses different learning paradigms symbol processing systems connectionist networks statistical syntactic pattern recognition systems possible candidates providing capabilities points several promising directions integrating multiple paradigms fashion towards goal
selforganizing feature map sequences selforganizing neural network sequence classification called described analyzed experimentally extends kohonen feature map architecture activation decay order create unique distributed response patterns different sequences yields extremely dense yet representations sequential input training iterations network proven successful mapping arbitrary sequences binary real numbers representations english words potential applications include isolated word recognition cognitive science models sequence processing
knowledge integration learning technical report abstract paper address problem acquiring knowledge integration aim construct integrated knowledge base several separate sources objective integration construct one system exploits knowledge available good performance aim paper discuss methodology knowledge integration present concrete results experiments performance integrated theory performance individual theories quite significant amount also performance much experiments repeated results indicate knowledge integration existing methods
evaluation selection biases machine learning introduction define term bias used machine learning systems importance automated methods evaluating selecting biases using framework bias selection search bias spaces recent research field machine learning bias
learning decision trees decision rules method initial results comparative study abstract standard approach determining decision trees learn examples approach decision tree learned difficult modify different decision making situations problems arise example attribute assigned node measured significant change costs measuring attributes frequency distribution events different decision classes attractive approach problem learn store knowledge form decision rules generate needed decision tree suitable given situation additional advantage approach facilitates building compact decision trees much simpler equivalent conventional decision trees compact trees decision trees may contain branches assigned set values nodes assigned derived attributes attributes logical mathematical functions original ones paper describes efficient method takes decision rules generated learning system builds decision tree optimizing given optimality criterion method work two modes standard mode produces conventional decision trees compact mode produces compact decision trees preliminary experiments shown decision trees generated decision rules conventional compact outperformed generated examples wellknown c45 program terms simplicity predictive accuracy
basis function networks global form local form appropriate constant factors technical report abstract smoothing radial basis functions studied extensively general smoothing basis functions sigmoidal proposed derive new classes order smoothing networks basis functions general transfer functions simple algebraic forms enable direct smoothness without need costly monte carlo tested sample problems compared quadratic weight decay new shown yield better generalization errors
reduced memory representations music address problem musical variation identification different musical sequences variations implications mental representations music according theories structural importance musical events mental representations may result production reduced memory representations musical study music performance produced variations analyses musical events across variations provided support account structural importance neural network trained produce reduced memory representations represented important events efficiently others agreement among network model predictions suggest across musical variation natural result mechanism producing memory representations
ensemble learning evidence maximization ensemble learning variational free energy minimization tool introduced neural networks hinton van learning described terms optimization ensemble parameter vectors optimized ensemble approximation posterior probability distribution parameters tool now applied variety statistical inference problems paper study linear regression model parameters hyperparameters demonstrate evidence approximation optimization regularization constants derived detail free energy minimization view point
adaptation self mcmc summary self mcmc tool constructing markov chain given stationary distribution constructing auxiliary chain stationary distribution elements auxiliary chain suitable random number times resulting chain stationary distribution article provide generic adaptation scheme algorithm adaptive scheme use knowledge stationary distribution far update course simulation method easy implement often leads considerable improvement obtain theoretical results adaptive scheme proposed methodology illustrated number realistic examples bayesian computation performance compared available mcmc techniques one applications develop nonlinear dynamics model modeling relationships
conceptual analogy conceptual analogy approach integrates memory organization based prior experiences analogical reasoning implemented tested support design process building engineering number features distinguish standard approaches cbr first automatically knowledge needed support design tasks complex case representations relevance object features relations proper attributevalue representations prior secondly effectively determines similarity complex case representations terms implemented integrated highly interactive adaptive system architecture allows incremental knowledge acquisition user support paper surveys basic assumptions psychological results influenced development knowledge representation employed needed integrate memory organization analogical reasoning
multipath execution opportunities limits even sophisticated techniques necessarily suffer even relatively small rates performance substantially processors paper investigate schemes improving performance face imperfect branch predictors processor simultaneously execute code taken outcomes branch paper presents data regarding limits multipath execution considers needs multipath execution discusses various dynamic schemes likelihood branch evaluations consider executing along several paths using paths relatively simple confidence predictor multipath execution speedups compared case average speedup suite associated increases requirements surprising less expected result significance separate stack path overall results indicate multipath execution offers significant improvements performance especially useful combined multithreading hardware costs approaches
robustness analysis bayesian networks global paper presents algorithms robustness analysis bayesian networks global robust bayesian inference calculation bounds posterior values given perturbations probabilistic model present algorithms robust inference including expected utility expected value variance bounds global perturbations modeled constant density ratio constant density bounded total variation classes distributions university
adaptive state space adding removing neurons paper describes control system mobile robot based local sensor data robot avoid feedback control system external reinforcement signal indicates whether reinforcement learning scheme used find correct mapping input sensor space output signal space adaptive scheme introduced discrete division input space built scratch system
evaluation ordering rules extracted feedforward networks rules extracted trained feedforward networks used explanation validation network output decisions paper introduces rule evaluation ordering mechanism orders rules extracted feedforward networks based three performance measures detailed experiments using three rule extraction techniques applied breast cancer database illustrate power proposed methods moreover method integrating output decisions extracted rulebased system corresponding trained network proposed integrated system provides improvements
highlevel representations
evolutionary algorithm constructs recurrent neural networks standard methods inducing structure weight values recurrent neural networks fit assumed class architectures every task necessary interactions network structure function understood evolutionary computation includes genetic algorithms evolutionary programming search method shown promise complex tasks paper argues genetic algorithms inappropriate network acquisition describes evolutionary program called simultaneously structure weights recurrent networks algorithms empirical acquisition method allows emergence complex behaviors topologies potentially artificial constraints standard network induction methods
spline smoothing data applications association
using genetic encoding neural networks evolve finitestate behaviour new mechanism genetic encoding neural networks proposed based structure biological dna mechanism allows aspects network structure including number nodes connectivity evolved genetic algorithms effectiveness encoding scheme demonstrated object recognition task requires artificial whose behaviour driven neural network develop highlevel finitestate exploration discrimination strategies task requires solving problem developing functional understanding effects movement sensory input
smoothing spline anova bayesian confidence intervals appear computational graphical statistics study multivariate smoothing spline estimate function several variables based anova decomposition main effect functions one variable interaction functions two variables etc derive bayesian confidence intervals components decomposition demonstrate even multiple smoothing parameters efficiently computed using available code originally designed just compute estimates small monte carlo study see closely actual properties confidence intervals match nominal confidence levels analyze data function using polynomial spline main effects model
structured machine learning soft classification smoothing spline anova tuning testing appear eds advances neural information processing systems san morgan paper written limitation presented session conference neural information processing synthetic november supported nsf grants national eye grants
improving quality automatic dna sequence assembly using classifications largescale projects use automatic programs aid determination dna sequences require substantial transform size projects increases becomes essential improve quality automated may reduced current technology uses base calls made dna run present new representation trace data associated individual base calls representation used assembly improve quality demonstrate one use suboptimal data results significant improvement quality subsequent
techniques extracting instruction level parallelism mimd architectures extensive research done extracting parallelism single instruction stream processors paper presents results investigation ways modify mimd architectures allow extract instruction level parallelism achieved current superscalar machines new architecture proposed utilizes advantages multiple instruction stream design addressing limitations mimd architectures performing ilp operation new code scheduling mechanism described support new architecture partitioning instructions across multiple processing elements order exploit level parallelism
multiple instruction stream computer paper describes single multiple instruction stream computer capable extracting instruction level parallelism broad spectrum programs architecture uses multiple asynchronous processing elements separate program executed parallel integrates message system level processor design facilitate low latency communication approach allows increased machine parallelism minimal code expansion provides alternative approach single instruction stream machines superscalar
optimal navigation world paper define examine two versions bridge problem first variant bridge problem model agent transitions priori probabilities transitions second variant transitions fixed probability time step problems applicable planning uncertain domains routing computer network show agent act models reduction markov decision processes describe methods solving note methods intractable reasonably problems finally suggest programming method value function approximation types models
eeg signal classification different signal representations large number hidden units several mental states reliably distinguished recognizing patterns eeg device like mental states article report study comparing four representations eeg signals classification neural network sigmoid activation functions neural network implemented server processor simd architecture adaptive solutions decrease training time
learning conjunctions malicious noise show learn presence malicious noise distribution product distribution show results apply product distributions wide class distributions
sample complexity learning recurrent perceptron mappings recurrent perceptron classifiers generalize classical perceptron model take account correlations among input arise linear digital filtering paper provides tight bounds sample complexity associated fitting models experimental data
neural net architectures temporal sequence processing present general taxonomy neural net architectures processing timevarying patterns taxonomy many existing architectures literature points several promising architectures yet examined architecture processes timevarying patterns requires two distinct components shortterm memory holds relevant past events uses shortterm memory classify predict taxonomy based characterization shortterm memory models along dimensions form content experiments predicting future values financial time series exchange rates presented using several alternative memory models results experiments serve baseline sophisticated architectures compared neural networks proven promising alternative traditional techniques nonlinear temporal prediction tasks however temporal prediction particularly challenging problem conventional neural net architectures algorithms suited patterns vary time use neural nets structural pattern recognition task collection semantic presented network network must input feature pattern one classes example network might trained classify animal species based set attributes describing living network trained recognize visual patterns twodimensional array tasks network presented relevant information simultaneously contrast temporal pattern recognition involves processing patterns evolve time appropriate response particular point time depends current input potentially previous inputs illustrated figure shows basic framework temporal prediction problem assume time discrete steps assumption many time series interest discrete continuous series sampled fixed interval input time univariate series input
selforganizing feature map model artificial neural network model mental built test computationally whether consist separate feature maps different semantics connected ordered pathways model semantic feature maps formed unsupervised process based symbol meaning model organized various damage system simulated resulting similar observed human patients
theory synaptic plasticity visual cortex
natural language processing neural networks
beyond cognitive map contributions computational theory navigation
neural nets systems models controllers neural nets models dynamical paper briefly surveys recent results relevant
learning decomposition paper describe new selforganizing decomposition technique learning highdimensional mappings problem decomposition performed manner resulting equally approximated method combines unsupervised learning scheme feature maps nonlinear approximator backpropagation resulting learning system stable effective changing environments backpropagation much powerful extended feature maps proposed extensions method give rise active exploration strategies autonomous agents unknown environments general purpose method will demonstrated mathematical function approximation
feature subset selection search probabilistic estimates irrelevant features weakly relevant features may reduce comprehensibility accuracy concepts induced supervised learning algorithms formulate search feature subset abstract search problem probabilistic estimates searching space using evaluation function random variable requires trading accuracy estimates increased state exploration show recent feature subset selection algorithms machine learning literature fit search problem simple hill climbing approaches conduct small experiment using search technique
parallel genetic programming simd architectures field genetic programming application increases need parallel implementations becomes necessary system recently presented koza one parallel implementations today implementation proposed parallel using simd architecture except approach although others exploited one reason apparent difficulty dealing parallel evaluation different single instruction executed time every processor aim paper present implementation parallel simd system processor efficiently evaluate different implemented approach computer will present results extent simd machines like available offer cycles experimentation useful approach
unified analysis reinforcementlearning algorithms reinforcement learning problem generating optimal behavior sequential decisionmaking environment given opportunity interacting many algorithms solving reinforcementlearning problems work computing improved estimates optimal value function extend prior analyses reinforcementlearning algorithms present powerful new theorem provide unified analysis reinforcementlearning algorithms usefulness theorem lies allows asynchronous convergence complex reinforcementlearning algorithm proven simpler algorithm converges illustrate application theorem analyzing convergence qlearning modelbased reinforcement learning qlearning updates qlearning markov games reinforcement learning
using path diagrams structural equation modelling tool
analyzing data independent component analysis image sensors provide images large number spectral channels per enable information different within obtained problem may viewed specific case blind source separation problem data consists mixed signals case goal determine contribution without prior knowledge technique independent component analysis ica assumes spectral components close statistically independent provides unsupervised method blind source separation introduce contextual ica context data analysis apply method data mixed real image
incremental methods computing bounds partially observable markov decision processes partially observable markov decision processes pomdps allow one model complex dynamic decision control problems include action outcome uncertainty imperfect control problem formulated dynamic optimization problem value function combining costs rewards multiple steps paper propose analyse test various incremental methods computing bounds value function control problems infinite discounted horizon criteria methods described tested include novel incremental versions linear method simple lower bound method updates work arbitrary points belief space enhanced various heuristic point selection strategies also introduced new method computing initial upper bound fast bound method method able improve significantly standard commonly used upper bound computed method quality resulting bounds tested navigation problem states actions observations
bayesian nonlinear modelling prediction competition energy prediction competition involved prediction series building energy series environmental input variables nonlinear regression using neural networks popular technique modeling tasks since obvious large inputs appropriate preprocessing inputs best viewed regression problem many possible input variables may actually irrelevant prediction output variable finite data set will show random correlations irrelevant inputs output conventional neural network even regularisation weight decay will set coefficients inputs zero thus irrelevant variables will models performance automatic relevance determination model prior regression parameters concept relevance done simple soft way introducing multiple regularisation constants one associated input using bayesian methods regularisation constants inputs automatically inferred large inputs significant overfitting
integration casebased reasoning neural networks approaches classification several different approaches used describe concepts supervised learning tasks paper describe two approaches incremental neural networks casebased reasoning approaches show improve neural network model specific instances cbr memory system leads propose hybrid model classification
code scheduling multiple instruction stream architectures extensive research done extracting parallelism single instruction stream processors paper presents investigation ways modify mimd architectures allow extract instruction level parallelism achieved current superscalar machines new architecture proposed utilizes advantages multiple instruction stream design addressing limitations mimd architectures performing ilp operation new code scheduling mechanism described support new architecture partitioning instructions across multiple processing elements order exploit level parallelism
performance prediction method parallel neural network simulations performance prediction method presented indicating performance range mimd parallel processor systems neural network simulations total execution time parallel application modeled sum calculation communication times method based times measured one processor one communication link performance speedup efficiency predicted larger processor system applying two popular neural networks backpropagation kohonen selforganizing feature map decomposed system agreement model measurements within
learning classification trees algorithms learning classification trees artificial intelligence statistics many years paper outlines tree learning algorithm derived using bayesian statistics introduces bayesian techniques splitting smoothing tree averaging splitting rule similar information gain smoothing averaging replace pruning comparative experiments minimum encoding approach cart show full bayesian algorithm produce paper final draft submitted statistics computing journal version changes appeared pages accurate predictions versions approaches though computational price
issues evolutionary robotics version paper appears proceedings second international conference simulation adaptive behaviour mit press cambridge
learning sorting decision trees pomdps pomdps general models sequential decisions actions observations probabilistic many problems interest formulated pomdps yet use pomdps limited lack effective algorithms recently change number problems robot navigation planning beginning formulated solved pomdps advantage approach semantics ability produce principled solutions integrate physical information actions paper approach context two learning tasks learning sort vector numbers learning decision trees data problems formulated pomdps solved general algorithm main lessons results use suitable heuristics representations allows solution sorting classification pomdps nontrivial sizes quality resulting solutions competitive best algorithms aspects decision tree learning test misclassification costs noisy tests missing values naturally
abstract given arbitrary learning situation difficult determine appropriate learning strategy goal research provide general representation processing framework introspective reasoning strategy selection learning framework introspective system perform reasoning task system also records trace reasoning along results reasoning reasoning failure occurs system applies introspective explanation failure order understand error knowledge base knowledge structure called pattern used explain conclusions derived conclusions fail reasoning represented explicit declarative manner system examine reasoning analyze reasoning failures identify needs learn select appropriate learning strategies order learn required knowledge without
abstract describe ongoing project develop adaptive training system dynamically models students learning processes provide specialized adapted students knowledge state learning style student modeling component uses machine learning techniques students transition learning methods student used reach current knowledge state comparing students solution trace expert solution generating plausible hypotheses errors student made casebased approach used generate hypotheses applying analogy student expert models use representation includes abstract concepts relationships strategies problem solving fuzzy methods used represent uncertainty student model paper describes design gives detailed example system model student typical session domain use example level
automated model selection many algorithms parameters set user machine learning algorithms parameter setting nontrivial task influence knowledge model algorithm parameter values usually set approximately according characteristics target problem obtained different ways usual way use background knowledge target problem perform testing experiments paper presents approach automated model selection based local optimization uses empirical evaluation constructed concept description guide search approach tested using inductive concept learning system
inductive logic programming inducing logic programs without explicit negative examples paper presents method learning logic programs without explicit negative examples exploiting assumption output mode supplied target predicate training input assumed outputs outputs generated incomplete program implicitly represent negative examples however large numbers negative examples never need generated method incorporated two ilp systems use background knowledge tests two natural language acquisition tasks mapping learning illustrate advantages approach
instancebased learning methods explicitly data receive usually training phase prediction time perform computation take query search database similar build online local model local average local regression predict output value paper review advantages instance based methods autonomous systems also note cost slow computation database grows large present evaluate new way database new algorithm maintains advantages instancebased learning earlier attempts combat cost instancebased learning explicit data applicable instancebased predictions based small number near neighbors explicit training phase form data structure approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously permits query database conventional linear search greatly reduced computational cost
approach clustering paper introduce new clustering algorithm pattern cluster represented collection fuzzy initially number calculated represent pattern samples algorithm applies multiresolution techniques combine manner scheme found yield encouraging results realworld clustering problems
induction decision trees paper introduces randomized technique partitioning examples using hyperplanes standard decision tree techniques id3 partition set points axisparallel hyperplanes method contrast attempts find hyperplanes orientation purpose general technique find smaller equally accurate decision trees created methods tested algorithm real simulated data found cases produces surprisingly small trees without predictive accuracy small trees allow turn obtain simple qualitative descriptions problem domain
classification empirical evaluation hybrid genetic decision tree induction algorithm paper introduces new algorithm classification uses genetic algorithm evolve population biases decision tree induction algorithm fitness function genetic algorithm average cost classification using decision tree including costs tests features measurements costs classification errors compared three algorithms classification also c45 without cost five algorithms evaluated empirically five realworld medical datasets three sets experiments performed first set examines baseline performance five algorithms five datasets performs significantly better second set tests robustness variety conditions shows maintains advantage third set search bias space discovers way improve search
understanding musical sound forward models physical models
mathematical programming neural networks paper role mathematical programming particularly linear programming training neural networks neural network description given terms separating input space suggests use linear programming determining standard description terms mean square error output space also given leads use minimization techniques training neural network linear programming approach demonstrated brief description system breast cancer diagnosis use last four years major medical
understanding creativity casebased approach existing standard casebased reasoning cbr systems investigate make systems creative mean creative paper discusses three research goals understanding creative processes better investigating role cases cbr creative problem solving understanding framework supports interesting kind casebased reasoning addition discusses methodological issues study creativity particular use cbr research paradigm exploring creativity
stochastic decomposition dna sequences using hidden markov models work presents application machine learning characterizing important property natural dna sequences compositional compositional segments often correspond biological units taking account successful recognition functional features dna sequences especially genes present technique dna segmentation using hidden markov models dna sequence represented chain homogeneous segments described one statistically hidden states whose form firstorder markov chain technique used describe compare completely results indicate existence separated states gives support theory also explore models likelihood landscape analyze dynamics optimization process thus addressing problem reliability obtained optima efficiency algorithms
weight matrix weight modifications traditional neural nets computed algorithms without exception previous weight change algorithms many specific limitations principle possible overcome limitations algorithms allowing neural nets run improve weight change algorithms paper demonstrates answer principle derive initial sequence learning algorithm recurrent network weight matrix terms activations uses input output units observing errors explicitly analyzing modifying weight matrix including parts weight matrix responsible analyzing modifying weight matrix result first introspective neural net explicit potential control adaptive parameters algorithm high computational complexity per time step independent sequence length number connections another high number local minima complex error surface purpose paper however efficient introspective weight change algorithm show algorithms possible
memory paper discusses problem implement mapping within connectionist models traditional symbolic approaches work explicit representation alternatives via stored links implicitly algorithms classical pattern association models issue generating multiple outputs single input pattern recent research recurrent networks promising field clearly focused upon goal paper define memory several possible variants discuss utility general cognitive modeling extend sequential networks fit task perform several initial experiments demonstrate feasibility concept
development neural circuits visual image stabilization eye movements human visual systems maintain stable internal representation scene even though image changing eye movements stabilization theoretically dynamic shifts receptive field neurons visual system paper examines neural circuit learn generate shifts shifts controlled eye position signals movement image caused eye movements development neural circuit van modeled using connections connections gated signals indicate direction eye position signals simulations neural model sequences stimuli appropriate eye position signals initially
machine learning methods international conflict databases case study predicting outcome paper tries identify rules factors predictive outcome international conflict management attempts use c45 advanced machine learning algorithm generating decision trees prediction rules cases database results show simple patterns rules often also reliable complex rules simple decision trees able improve correctly predicting outcome conflict management attempt suggests per results achieved far
sequential technique multimodal function optimization technical report part article may commercial purposes abstract technique described allows function optimization methods extended efficiently optima multimodal problems describe algorithm based traditional genetic algorithm involves uses knowledge gained one iteration avoid subsequent iterations regions problem space solutions already found achieved applying fitness function raw fitness function fitness values regions problem space solutions already found consequently likelihood discovering new solution iteration dramatically increased technique may used various styles optimization methods simulated annealing effectiveness algorithm demonstrated number multimodal test functions technique least fast fitness sharing methods provides speedup problem optima depending value convergence time complexity
learning examples agent teams concept international journal pattern recognition also available report
robust value function approximation working computing accurate value function key paper examine operate approximating asynchronous value iteration note important class discrete acyclic stochastic tasks value iteration inefficient compared algorithm essentially performs one instead many working goal question address paper whether analogous algorithm used large stochastic state spaces requiring function approximation present algorithm analyze give comparative results several domains state using solve mdps either special classes quite inefficient since performs entire space whereas useful improving values fact classical algorithms problem classes compute efficiently explicitly working deterministic class algorithm acyclic class first mdp producing linear ordering states every state states runs list performing one per state worstcase bounds deterministic domains states although presents deterministic acyclic problems applies
transformation system interactive reformulation design optimization strategies automatic design optimization highly sensitive problem formulation choice objective function constraints design parameters dramatically impact computational cost optimization quality resulting design best formulation varies one application another design will usually know best formulation advance order address problem developed system supports interactive formulation testing reformulation design optimization strategies system includes language representing optimization strategies language allows define multiple stages optimization using different approximations objective constraints different abstractions design space also developed set transformations strategies represented language transformations approximate objective constraint functions search spaces optimization process multiple stages system applicable principle design problem expressed terms constrained optimization however expect system useful design algebraic ordinary differential equations tested system problems design engine design report experimental results demonstrating reformulation techniques significantly improve performance automatic design optimization research demonstrates reformulation methodology combines symbolic program transformation numerical experimentation important first step research program aimed entire strategy formulation process fully accepted research engineering design
segmentation classification combined optical classification performance neural network combined scene carried different combinations data either raw using available training test sets created training sets used learning test sets used neural network different combinations evaluated
learning markov chains variable memory length noisy output problem modeling complicated data sequences dna speech often arises practice algorithms select hypothesis within model class assuming observed sequence direct output underlying generation process paper consider case output noisy channel observation particular show class markov chains variable memory length learning affected factors despite still small practical cases markov models variable memory length probabilistic finite automata introduced learning theory also described polynomial time learning algorithm present modification algorithm uses sample knowledge noise structure algorithm still noise known exactly good estimation available finally experimental results presented removing noise corrupted english text measure performance learning algorithm affected size noisy sample noise rate
distribution category users guide parallel genetic algorithm library
intelligent agents tasks approach present evaluate implemented system rapidly easily build intelligent software agents tasks design around two basic functions given highly accurate functions standard heuristic search lead efficient retrieval useful information approach allows users systems behavior providing approximate advice functions advice neural network implementations two functions subsequent web links retrieved pages user provide respectively used refine link functions hence architecture provides agent programming languages systems solely learn user preferences users pages describe internal representation web pages major predicates advice language advice neural networks mechanisms refining advice based subsequent feedback also present case study provide simple advice generalpurpose system empirical study demonstrates approach leads effective leading commercial web search site
dynamically concepts accommodate changing contexts concept learning objects domain together based similarity determined attributes used describe existing concept learners require set attributes known advance presented learning begins additionally systems possess mechanisms attribute set concepts learned consequently attribute set relevant task concepts used must supplied learning turn usefulness concepts limited task attributes originally selected order efficiently accommodate changing contexts concept learner must able set without prior knowledge domain introduce notion dynamic modification attribute set used describe instances problem domain implemented capability concept learning system evaluated along several dimensions using existing concept formation system
bayesian mixture modeling monte carlo simulation shown bayesian inference data modeled mixture distribution performed via monte carlo simulation method exhibits true bayesian predictive distribution implicitly integrating entire underlying parameter space infinite number mixture components without difficulty using prior distribution mixing selects reasonable subset components explain finite training set need decide correct number components thereby avoided feasibility method shown empirically simple classification task
machine learning efficient reinforcement learning evolution article presents new reinforcement learning method called adaptive neuroevolution evolves population neurons genetic algorithms form neural network capable performing task evolution cooperation specialization results fast efficient genetic search convergence suboptimal solutions problem formed effective networks times faster adaptive heuristic times faster qlearning neuroevolution approach without loss generalization efficient learning combined domain assumptions make promising approach broad range reinforcement learning problems including many realworld applications
probabilistic evaluation sequential plans causal models hidden variables paper concerns probabilistic evaluation plans presence variables plan consisting several concurrent sequential actions establish graphical criterion recognizing effects given plan predicted observations measured variables criterion satisfied expression provided probability plan will achieve specified goal
control flow prediction dynamic ilp processors introduce technique enhance ability dynamic ilp processors exploit executed parallelism existing branch prediction mechanisms used establish dynamic window ilp extracted limited abilities create large accurate dynamic window large number instructions window every cycle iii multiple branches control flow graph per prediction introduce control flow prediction uses information control flow graph program overcome limitations discuss information present control flow graph represented using hardware using control flow tables control flow prediction evaluate potential control flow prediction abstract machine dynamic ilp processing model results indicate control flow prediction powerful effective assist hardware making run time decisions program control flow
mean field theory sigmoid belief networks develop mean field theory sigmoid belief networks based ideas statistical mechanics mean field theory provides tractable approximation true probability distribution networks also yields lower bound likelihood evidence demonstrate utility framework benchmark problem statistical pattern classification handwritten digits
statistical approach solving ebl utility problem many learning experience systems use information extracted problem solving experiences modify performance element new element solve similar problems efficiently however transformations improve performance one set problems performance sets new always better original depends distribution problems therefore performance element whose expected performance distribution optimal unfortunately actual distribution needed determine element optimal usually known moreover task finding optimal element even distribution intractable interesting spaces elements paper presents method problems using set samples estimate unknown distribution using set transformations local optimum process based mathematically rigorous form utility analysis particular uses statistical techniques determine whether result proposed transformation will better original system also present efficient way implementing learning system context general class performance elements include empirical evidence approach work effectively much work performed university toronto supported institute robotics intelligent systems operating grant national science engineering research also many helpful comments
modular qlearning architecture manipulator task decomposition data storage model compositional qlearning modular approach learning perform composite tasks made several elemental tasks reinforcement learning skills acquired performing elemental tasks also applied solve composite tasks individual skills right act skills included decomposition composite task extend original concept two ways general reward function agent one use architecture acquire skills performing composite tasks simulated manipulator large state action spaces manipulator nonlinear dynamical system require specific positions fast function approximation achieved use array model controller research involve scaling machine learning methods especially reinforcement learning autonomous robot control interested function approximators suitable reinforcement learning problems large state spaces model controller permit fast online learning good local generalization addition interested task decomposition reinforcement learning use hierarchical modular function approximator architectures examining effectiveness modified hierarchical mixtures experts jordan approach reinforcement learning since original developed mainly supervised learning batch learning tasks domain knowledge reinforcement learning agents important way extending capabilities default policies specified domain knowledge also used restrict size space leading faster learning investigating use qlearning planning tasks using classifier system encode necessary rules jordan hierarchical mixtures experts algorithm technical report mit computational cognitive science
hyperplane dynamics means understanding backpropagation learning network plasticity processing performed feedforward neural network often interpreted use decision hyperplanes layer adaptation process however normally explained using gradient descent error landscape paper dynamics decision hyperplanes used model adaptation process analogy drawn dynamics hyperplanes determined interaction forces hyperplanes represent patterns relaxation system determined increasing hyperplane used dynamics learning way explaining learning certain local minima furthermore network plasticity introduced dynamic property system reduction necessary consequence information storage hyperplane used explain avoid trained networks
modifications recursive memory presented allow store complex data structures previously reported modifications include adding extra layers networks employing integer rather realvalued representations weights representations resulting system tested data set syntactic trees extracted
draft efficient boosting algorithm combining preferences problem combining preferences arises several applications combining results different search work describes efficient algorithm combining multiple preferences first give formal framework problem describe analyze new boosting algorithm combining preferences called also describe efficient implementation algorithm restricted case discuss two experiments carried assess performance first experiment used algorithm combine different search strategies query given domain task compare performance individual search strategies second experiment task specifically making present results comparing nearest neighbor regression algorithms
using decision trees improve casebased learning paper shows decision trees used improve performance casebased learning systems introduce performance task machine learning systems called prediction lies classification task performed decision tree algorithms flexible prediction task performed conceptual clustering systems prediction learning improve prediction specific set features known priori rather single known feature classification arbitrary set features conceptual clustering describe one task natural language processing present experiments compare solutions problem using decision trees hybrid approach combines two hybrid approach decision trees used specify features included knearest neighbor case retrieval results experiments show hybrid approach outperforms decision tree casebased approaches two casebased systems incorporate expert knowledge case retrieval algorithms results clearly indicate decision trees used improve performance systems without potentially expensive expert knowledge
factor analysis using learning technical report department statistics university toronto describe linear network models correlations realvalued variables using one realvalued hidden variables factor analysis model model seen linear version machine parameters learned using method learning primary generative model recognition model whose role values hidden variables based values variables generative recognition models learned phases using just rule learning procedure comparable simplicity version hebbian learning produces somewhat different representation correlations terms principal components argue simplicity learning makes factor analysis alternative hebbian learning model cortical plasticity
using dirichlet mixture priors derive hidden markov models protein families bayesian method estimating distributions states hidden markov model hmm protein family multiple alignment family introduced method uses dirichlet mixture densities priors distributions mixture densities determined previously constructed hmms multiple shown bayesian method improve quality hmms produced small training sets specific experiments reported priors shown produce hmms higher likelihood unseen data fewer false false database search task
get free lunch simple cost model machine learning applications paper proposes simple cost model machine learning applications based notion net present value model extends models used attempts answer question given machine learning system now prototype stage models inputs systems matrix flow matrix application cost per decision cost system rate return like convex method present model used decisionmaking even input variables known exactly despite simplicity number nontrivial consequences example free lunch theorems learning theory longer apply
aspects graphical models connected causality paper demonstrates use graphs mathematical tool formal language processing causal information statistical analysis show complex information external organized represented graphical representation used facilitate quantitative predictions effects first review markovian account causation show directed acyclic graphs offer scheme representing conditional independence assumptions logical consequences assumptions introduce account causation show defines simple transformation probability distribution will change result external system using transformation possible quantify data effects external specify conditions randomized experiments necessary finally paper offers graphical interpretation model causal effects demonstrates equivalence account causation tradeoffs two approaches deriving nonparametric bounds treatment effects conditions imperfect
soft vector quantization algorithm running title soft vector quantization section
evolving representations design cases use creative design casebased design adaptation design case new design requirements plays important role sufficient adapt predefined set design parameters task easily automated however creative changes required current systems provide limited success paper describes approach creative design adaptation based notion creativity goal oriented shift focus search process evolving representation used search space designs similar example case lie focus search focus used starting point create new designs
nonlinear models time series using mixtures experts consider novel nonlinear model time series analysis study model theoretical aspects practical applicability architecture model demonstrated sufficiently rich sense approximating unknown functional forms yet simple intuitive characteristics linear models comparison established nonlinear models will theoretical issues prediction results benchmark time series computer generated data sets efficient estimation algorithms seen applicable made possible mixture based structure model large sample properties estimators discussed specified settings also demonstrate inference data structure may made model resulting better intuitive understanding structure performance model
learning concepts coverage learning algorithm number concepts learned algorithm samples given size paper whether good learning algorithms designed maximizing coverage paper extends previous upper bound coverage boolean concept learning algorithm describes two coverage approaches upper bound experimental measurement coverage id3 algorithms shows coverage far bound analysis shows although learns many concepts seem interesting concepts hence coverage maximization alone appear yield learning algorithms paper concludes definition coverage within bias suggests way coverage maximization applied weak preference biases
analyzing gas using markov models ordered states previous workshop presented initial results using markov models analyze behavior genetic algorithms gas used function paper states markov model ordered via simple mathematically ordering used initially paper explore alternative states based interesting semantic properties average fitness degree average attractive force etc also explore techniques reducing size state space analysis markov models provides new insights behavior gas general particular
emergent behaviour coevolutionary design important aspect creative design concept emergence though emergence important mechanism either understood limited domain considering definitions emergent behaviour artificial life alife research community new insights proposed computational technique called evolving representations design genes extended emergent behaviour demonstrate emergent behaviour coevolutionary model design coevolutionary approach design allows solution space structure space evolve response problem space behaviour space since behaviour space now active behaviour may emerge new structures end design process paper emergent behaviour identified using technique plan example extended demonstrate behaviour emerge coevolutionary design process
learning noisy incomplete examples investigate learnability pac model data used learning attributes labels either corrupted incomplete order prove main results define new complexity measure statistical query learning algorithms view algorithm maximum queries algorithm number input bits query depends show restricted view algorithm class general sufficient condition learnability models attribute noise missing attributes show since algorithms question statistical also simultaneously tolerate classification noise classes results hold therefore learned simultaneous attribute noise classification noise include dnf representations conjunctions relevant variables uniform distribution decision lists noise models first pac models training data attributes labels may corrupted random process previous researchers shown class learnable attribute noise attribute noise rate known exactly show attribute noise learnability results either without classification noise also hold exact noise rate known provided learner instead good approximation noise rate addition show results also hold just one noise rate distinct noise rate attribute results learning random covering require learner even approximation covering rate addition hold setting distinct covering rates attribute finally give lower bounds number examples required learning presence attribute noise covering
finding genes dna hidden markov model study describes new hidden markov model hmm system dna sequences exons introns regions separate hmm modules designed trained specific regions dna exons introns regions splice sites models together form biologically feasible topology integrated hmm trained set dna sequences tested using separate set sequences resulting hmm system called obtains overall accuracy test data total bases correctly correlation using test exact prediction correctly coding exons exons predicts exactly correct results compare favorably best previous results gene structure prediction demonstrate benefits using hmms problem
discovering structure multiple learning tasks algorithm recently increased interest lifelong machine learning methods transfer knowledge across multiple learning tasks methods repeatedly found outperform conventional learning algorithms learning tasks appropriately related increase robustness approaches methods desirable reason individual learning tasks order avoid arising tasks thus potentially paper describes algorithm clusters learning tasks classes related tasks new learning task first determines related task cluster exploits information task cluster empirical study carried mobile robot domain shows outperforms situations small number tasks relevant
generalized update belief change dynamic settings belief revision belief update proposed two types belief change different purposes belief revision intended capture changes agents belief state new information static world belief update intended capture changes belief response changing world argue belief revision belief update belief change involves elements present model generalized update allows updates response external changes agent prior beliefs model update combines aspects revision update providing realistic characterization belief change show certain assumptions original update postulates satisfied also demonstrate revision update special cases model way formally revision suitable static belief change
bayesian regression filters issue priors propose bayesian framework regression problems covers areas usually function approximation online learning algorithm derived solves regression problems kalman filter solution always improves increasing model complexity without risk overfitting infinite dimension limit approaches true bayesian posterior issues prior selection overfitting also discussed showing commonly held beliefs practical implementation simulations using popular available data sets used demonstrate method important issues concerning choice priors
performance analysis sparse connectionist networks report deals efficient mapping sparse neural networks develop parallel vector code sparse network determine performance three memory systems use code evaluate memory systems one will implemented prototype current design
two better one genotype neural networks nature genotype many organisms exhibits includes two copies every gene paper describe results simulations comparing behavior populations neural networks living fixed changing environments show create variability fitness population better environmental change consequence one obtain good results average peak fitness single population one choose population appropriate mutation rate results simulations parallel biological findings
experiments hybrid model learning sequential decision making
belief revision problem belief agent revise beliefs upon learning new active area research artificial intelligence many approaches belief change proposed literature goal introduce yet another approach examine carefully rationale underlying approaches already taken literature view methodological problems literature main message study belief change carefully must quite explicit underlying belief change process missing previous work focus postulates analysis shows must particular attention two issues often taken first model agents state use set beliefs structure ordering worlds use set beliefs language beliefs expressed second observations observations known true just latter case belief example argue even postulates called beyond agents beliefs include beliefs state external world issues observations arise particularly consider iterated belief revision must possibility revising
qualitative markov assumption implications belief change study belief change active area recent years two special cases belief change belief revision belief update studied detail roughly revision surprising observation previous beliefs update surprising observation world changed general expect agent making observation may revise earlier beliefs assume change world define novel approach belief change allows applying ideas probability theory qualitative settings key idea use qualitative markov assumption state transitions independent show recent approach modeling qualitative uncertainty using plausibility measures allows make qualitative markov assumption relatively straightforward way show markov assumption used provide attractive model
applying online search techniques reinforcement learning reinforcement learning frequently necessary approximation true optimal value function investigate benefits online search cases examine local searches agent performs lookahead search global searches agent performs search trajectory way current state goal state key success methods lies taking value function gives solution hard problem finding good trajectories every single state combining online search gives accurate solution easier problem finding good trajectory specifically current state
generalized queries probabilistic contextfree grammars probabilistic contextfree grammars provide simple way represent particular class distributions sentences contextfree language efficient algorithms particular queries calculating probability given sentence finding likely parse applied variety problems extend class queries several ways allowing missing sentence sentence supporting queries intermediate structure presence particular flexible conditioning variety types evidence method works constructing bayesian network represent distribution parse trees induced given network structure standard generated using similar approach present algorithm constructing bayesian networks show queries patterns queries network correspond interesting queries
qualitative probabilities default reasoning belief revision causal modeling paper presents recent developments toward formalism combines useful properties logic probabilities like logic formalism qualitative sentences provides symbolic deriving closed beliefs like probability permits express rules different levels beliefs response changing observations rules interpreted approximations conditional probabilities constraints worlds inferences supported unique priority ordering rules derived knowledge base ordering accounts rule interactions specificity facilitates construction coherent states beliefs practical algorithms developed analyzed testing consistency computing rule ordering queries observations incorporated using qualitative versions rule bayesian updating result coherent belief revision naturally finally causal rules interpreted markovian conditions world reflect causal constraints shown facilitate reasoning causal explanations actions change
using smoothing spline anova examine relation risk factors
integrating motor schemas reinforcement learning evolutionary architecture autonomous robots integrates motor control reinforcement learning robots utilizing benefit realtime performance motor schemas continuous dynamic environments taking advantage adaptive reinforcement learning groups motor schemas using embedded reinforcement learning modules coordination modules specific based situation learning occurs robot selects samples reinforcement signal time experiments robot soccer simulation illustrate performance utility system
cortical synchronization perceptual
local learning algorithm dynamic feedforward recurrent networks known learning algorithms dynamic neural networks nonstationary environments need global computations perform credit assignment algorithms either local time local space algorithms local time space usually deal hidden units contrast far now learning rules biological systems many hidden units local space time paper propose parallel online learning algorithm performs local computations yet still designed deal hidden units units whose past activations hidden time approach inspired idea classifier systems transformed run neural network fixed topology result feedforward recurrent neural system trying onto connections appropriate way simple experiments demonstrating algorithm reported
model creative understanding although creativity largely studied problem solving contexts creativity consists generative component component particular creativity essential part reading understanding natural language stories understanding process developed algorithm capable producing creative understanding behavior also created novel knowledge organization scheme assist process model creativity implemented integrated story analysis creativity reading system system models creative reading science stories
explaining recognition design creative designers often see solutions design problems objects often lead insight sometimes new functions purposes common design process interested modeling recognition solutions problems context creative design paper ability analyzing observations made context forms recognition propose computational model capture explore recognition based ideas dynamic memory situation assessment casebased reasoning
estimation probabilities attribute selection measures decision tree induction paper analyze two wellknown measures attribute selection decision tree induction index particular interested influence different methods estimating probabilities two measures results experiments show different measures obtained different probability estimation methods determine order attributes given node therefore determine structure constructed decision tree feature especially realworld applications several different trees often required
learning switching concepts consider learning situations function used classify examples may back small number different concepts course learning examine several models situations models made independent selection examples models single adversary controls concept example selection show relationships models schapire present polynomialtime algorithms learning two formulas model present model success popular competitive analysis used studying online algorithms describe randomized query algorithm two monotone competitive total number mistakes plus queries high probability bounded number plus fixed polynomial number variables also use notions described provide sufficient conditions learning class decision rule implies able learn class model probability
formal analysis case base retrieval case based systems typically retrieve cases case base applying similarity measures measures usually constructed manner report presents systematic construction similarity measures addition way design methodology similarity measures systematic approach facilitates identification opportunities case base retrieval
theory questions question
data structures genetic programming two techniques reducing run time real world applications software use memory must via data structures software using data must data structures implementation details achieve using abstract data structures records demonstrate genetic programming automatically implement simple abstract data structures considering detail task evolving list show general reasonably efficient implementations automatically generated simple model maintaining evolved code demonstrated using list problem much published work genetic programming evolves functions without learn patterns test data contrast human written programs often make extensive explicit use memory indeed memory form required programming system turing complete possible computable program system however memory make interactions parts programs much complex make produce programs despite shown automatically create programs explicitly use memory normal genetic programming considerable benefits found structured approach example koza shows introduction evolvable code modules automatically defined functions greatly help reach solution suggest corresponding structured approach use data will similarly significant advantage earlier work demonstrated genetic programming automatically generate simple abstract data structures namely evolve programs memory via simple data structures used external software without know implemented chapter shows possible evolve list data structure basic suggest three different ways implement list experiments show evolve implementation requires list components agree one implementation together section describes architecture including use multiple component fitness scoring measures aimed search evolved solutions described section section presents candidate model maintaining evolved software followed discussion learned conclusions drawn
networks real weights analog computational complexity contrast classical computational models models report abstract particular approach analog computation based dynamical systems type used neural networks research systems fixed structure invariant time corresponding number neurons allowed exponential time computation turn power however polynomialtime constraints limits capabilities though powerful turing machines similar restricted model shown polynomialtime equivalent classical digital computation previous work moreover precise correspondence nets standard circuits equivalent resources consequence one lower bound constraints compute relationship perhaps surprising since analog devices change manner input size note networks likely solve problems model implies almost complete standard polynomial hierarchy
approach total variation convergence mcmc algorithms introduce convergence diagnostic procedure mcmc operates estimating total variation distribution algorithm certain numbers iterations method advantages many existing methods terms applicability utility computational used assess convergence marginal joint posterior densities show applied two commonly used mcmc samplers gibbs sampler metropolis algorithm examples utility proposed diagnostic also limitations
independent component analysis data distance brain different eeg data collected point human includes activity generated within large brain area spatial eeg data involve significant time however suggesting independent component analysis ica algorithm bell sejnowski suitable performing blind source separation eeg data ica algorithm problem source identification source localization first results applying ica algorithm eeg potential data collected auditory detection task show ica training different random ica may used obvious eeg components line noise eye movements sources ica capable overlapping eeg phenomena including components separate ica channels eeg behavioral state using ica via changes amount residual correlation output channels
references elements solve difficult learning control problems simulation adaptive behavior pages number plus two limits capacity processing information psychological review towards compositional learning dynamic neural networks technical report encoding sequential structure simple recurrent networks technical report university computer science department
programming approach management
lookahead decision tree induction standard approach decision tree induction topdown greedy algorithm makes locally optimal decisions node tree paper study alternative approach algorithms use limited lookahead decide test use node systematically compare using large number decision trees quality decision trees induced greedy approach trees induced using lookahead main results experiments greedy approach produces trees just accurate trees produced much expensive lookahead step decision tree induction exhibits sense lookahead produce trees larger less accurate trees produced without
automatic feature extraction machine learning thesis presents machine learning model capable extracting discrete classes continuous valued input features done using inspired novel competitive classifier discrete classifications forward supervised machine learning model supervised learning model uses discrete classifications perhaps information available solve problem supervised learner generates feedback guide potentially useful classifications continuous valued input features two supervised learning models combined creating models simulated results analyzed based results several areas future research proposed
dynamically markov decision processes frequently called upon perform multiple tasks attention resource often know optimal solution task paper describe knowledge exploited efficiently find good solutions tasks parallel formulate problem dynamically merging multiple markov decision processes mdps composite mdp present new dynamic programming algorithm finding optimal policy composite mdp analyze various aspects algorithm every problem multiple tasks parallel attention resource running job shop must decide machines order robot must find intended simultaneously avoiding fixed mobile people still keep sufficiently frequently know perform task paper considers take information individual tasks combine efficiently find optimal solution entire set tasks parallel describe algorithm merging dynamically new tasks new job job shop online solution found ongoing set simultaneous tasks illustrate use simple merging problem
numerical taxonomy fitting tree metrics dimacs technical report
indexing plan derivations explanationbased analysis retrieval failures casebased planning provides way scaling planning solve large problems complex domains detailed search solution retrieval adaptation previous planning experiences general demonstrated improve performance generative planning however performance improvements provides dependent adequate problem similarity particular although may substantially reduce planning effort overall subject problem success depends retrieval errors relatively paper describes design implementation replay framework casebased planner extends current methodology incorporating explanationbased learning techniques allow explain learn retrieval failures techniques used refine case similarity response feedback decision made failure analysis used building case library addition cases large problems split stored single goal problems stored smaller cases fail full solution empirical evaluation approach demonstrates advantage learning retrieval failure
data exploration adaptive models
confidence estimation speculation control modern processors improve instruction level parallelism speculation outcome data control decisions predicted operations executed original predictions correct number ways processor resources used execution use speculation increases believe processors will need form speculation control balance benefits speculation possible activities confidence estimation one technique exploited speculation control paper introduce performance metrics compare confidence estimation mechanisms argue metrics appropriate speculation control compare number confidence estimation mechanisms focusing mechanisms small implementation cost gain benefit exploiting characteristics branch predictors clustering branches compare performance different confidence estimation methods using detailed simulations using simulations show improve confidence estimators providing better insight future comparing applying confidence estimators
relating relational learning algorithms relational learning algorithms special interest members machine learning community offer practical methods extending representations used algorithms solve supervised learning tasks five approaches currently explored address issues involved using relational representations paper surveys algorithms approaches summarizes empirical evaluations suggests potential directions future research
efficient learning boltzmann machines using linear response theory learning process boltzmann machines computationally expensive computational complexity exact algorithm exponential number neurons present new approximate learning algorithm boltzmann machines based mean field theory linear response theorem computational complexity algorithm number neurons absence hidden units show weights directly computed fixed point equation learning rules thus case need use gradient descent procedure learning process show solutions method close optimal solutions give significant improvement correlations play significant role finally apply method pattern task show good performance networks neurons
solving combinatorial optimization tasks reinforcement learning general methodology applied scheduling paper introduces methodology solving combinatorial optimization problems application reinforcement learning methods approach applied cases several similar instances combinatorial optimization problem must solved key idea analyze set training problem instances learn search control policy solving new problem instances search control policy goals finding solutions finding quickly results applying methodology scheduling problem show learned search control policy much effective best known search method based simulated annealing
learning curve bounds markov decision processes undiscounted rewards markov decision processes mdps undiscounted rewards represent important class problems decision control goal learning mdps find policy yields maximum expected return per unit time large state spaces computing averages directly feasible instead agent must estimate stochastic exploration state space case longer exploration times enable accurate estimates decisionmaking learning curve mdp measures agents performance depends allowed exploration time paper analyze learning curves simple control problem undiscounted rewards particular methods statistical mechanics used calculate lower bounds agents performance limit finite number time steps per policy evaluation size state space limit provide lower bound return policies appear optimal based imperfect statistics
comparison full partial predicated execution support ilp processors one effectively utilize predicated execution improve branch handling parallel processors although potential benefits predicated execution high tradeoffs involved design instruction set support predicated execution difficult one end design spectrum support full predicated execution requires increasing number source instructions full predicate support provides flexibility largest potential performance improvements end partial predicated execution support conditional moves requires little change existing architectures paper presents preliminary study address benefit full partial predicated execution support current compiler technology show compiler use partial full predication achieve speedup large programs details code generation techniques shown provide insight benefit partial full predication preliminary experimental results encouraging partial predication provides average performance improvement issue processor predicate support full predication provides additional improvement
power learning paper studies learning variant online learning model learner selects presentation order instances give tight bounds complexity learning concept classes dnf formulas rectangles results demonstrate number mistakes learning surprisingly small prove model learning powerful commonly used online query learning models next explore relationship complexity learning vapnikchervonenkis dimension finally explore relationship version space algorithm existence learning algorithms make mistakes supported part foundation grant nsf grant part research conducted author mit laboratory computer science supported nsf grant grant siemens corporation net address
result power abstract paper presents result computational power genetic algorithm context combinatorial optimization describe new genetic algorithm genetic algorithm prove class monotonic functions algorithm finds optimal solution exponential convergence rate analysis behavior algorithm main task reduces showing convergence probability distributions search space combinatorial structures optimal one take exponential convergence efficient algorithm although sampling theory needed better relate limit behavior actual behavior paper concludes discussion problems lie genetic algorithm
forecasting using nonlinear mixture experts paper study forecasting model based mixture experts predicting electric daily energy split task two parts using mixture experts first model predicts variables temperature degree cover viewed nonlinear regression model mixture gaussians using single neural network second model predicts evolution residual error first one viewed nonlinear model analyze splitting input space generated mixture experts model compare performance models used
neural computation provide cost function chain transition costs simple example given obtained optimal least error criterion decreases towards becomes worse cases poor example suggests need understand better circumstances td0 qlearning obtain satisfactory neural net compact cost function variation td0 also performs example
chain graphs learning
case representations casebased reasoning involves reasoning cases specific experience used solve problems use term representations capable relations two objects case allow set relations used vary case case allow set possible relations necessary describe new cases representations implemented example semantic networks lists concrete logic believe representations offer significant advantages thus investigating ways implement representations efficiently make casebased argument using examples two systems show representation supports two different kinds casebased planning two different domains discuss costs associated representations describe approach reducing costs
employing linear regression regression tree leaves
cortical emergence selforganization complex structures individual collective dynamics general theory quantitative results abstract human genotype represents ten binary whereas human brain contains times synapses brain structure essentially due selforganization selforganization relevant areas ranging design intelligent complex systems many brain structures emerge collective phenomenon dynamics stochastic dynamics neuronal action synaptic dynamics modeled local coupling dynamics type synaptic efficiency increases spiking neuron dynamics transformed collective dynamics theory models empirical findings topology preserving neuronal maps assumed selforganization suggested empirical observation reported shown stable due short range electrical chemical visual cortex neuronal stimulus orientation preference empirically measured orientation patterns determined equation equation orientation pattern emergence derived complex cognitive abilities emerge basic local synaptic changes emergent attention attention focus combination general theory presented emergence synaptic systems theory provides transformation collective dynamics used quantitative modeling empirical data
methodology evaluating theory revision systems results abstract theory revision systems learning systems goal making small changes original theory account new data measure distance two theories proposed measure corresponds minimum number operations level required transform one theory another computing distance original theory revised theory claim theory revision system makes revisions theory may evaluated present data using accuracy distance metric
dataset decomposition approach data mining machine discovery present novel data mining approach based decomposition order analyze given dataset method hierarchy smaller less complex datasets analyzed independently method experimentally evaluated realworld allocation dataset showing decomposition discover intermediate concepts relatively complex dataset datasets easy analyze derive classifier high classification accuracy also show human interaction positive effect comprehensibility classification accuracy
generalizing case studies case study empirical evaluations machine learning algorithms case studies evaluations multiple algorithms multiple databases authors case studies implicitly explicitly hypothesize pattern results often suggests one algorithm performs significantly better others limited small number databases investigated instead holds general class learning problems however hypotheses supported additional evidence leaves paper describes empirical method generalizing results case studies example application method yields rules describing algorithms significantly outperform others dependent measures advantages generalizing case studies limitations particular approach also described
error reduction learning multiple descriptions learning multiple descriptions class data shown reduce generalization error amount error reduction varies greatly domain domain paper presents novel empirical analysis helps understand variation hypothesis amount error reduction degree descriptions class make errors correlated manner present precise novel definition notion use data sets show amount observed error reduction correlated degree descriptions make errors correlated manner empirically show possible learn descriptions make less correlated errors domains many search evaluation measure information gain learning paper also presents results help understand multiple descriptions help irrelevant attributes much help large amounts class noise
appears working notes integrating multiple learned models improving scaling machine learning algorithms paper presents system combines artificial neural networks achieve expert level accuracy difficult scientific task recognizing images surface uses anns vary along two dimensions set input features used train number hidden units anns combined simply averaging output activations used classification module image analysis system called tool accuracy sensitivity specificity good human test suite also achieves best algorithmic accuracy images date
planning macro actions planning learning multiple levels temporal abstraction key problem artificial intelligence paper summarize approach problem based mathematical framework markov decision processes reinforcement learning conventional modelbased reinforcement learning uses primitive actions last one time step modeled independently learning agent generalized macro actions actions specified arbitrary policy way macro actions generalize classical notion macro operator closed loop uncertain variable macro actions needed represent actions lunch object paper generalizes prior work temporally abstract models sutton extends prediction setting include actions control planning define semantics models macro actions guarantees validity planning using models paper present new results theory planning macro actions illustrates potential advantages task
approximate statistical tests comparing supervised classification learning algorithms paper reviews five approximate statistical tests determining whether one learning algorithm outperforms another particular learning task tests compared experimentally determine probability detecting difference difference exists type error two statistical tests shown high probability type error certain situations never used tests test difference two test based taking several random splits third test test based crossvalidation exhibits somewhat probability type error test test shown low type error test new test based iterations crossvalidation experiments show test also acceptable type error paper also measures power ability detect algorithm differences exist tests test powerful test shown slightly powerful test choice best test determined computational cost running learning algorithm algorithms executed test test acceptable type error algorithms executed ten times test slightly powerful directly measures variation due choice training set
learning active classifiers many classification algorithms assign instance based description given even description incomplete contrast active classifier cost obtain values missing attributes upon class expected utility using active classifier depends cost required obtain additional attribute values penalty outputs classification paper considers problem learning nearoptimal active classifiers using variant pac model defining framework perhaps main contribution paper describe situation task achieved efficiently show task often intractable
elimination unifying framework probabilistic inference probabilistic inference algorithms finding explanation maximum hypothesis maximum expected utility updating belief algorithm called elimination principle common many algorithms literature relationship dynamic programming algorithms also present general way combining conditioning elimination within framework bounds complexity given algorithms function problems
bayesian model selection social research discussion article will published methodology peter cambridge adrian raftery professor statistics department university washington seattle research supported grant like thank michael long peter two detailed comments earlier version also grateful david john david david michael helpful discussions correspondence
parameters tradeoff paper propose family algorithms combining conditioning trade space time algorithms useful reasoning probabilistic deterministic networks optimization tasks analyzing problem structure will possible select spectrum algorithm best given
global conditioning probabilistic inference belief networks paper propose new approach probabilistic inference belief networks global conditioning simple generalization method conditioning show global conditioning conditioning thought special case method refined approach provides new opportunities parallel processing case sequential processing tradeoff time memory also show hybrid method others combining conditioning method viewed within framework exploring relationships methods develop unifying framework advantages approach combined successfully
associative memory using action potential dynamics collective properties feedback networks spiking neurons investigated special emphasis given potential computational role shown model systems neurons function associative memories two distinct levels first level binary patterns represented activity second level analog patterns encoded relative times individual underlying coding schemes may within network results suggest cortical neurons may perform broad spectrum associative computations far beyond scope traditional
simple schemes paper considers new method maintaining diversity creating standard evolutionary algorithm unlike methods concept distance individuals bits identify individual two variations method presented feasibility approach
local feature analysis general statistical theory object representation
data distributions regularization invariant learning pattern recognition machines provide constant output inputs transformed group desired invariances invariances achieved training data include examples inputs transformed elements corresponding targets cost function training include regularization term changes output input transformed group paper two approaches showing precisely sense cost function approximates result adding transformed examples training data cost function enhanced training set equivalent sum original cost function plus unbiased models reduces intuitively obvious choice term changes output inputs transformed group transformations regularization term reduces variance introduced training data correspondence provides simple bridge two approaches
exploiting causal independence bayesian network inference new method proposed exploiting causal exact bayesian network inference bayesian network viewed representing joint probability set conditional probabilities present notion causal independence enables one conditional probabilities combination even smaller factors consequently obtain joint probability new formulation causal independence specify conditional probability variable given parents terms associative operator sum max contribution start simple algorithm bayesian network inference given evidence query variable uses find posterior distribution query show algorithm extended exploit causal independence empirical studies based networks medical diagnosis show method efficient previous methods allows inference larger networks previous algorithms
cognitive modeling action selection learning goal develop hybrid cognitive model humans acquire skills complex cognitive tasks goal designing hybrid computational architectures navigation task requires coordination paper describe results directly fitting human execution data task next present empirically compare two methods modeling control knowledge acquisition reinforcement learning novel variant action models human learning task paper concludes experimental impact background knowledge system performance results indicate performance action models approach closely approximates rate human learning task reinforcement learning
improved noisetolerant learning generalized statistical queries statistical query learning model viewed tool creating demonstrating existence noisetolerant learning algorithms pac model complexity statistical query algorithm conjunction complexity simulating algorithms pac model noise determine complexity noisetolerant pac algorithms produced although roughly optimal upper bounds shown complexity statistical query learning corresponding noisetolerant pac algorithms optimal due inefficient simulations paper provide improved simulations new variant statistical query model order overcome improve time complexity classification noise simulation statistical query algorithms new simulation roughly optimal dependence noise rate also derive simpler proof statistical queries simulated presence classification noise proof makes fewer assumptions queries therefore allows one simulate general types queries also define new variant statistical query model based relative error show variant natural powerful standard additive error model demonstrate efficient pac simulations algorithms new model give general upper bounds learning relative error statistical queries pac simulation show statistical query algorithm simulated pac model malicious errors way pac algorithm roughly optimal malicious error rate sample complexity finally generalize types queries allowed statistical query model discuss advantages allowing generalized queries show results improved simulations also hold queries paper available center research computing technology division applied sciences university technical report
incremental reduced error pruning paper outlines problems may occur reduced error pruning relational learning algorithms efficiency new method incremental reduced error pruning proposed attempts address problems experiments show many noisy domains method much efficient alternative algorithms along gain accuracy however experiments show use algorithm domains require specific concept description
tutorial use tools simulations report contains description use tools neural network simulation programs using several sample technical details refer technical description
meter mechanism neural network learns metrical patterns one kind structure music language meter yet detailed measurements music speech show nested define metrical structure noisy sense kind system produce variable metrical take store particular metrical patterns longterm memory system developed network coupled oscillators produces metrical patterns addition beginning initial state biases learns patterns like patterns models general class learn musical patterns given way process speech extract appropriate model applicable metrical structure speech language metrical meter refers particular patterns time abstract description patterns potentially cognitive representation cases two hierarchical levels equally events occur periods characterizing levels usually hierarchy implied standard musical different levels kinds notes notes notes etc separating measures example basic meter individual sets three every third one stronger meter hierarchy consisting faster cycle level slower one measure level fast zero phase zero phase every third metrical systems like seem forms music around world often human speech however difficulty definition employs notion integer since data music speech show clearly perfect temporal predicted definition observed performance music performance various kinds systematic temporal specified musical known
knowledge integration rule extraction neural networks proposal
abduction belief revision propose model abduction based revision state agent explanations must sufficient induce belief sentence explained instance observation ensure consistency beliefs manner accounts sentences model will generate explanations predict observation thus generalizing current accounts require deductive relationship explanation observation also provides natural preference ordering explanations defined terms plausibility illustrate generality approach two key paradigms modelbased diagnosis abductive diagnosis within framework reconstruction provides alternative semantics extends systems accommodate predictive explanations semantic preferences explanations also illustrates general information incorporated principled manner parts paper appeared preliminary form abduction belief revision model explanations proc national artificial intelligence washington
best probability activation performance comparisons several designs sparse distributed memory report abstract optimal probability activation corresponding performance studied three designs sparse distributed memory namely original design design design will assume hard locations case storage addresses stored data randomly chosen will consider different levels random noise reading address
comments information stored sparse distributed memory report abstract consider sparse distributed memory randomly chosen hard locations unknown number random data vectors stored method given estimate content memory high accuracy fact estimate unbiased variation roughly proportional number hard locations memory length data accuracy made arbitrarily high making memory enough consequence good reading methods used without need special extra location introduced
systems simple approach belief revision belief update reasoning evidence actions describe semantics rules exceptions provides coherent framework many causal reasoning rule automatically extracted form knowledge base facilitate construction plausible beliefs represent causation formalism incorporates principle markov set independence constraints interpretations show formalism classical problems associated specificity prediction abduction offers natural way unifying belief revision belief update reasoning actions
promising genetic algorithm approach jobshop scheduling scheduling problems
learning logical definitions relations machine learning firstorder theory revision
convergence properties algorithm gaussian mixtures build mathematical connection expectationmaximization algorithm approaches maximum likelihood learning finite gaussian mixtures show step parameter space obtained gradient via projection matrix provide explicit expression matrix analyze convergence terms special properties provide new results analyzing effect likelihood surface based mathematical results present comparative discussion advantages algorithms learning gaussian mixture models
perception time phase toward model pattern processing
reference bayesian test nested hypotheses relationship criterion
first order regression applications realworld domains first order regression algorithm capable handling realvalued continuous variables introduced applications presented learning assumes realvalued class discrete realvalued variables algorithm combines learning standard ilp concepts first order concept description background knowledge clause generated refining initial clause adding literals form discrete attributes realvalued attributes background knowledge literals clause body algorithm employs covering approach search heuristic function stopping criteria based local improvement minimum number examples maximum clause length minimum local improvement minimum description length allowed error variable depth outline algorithm results systems application artificial realworld domains presented realworld domains modelling behavior modelling process modelling operators behavior process electrical special emphasis given evaluation obtained models domain experts comments aspects practical use induced knowledge results obtained knowledge acquisition process show several important guidelines knowledge acquisition concerning mainly process interaction domain experts primarily importance comprehensibility induced knowledge
measuring organization topographic maps address problem measuring degree organization organization computational model cortex theoretical framework measures developed used produce algorithms measuring degree organization symmetry topographic map formation performance resulting measures tested several topographic maps obtained selforganization initially random network results compared made humans found agreement human obtained using organization measures based error averaging measures developed correct large constant topographic maps
induction temporal structure learning structure sequences difficult computational problem fraction relevant information available although variants back propagation principle used find structure sequences practice sufficiently powerful discover arbitrary especially long temporal intervals involving high order statistics example designing connectionist network music encountered problem net able learn musical structure occurs locally relations among notes within musical structure occurs longer time relations among address problem require means constructing reduced description sequence makes global aspects explicit propose achieve using hidden units operate different time constants simulation experiments indicate slower hidden units able global structure structure simply learned standard many patterns world temporal speech music events recurrent neural net architectures accommodate timevarying sequences example architecture shown figure map sequence inputs sequence outputs learning structure sequences difficult computational problem input pattern may contain information thus back propagation
sequence learning incremental higherorder neural networks incremental higherorder neuralnetwork combines two properties found useful sequence learning higherorder connections incremental introduction new units incremental higherorder neuralnetwork adds higher orders needed adding new units dynamically modify connection weights new units modify weights next information previous step since theoretically number units added network information arbitrarily past bear prediction temporal tasks thereby learned without use feedback contrast recurrent recurrent connections training simple fast experiments demonstrated speedups two orders magnitude recurrent networks
convergence controls mcmc algorithms applications hidden markov chains complex models like hidden markov chains convergence mcmc algorithms used approximate posterior distribution bayes estimates parameters interest must controlled robust manner propose paper series online controls rely classical nonparametric tests evaluate independence distribution stability markov chain asymptotic tests lead graphical control presented normal mixture hidden markov chains compare full gibbs sampler gibbs sampler based formulae
application neural networks classification disease quantitative three different methods investigated determine ability detect classify various categories disease statistical method discriminant analysis supervised neural network called backpropagation selforganizing feature map examined investigation performed basis previously selected set image parameters limited number patients successfully extended generating additional independent data identical statistical properties generated data used training test sets final test made original patient data validation set neural networks attractive alternative traditional statistical techniques dealing medical detection classification tasks moreover use generated data training networks discriminant classifier shown
principal independent components neural networks recent developments nonlinear extensions principal component analysis pca neural networks introduced earlier authors reviewed networks nonlinear hebbian learning rules related signal like projection pursuit independent component analysis ica separation results mixtures real world signals given
generalization allocation credit unsupervised category learning acknowledgements research supported part office naval research cognitive neural sciences foundation special opportunity grant thank valuable discussions
flexible model human many processes vary pattern although function used model patterns functional form appropriate peak phases paper describe spline function fit model includes phase time magnitude peak estimated also describe tests fit components model data experiment study responses humans used demonstrate methods
genetic algorithms largescale optimization present highlevel algorithms largescale optimization problems containing integer variables demonstrate effectiveness solution largescale graph partitioning problems algorithms combine paradigm lower bounds decomposition methods genetic approaches building blocks partial solutions even graph partitioning problems requiring variables standard formulation approach produces solutions measured easily computed lower bound substantially outperforms graph partitioning techniques based heuristics spectral methods
hierarchical spatiotemporal mapping disease rates maps rates useful tools determining spatial patterns disease combined information also permit assessment environmental whether certain suffer certain effects environmental bayes empirical bayes methods proven useful smoothing maps disease risk eliminating estimates areas maintaining resolution paper extend existing hierarchical spatial models account temporal effects spatiotemporal interactions fitting resulting models requires implementation markov chain monte carlo mcmc methods novel techniques model evaluation selection illustrate approach using dataset cancer rates state period
feature extraction using unsupervised neural network novel unsupervised neural network dimensionality reduction seeks directions presented connection exploratory projection pursuit methods discussed leads new statistical insight synaptic modification equations learning bcm neurons importance dimensionality reduction principle based solely distinguishing features demonstrated using recognition experiment extracted features compared features extracted using backpropagation network
investigating value good input representation paper computational learning theory natural learning systems vol eds mit press abstract ability inductive learning system find good solution given problem dependent upon representation used features problem number factors including size ability learning algorithm perform constructive induction effect input representation accuracy learned concept description present experiments evaluate effect input representation generalization performance realworld problem finding genes dna experiments demonstrate two different input representations task result significantly different generalization performance neural networks decision trees neural symbolic methods constructive induction fail bridge gap two representations believe realworld domain provides interesting challenge problem machine learning constructive induction relationship two representations known representational shift involved constructing better representation
overview selection schemes suggested classification paper role selection evolutionary algorithms briefly review common selection schemes fields genetic algorithms evolution strategies genetic programming however classify selection schemes according group evolutionary algorithm belong rather distinguish selection schemes global competition schemes local competition schemes paper fully review analyse presented selection schemes tries short reference standard advanced selection schemes
learning maps using backpropagation parallel machine backpropagation unsupervised learning procedure feedforward networks desired output vector identical input vector backpropagation able use powerful running parallel machines maps hand developed variant competitive learning procedure however case backpropagation version competitive learning simple extension cost function backpropagation leads competitive version backpropagation used produce topographic maps demonstrate approach applied problem algorithm implemented using backpropagation simulator parallel machine rap
representing patterns network oscillators paper describes evolving computational model perception production simple patterns model consists network oscillators different frequencies input patterns oscillators whose frequencies match input tend become metrical structure represented explicitly network form clusters oscillators whose frequencies phase constrained maintain relationships characterize meter patterns represented explicit oscillators network become expected pattern fails appear model makes predictions relative difficulty nested defines musical probably also linguistic meter appears fundamental way people produce patterns time meter however sufficient describe patterns interesting metrical hierarchy simplest gaps one levels hierarchy normally removed regular intervals match period level metrical hierarchy will call simple pattern figure shows example simple pattern grid representation meter behind pattern patterns effect input
radial basis functions approximation orders paper generalize several results uniform approximation orders radial basis functions approximation orders results apply particular spaces translates radial basis functions examples results apply include approximation radial function spaces
radial basis function approximation centers centers paper studies norm approximations space discrete set translates basis function attention restricted functions whose fourier transform smooth examples basis functions splines types radial basis functions employed approximation theory approximation problem case set points ffi used forms lattice many optimal approximation schemes already found literature contrast mostly specific results known set ffi points main objective paper provide general tool extending approximation schemes use integer translates basis function case introduce single relatively simple method approximation orders provided large number schemes literature precisely almost stationary schemes future introduction new schemes uniform effort made conditions function still allow unified error analysis hold course discussion recent results center approximation improved upon
upper bound approximation power principal spaces upper bound approximation power provided principal spaces derived assumptions applies stationary nonstationary shown apply spaces generated exponential box splines splines kernel
machine learning explanationbased learning reinforcement learning unified view problems full descriptions operators known explanationbased learning ebl reinforcement learning methods applied paper shows methods involve process information goal toward starting state methods perform propagation basis ebl methods compute operators hence perform propagation basis observed many algorithms reinforcement learning viewed asynchronous dynamic programming based observation paper shows develop dynamic programming versions ebl call dynamic programming explanationbased reinforcement learning paper compares batch online versions batch online versions dynamic programming standard ebl results show dynamic programming combines strengths ebl fast learning ability scale large state spaces strengths reinforcement learning algorithms learning optimal policies results shown chess synthetic tasks
extensions algorithm image segmentation pattern classification paper present extensions algorithm vector quantization permit efficient use image segmentation pattern classification tasks shown introducing state variables correspond certain statistics dynamic behavior algorithm possible find representative centers lower dimensional define boundaries classes multidimensional multiclass data permits one example find class boundaries directly sparse data image segmentation tasks efficiently place centers pattern classification local gaussian classifiers state variables used define algorithms determining adaptively optimal number centers data density examples application extensions also given report describes research done within center biological computational learning department brain cognitive sciences artificial intelligence laboratory research sponsored grants office naval research grant national science foundation contract asc9217041 grant national health contract additional support provided organization atr visual perception research laboratories electric corporation siemens support laboratorys artificial intelligence research provided onr contract supported part grant
limitations selforganizing maps vector quantization multidimensional scaling limitations using selforganizing maps either quantization multidimensional scaling discussed recent empirical findings relevant theory remaining ability time new combined technique online clustering plus mapping cluster shown perform significantly worse terms quantization error recovering structure clusters preserving topology empirical study using series multivariate normal clustering problems
robust reinforcement learning motion planning exploring find better solutions agent performing online reinforcement learning perform worse acceptable cases exploration might even results often modeled terms failure states agents environment paper presents method uses domain knowledge reduce number failures exploration method set actions agent control policy ensure exploration conducted policy space policies resulting action set abstract relationship task solved common many applications although cost added learning may result suboptimal solution argue appropriate tradeoff many problems illustrate method domain motion planning
selecting input variables using mutual information density estimation
investigating role simulated populations evolving individuals work applying genetic algorithms populations neural networks real distinction genotype nature information contained genotype mapping genetic information usually much complex many organisms exhibit include two copies gene two copies identical sequences therefore functional difference products usually proteins expressed feature one one expressed paper review literature use dominance operators genetic algorithms present new results obtained simulations changing environments finally discuss results simulations parallel biological findings
task selection processor architecture distributed processor organization speculation exploit high degrees instruction level parallelism ilp sequential programs without improvements main goal paper understand key implications features distributed processor organization speculation compiler task selection point view performance identify fundamental performance issues control speculation data communication data dependence speculation load task show issues related key characteristics tasks task size control data dependence describe compiler heuristics select tasks characteristics report experimental results show heuristics successful boosting overall performance larger ilp
approach trainer technical report january abstract paper introduces approach method learning agent employing reinforcement learning decide training agent instruction using approach find number responses produced significantly faster learners learner aid randomly guidance received via approach random guidance thus reduce interaction training agent learning agent without reducing speed learner develops policy fact intelligent learner help even increase learning speed level trainer interaction
constructive induction using strategy feature selection present method feature construction selection finds minimal set conjunctive features appropriate perform classification task problems bias appropriate method outperforms constructive induction algorithms able achieve higher classification accuracy application method search minimal boolean expressions presented analyzed help examples
bayesian finite mixtures nonlinear modeling data paper discuss bayesian approach finding latent classes data approach use finite mixture models describe underlying structure data demonstrate possibility use full joint probability models interesting new exploratory data analysis concepts methods discussed illustrated case study using data set recent study bayesian classification approach described implemented presents addition standard exploratory data analysis data
constructive algorithms hierarchical mixtures experts present two hierarchical mixture experts architecture view tree structured classifier firstly applying likelihood splitting criteria expert grow tree adaptively training secondly considering path tree may branches away either become redundant demonstrate results growing pruning algorithms show significant speed efficient use parameters conventional algorithms two classifying bit parity patterns
empirical comparison pruning methods decision tree induction machine improving shared rules multiple category domain theories technical report artificial intelligence university december
machine learning learning imperfect data systems interacting realworld data must address issues raised possible presence errors observations makes paper first present framework imperfect data resulting problems may cause distinguish two categories errors data random errors noise systematic errors examine relationship task describing observations way also useful future problemsolving learning tasks secondly examine techniques currently used research errors
evolving sort lessons genetic programming applying genetic programming paradigm task evolving iterative sorting algorithms variety interesting lessons learned proper selection sorting algorithms evolved general nontrivial sorting problem used evaluate value several alternative parameters small gains shown value applying steady state genetic algorithm techniques genetic programming called steady state genetic programming demonstrated one genetic operator created single crossover shows promise least environment
feature subset selection keywords minimum description length principle cross validation noise irrelevant redundant features may reduce predictive accuracy comprehensibility induced concepts common machine learning approaches selecting good subset relevant features rely crossvalidation alternative present application particular minimum description length mdl measure task feature subset selection using mdl principle allows taking account available data new measure plausible yet still simple therefore efficiently computable show empirically new method value feature subsets efficient performs least methods based crossvalidation domains large number training examples large number possible features yield gains efficiency thus new approach seems scale better large learning problems previous methods
learning decision lists using homogeneous rules rules inductive algorithms cn2 learn decision lists incrementally one rule time algorithms face rule problem classification accuracy decision list depends learned rules thus even though rules learned evaluated existing algorithms solve problem greedy iterative structure rule learned training examples match rule removed training set propose novel solution problem decision lists homogeneous rules rules whose classification accuracy change position decision list prove problem finding accurate decision list reduced problem finding accurate homogeneous rules report performance algorithm data sets repository problems
constructing fuzzy graphs examples methods build function approximators example data gained considerable interest past especially build models allow interpretation attention existing algorithms however either complicated use infeasible highdimensional problems article presents efficient easy use algorithm construct fuzzy graphs example data resulting fuzzy graphs based locally independent fuzzy rules operate solely selected important attributes enables application fuzzy graphs also problems high dimensional spaces using examples real world data set demonstrated resulting fuzzy graphs offer insights structure example data underlying model
hidden markov model analysis
modeling student reinforcement learning describe methodology intelligent teaching system make high level strategy decisions basis low level student modeling information framework less costly construct superior hand coding teaching strategies learners needs order reinforcement learning used learn associate superior teaching actions certain states students knowledge reinforcement learning shown flexible handling noisy data need expert domain knowledge often needs significant number trials learning propose offline learning methodology using sample data simulated students small amounts expert knowledge problem
hierarchical model processing application visual trajectory classification intelligent system whether human robotic must capable dealing patterns time temporal pattern processing achieved system shortterm memory capacity different representations time work propose neural model realized selforganizing system model exhibits ability extract construct complex structured hierarchical manner starting basic primitive temporal elements important feature proposed model use temporal correlations express dynamic
estimating form eeg power spectrum estimating tasks requiring attention human varies time scale serious consequences ranging air traffic control monitoring power changes eeg power spectrum level measuring simultaneous changes eeg performance auditory monitoring task combining power spectrum estimation principal component analysis artificial neural networks show continuous accurate near realtime estimation operators global level feasible using eeg measures two central sites lead practical system monitoring cognitive state human operators settings
analysis temperature using smoothing spline anova
robustness analysis bayesian networks generated convex sets distributions paper presents exact solutions convergent approximations inferences bayesian networks associated generated convex sets distributions robust bayesian inference calculation bounds posterior values given perturbations probabilistic model paper presents exact inference algorithms analyzes circumstances exact inference becomes intractable two classes algorithms numeric approximations developed transformations original model first transformation reduces robust inference problem estimation probabilistic parameters bayesian network second transformation uses algorithm generate sequence maximization problems bayesian network analysis extended lower density bounded belief function density bounded total variation density ratio classes distributions university
chaos genetic algorithms
learning one fundamental problems learning identifying members two different classes example cancer one must learn discriminate previously determined diagnosis one learns function distinguishing acquired knowledge used new perceptron simple biologically inspired model learning problem perceptron trained constructed using examples two classes perceptron used classify new examples describe perceptron capable learning using develop framework investigating different methods training perceptron depending define best perceptron different minimization problems developed training perceptron effectiveness methods evaluated empirically four practical applications breast cancer diagnosis detection disease voting recognition paper assume prior knowledge machine learning pattern recognition
draft modeling time series prediction characterization
density networks application protein modelling define latent variable model form neural network target outputs specified inputs although inputs missing still possible train model simple probability distribution unknown inputs maximizing probability data given parameters model discover description data terms underlying latent variable space lower dimensionality present preliminary results application models protein data
prior information generalized questions paper studies aspects two categories usually differ like relevance generalization role loss function presents unifying formalism types information identified answers generalized questions shows kind generalized information necessary enable learning put usual training data prior information equal possibilities variants measurement control generalized questions including examples smoothness reviews measurement linguistic concepts based fuzzy priors principles combine uses bayesian decision theoretic framework parallel inverse decision problems proposes problems aspects bayesian two step approximation consisting posterior maximization subsequent risk minimization analyses empirical risk minimization aspect information compares bayesian two step approximation empirical risk minimization including interpretations occams razor examples conditions maximum posterior approximation priors leading nonlinear equations similar example equations theory physics summary paper focuses dependencies answers different questions training examples alone dependencies enable generalization need empirical measurement control explicit treatment theory report describes research done within center biological computational learning department brain cognitive sciences massachusetts institute technology research sponsored grant national science foundation contract asc9217041 grant contract author supported fellowship fellowship
evolving graphs networks edge encoding preliminary report present alternative cellular encoding technique evolving graph network structures via genetic programming new technique called edge encoding uses edge operators rather node operators cellular encoding cellular encoding edge encoding produce possible graphs two bias genetic search process different ways may therefore useful different set problems problems techniques may used edge encoding may particularly useful include evolution recurrent neural networks finite automata queries symbolic knowledge bases preliminary report present technical description edge encoding initial comparison cellular encoding experimental investigation relative encoding schemes currently progress
geometric comparison classifications rule sets present technique evaluating classifications geometric comparison rule sets rules represented objects similarity classes computed geometric class descriptions system produces correlation matrix indicates degree similarity pair classes technique applied classifications generated different algorithms different numbers classes different attribute sets experimental results case study medical domain included
learning natural resources become less naturally become interested bear key importance learning argue paper model learning viewed process uses environmental feedback sensory sensory useful information vehicles indicators phenomena main aim will show computer implementation model used enhance learning abilities simulated playing
testing causal effects paper concerns probabilistic evaluation effects actions presence variables show identification causal effect variable set variables accomplished systematically time polynomial number variables graph causal effect expression obtained probability action will achieve specified goal set goals
visual schemas neural networks object recognition scene analysis visor large connectionist system shows visual schemas learned represented used mechanisms natural neural networks processing visor based cooperation competition parallel bottomup topdown activation schema representations simulations show visor robust noise variations inputs parameters indicate confidence analysis attention important differences use context recognize objects experiments also suggest representation learning stable behavior consistent human processes perceptual reversal learning schema mechanisms visor serve starting point building robust highlevel vision systems perhaps motor control natural language processing systems
learning algorithms applications robot navigation protein
learning limited dependence bayesian classifiers present framework characterizing bayesian classification methods framework thought spectrum dependence given probabilistic model naive bayes algorithm end learning full bayesian networks general much work carried along two spectrum surprising little done along analyze assumptions made one moves along spectrum show tradeoffs model accuracy learning speed become critical consider variety data mining domains present general induction algorithm allows spectrum depending available computational power induction show application number domains different properties
evolutionary cost learning acquired members evolving population lifetime adaptive processes learning become specified later generations thus change level learning population evolutionary time paper explores idea benefits gained learning may also costs ability learn costs selection pressure genetic acquired two models presented attempt illustrate first uses fitness landscapes show effect explicit implicit costs characteristic observed graph level plasticity population showing learning first selected evolution second model practical example neural network controllers evolved small mobile robot results experiment also show
landscapes learning costs genetic evolution population guided acquired members population lifetime phenomenon known baldwin effect speed evolutionary process initially acquired become specified later generations paper presents conditions genetic take place benefits lifetime adaptation give population may cost adaptive ability evolutionary tradeoff costs benefits provides selection pressure acquired become specified also space evolution operates space adaptive processes learning operate general different nature guarantee acquired characteristic become specified spaces must property correlation means small distance two individuals space implies small distance two individuals space
networks pattern recognition trees guidance decision trees widely used tasks relatively much faster build compared neural networks humans normal decision trees based input vector one branch followed probabilistic trees based input vector follow probability probabilities learned system probabilistic decisions likely useful boundary classes noise input data addition provide confidence measure allow nodes trees instead uniform voting learn every
finite state machines recurrent neural networks automata dynamical systems approaches
backpropagation convergence via deterministic minimization fundamental backpropagation algorithm training artificial neural networks deterministic gradient method certain natural assumptions series learning rates series established every point online stationary point error function results presented cover serial parallel online modified term weight decay
constructing deterministic finitestate automata recurrent neural networks recurrent neural networks trained behave like deterministic finitestate automata show performance tested long strings performance internal representation learned dfa states use sigmoidal discriminant function together recurrent structure contribute prove simple algorithm construct recurrent neural networks sparse interconnection topology sigmoidal discriminant function internal dfa state representations stable constructed network correctly strings arbitrary length algorithm based encoding strengths weights directly neural network derive relationship weight strength number dfa states robust string classification dfa states input symbols constructive algorithm generates neural network neurons weights compare algorithm methods proposed literature
report recent results techniques nonlinear stability abstract paper presents lyapunov function theorem motivated robust control analysis design result based upon generalizes various aspects wellknown classical theorems unified natural manner includes arbitrary bounded system deals global asymptotic stability results smooth lyapunov functions applies stability respect necessarily compact invariant sets obtained theorem show wellknown lyapunov sufficient condition stability also necessary open question raised several authors past years
extraction rules discretetime recurrent neural networks technical report university college abstract extraction symbolic knowledge trained neural networks direct encoding partial knowledge networks prior training important issues allow exchange information symbolic connectionist knowledge representations focus paper quality rules extracted recurrent neural networks discretetime recurrent neural networks trained correctly classify strings regular language rules defining learned grammar extracted networks form deterministic finitestate automata applying clustering algorithms output space recurrent state neurons algorithm extract different finitestate automata consistent training set network compare generalization performances different models trained network introduce heuristic permits choose among consistent model best approximates learned regular grammar
jobshop scheduling network jobshop scheduling important task interested particular task scheduling processing space program paper summarizes previous work task solution reinforcement learning algorithm previous work input features paper shows extend neural network architecture apply schedules experimental tests show network match performance previous system tests also show neural network approaches significantly outperform best previous solution problem terms quality resulting schedules number search steps required construct
power neural nets report abstract paper deals simulation turing machines neural networks networks made evolving processors updates state according sigmoidal linear combination previous states units main result states one may simulate turing machines nets linear time particular possible give net made processors computes universal function update report new results include simulation linear time machines opposed used previous version
complexity realtime search paper develop new algorithms variety tasks analyze complexity algorithm realtime search algorithm developed used reach stationary moving goal state identify shortest paths given start state stationary goal state algorithm start state goal state algorithms search horizon one require internal memory must able store information states example algorithm determines optimal universal plans finds optimal paths states set stationary goal states even actions available show tasks studied paper solved algorithms action state spaces size
convergence rates approximation translates paper consider problem approximating function function space linear combination translates given function using show possible define function spaces functions rate convergence zero error number dimensions apparent avoidance dimensionality due fact function spaces constrained dimension increases examples include spaces type number weak derivatives required larger number dimensions give results approximation norm norm interesting feature results constructive nature iterative procedure defined achieve rate paper describes research done within center biological information processing department brain cognitive sciences artificial intelligence laboratory department university department university research sponsored grant office naval research onr cognitive neural sciences division artificial intelligence center aircraft corporation support laboratorys artificial intelligence research provided advanced research projects agency department defense contract part onr contract massachusetts institute technology
acquiring recursive iterative concepts explanationbased learning explanationbased generalization generalizing explanation structures generalizing university computer sciences technical report abstract explanationbased learning specific problems solution generalized form later used solve similar problems research explanationbased learning involves constraints variables explanation specific example rather generalizing graphical structure explanation however acquisition concepts iterative recursive process implicitly represented explanation fixed number applications paper presents algorithm generalizes explanation structures reports empirical results demonstrate value acquiring recursive iterative concepts algorithm learns recursive iterative concepts integrates results multiple examples useful generalization problems learning recursive rule appropriate system produces result standard explanationbased methods applying learned recursive rules requires extension problem solver namely ability explicitly call specific rule empirical studies demonstrate generalizing structure explanations helps avoid recently reported negative effects learning
competitive environments evolve better solutions complex tasks
convergence rates gibbs samplers uniform distributions consider gibbs sampler applied uniform distribution bounded region show convergence properties gibbs sampler depend greatly smoothness boundary indeed sufficiently smooth boundaries sampler uniformly boundaries sampler fail even
constructing intermediate concepts decomposition real functions learning examples often useful representation intermediate concepts usual advantage learning problem makes learning easier improves comprehensibility induced descriptions paper develop technique discovering useful intermediate concepts class attributes realvalued technique based decomposition method originally developed design switching circuits recently extended handle specified functions also applied machine learning tasks paper introduce modifications needed real functions present symbolic form method evaluated number test functions results show method correctly fairly complex functions decomposition hierarchy depend given basic functions background knowledge
heterogeneous uncertainty sampling supervised learning uncertainty sampling methods iteratively class labels training instances whose classes uncertain despite previous labeled instances methods greatly reduce number instances expert need one problem approach classifier best suited application may expensive train use selection instances test use one classifier highly efficient probabilistic one select examples training another c45 rule induction program despite chosen heterogeneous approach uncertainty samples classifiers lower error rates random samples ten times larger
causal models latent variables certain causal models involving variables induce independence constraints among observed variables imply nevertheless constraints observed distribution paper derives general formula constraints induced variables variables directly affect variables help formula possible test whether model involving variables may account data whether given able
bootstrap confidence intervals smoothing splines comparison bayesian confidence intervals
improved boosting algorithms using predictions describe several improvements freund adaboost boosting algorithm particularly setting hypotheses may assign predictions give simplified analysis adaboost setting show analysis used find improved parameter settings refined criterion training weak hypotheses give specific method predictions decision trees method closely related one used method also suggests technique growing decision trees turns identical one proposed focus next apply new boosting algorithms multiclass classification problems particularly case example may belong one class give two boosting methods problem one leads new method handling case simpler effective techniques suggested freund schapire finally give experimental results comparing algorithms discussed paper
genetic evolutionary algorithms direct random search algorithms principles natural evolution method solve adaptation learning tasks general several features common observed genetic level living species paper algorithms capability adaptation learning wider sense demonstrated focused genetic algorithms illustrate learning process population level first level learning evolution strategies demonstrate learning process strategy parameters second level learning
learning bayesian networks local structure examine novel addition known methods learning bayesian networks data improves quality learned networks approach explicitly represents learns local structure conditional probability distributions quantify networks increases space possible models representation variable number parameters resulting learning procedure induces models better interactions present data describe theoretical foundations practical aspects learning local structures provide empirical evaluation proposed learning procedure evaluation indicates learning curves characterizing procedure converge faster number training instances standard procedure local structure results also show networks learned local structures tend complex terms yet require fewer parameters
validation voting paper contains method bound test errors voting members chosen trained classifiers many directly achieve useful error bounds fewer classifiers better classifiers use linear programming infer committee error bounds test method using credit data also extend method infer bounds classifiers general
reinforcement learning neural networks control applied accurate simulation used compare performance controller neural network trained predict output controller neural network trained minimize error output set point reinforcement learning agent trained minimize sum squared error time although controller works task neural networks result improved performance
rule induction cn2 recent improvements cn2 algorithm induces ordered list classification rules examples using entropy search heuristic short paper describe two improvements algorithm firstly present use error estimate alternative evaluation function secondly show ordered rules generated experimentally demonstrate significantly improved performances resulting changes thus usefulness cn2 inductive tool comparisons c45 also made
introduction theory neural computation
selective execution architecture major high performance superscalar processors paper present selective execution see execution model overcome executing paths branches present processor extension superscalar architecture architecture uses novel instruction register mechanism execute instructions multiple paths simultaneously processor maximum resource code sequences results simulations show see improve performance much benchmark average compared normal superscalar speculative execution processor moreover model practical implement using small amount additional state control logic
classifiers theoretical empirical study paper describes competitive tree learning algorithm derived first principles algorithm approximates bayesian decision theoretic solution learning task comparative experiments algorithm several statistical families tree learning algorithms currently use show derived bayesian algorithm consistently good better although sometimes computational cost using strategy design algorithms many supervised model learning tasks given just probabilistic representation kind knowledge learned second learning algorithm derived learning bayesian networks data implications incremental learning use multiple models also discussed
irrelevant features subset selection problem address problem finding subset features allows supervised induction algorithm induce small concepts examine notions relevance show definitions used machine learning literature partition features useful categories relevance present definitions two degrees relevance definitions improve understanding behavior previous subset selection algorithms help define subset features features selected depend features target concept also induction algorithm describe method feature subset selection using crossvalidation applicable induction algorithm discuss experiments conducted id3 c45 artificial real datasets
rulebased machine learning methods functional prediction describe machine learning method predicting value realvalued function given values multiple input variables method induces solutions samples form ordered disjunctive normal form dnf decision rules central objective method representation induction compact easily solutions rulebased decision model extended search efficiently similar cases prior approximating function values experimental results realworld data demonstrate new techniques competitive existing machine learning statistical methods sometimes yield superior regression performance
limited dual path execution work presents hybrid branch predictor scheme uses limited form dual path execution along dynamic branch prediction improve execution times ability execute paths conditional branch enables branch penalty however dual path execution infeasible due instruction rates far capability single branch others must using confidence information available dynamic branch prediction state tables limited form dual path execution becomes feasible reduces branch predictor allowing predictions low confidence avoided study present new approach branch prediction confidence little use confidence mechanism determine whether dual path execution branch prediction used comparing hybrid predictor model dynamic branch predictor shows decrease rate translates reduction runtime results imply dual path execution often thought resource method may approach restricted appropriate predicting set
multiple path execution paper presents multipath execution exploits existing hardware simultaneous multithreading processor execute multiple paths execution fewer threads processor hardware contexts multipath execution uses contexts execute code along less likely path branches paper describes hardware mechanisms needed enable processor efficiently speculative threads multipath execution mapping synchronization described enables multiple paths policies examined branches competition primary alternate path threads critical resources results show increases single program performance contexts average depending penalty programs high rate
computational learning humans machines paper review research machine learning relation computational models human learning focus initially concept induction examining five main approaches problem consider complex issue learning sequential behaviors compare sometimes appears machine learning psychological literature growing evidence different theoretical paradigms typically produce similar results response suggest concrete computational models currently field may less useful simulations operate abstract level illustrate point abstract simulation explains challenging phenomenon area category learning conclude general observations abstract models
detection via family pairwise search straightforward generalization pairwise sequence comparison algorithms function unknown biological sequence often accurately inferred identifying sequences original sequence given query set known exist least three general classes techniques finding additional pairwise sequence comparisons analysis hidden markov modeling pair sequence comparisons typically employed single query sequence known hidden markov models hmms hand usually trained sets sequences based methods fall two
pattern theoretic knowledge discovery future research directions knowledge discovery databases include ability extract concept relating useful data current limitations involve search complexity find concept means useful pattern theory research natural way domain goal paper first present new approach problem learning discovery robust pattern finding second explore current limitations pattern theoretic approach applied general problem third exhibit performance experimental results binary functions compare results c45 new approach learning demonstrates powerful method finding patterns robust manner
guide multiple alignment version please comments instructions understanding dynamic programming distance approach pairwise sequence alignment useful parts also use resources helpful part former see latter see chapter book course general rationale will understand multiple alignment considered challenging problem will study approaches try reduce number steps needed calculate optimal solution will study fast heuristics case study involving sequences will study multiple obtained results original paper revision history version updated revised solution submitted various content
system induction decision trees article describes new system induction decision trees system combines deterministic hillclimbing two forms find good split form hyperplane node decision tree decision tree methods especially domains attributes numeric although adapted symbolic mixed attributes present extensive empirical studies using real artificial data analyze ability construct trees smaller accurate axisparallel also examine benefits construction decision trees
adaptive tuning numerical prediction models part randomized related methods
hierarchical explanationbased reinforcement learning explanationbased reinforcement learning introduced way combining ability reinforcement learning learn optimal plans generalization ability explanationbased learning ebl extend work domains agent must order achieve sequence optimal fashion hierarchical effectively learn optimal policies sequential task domains even weakly interact also show planner achieve individual available method converges even faster
paradigm discretization continuously valued data discretization continuously valued data useful necessary tool many learning paradigms assume nominal data list objectives efficient effective discretization presented paradigm called boundary ranking classification evaluation attempts meet objectives presented along algorithm follows paradigm paradigm many objectives potential extension meet empirical results promising reasons potential effective efficient method discretization continuously valued data advantage general enough extended types learning
searching dependencies bayesian classifiers attributes naive bayesian classifiers make independence assumptions perform data sets poorly others explore ways improve bayesian classifier searching dependencies among attributes propose evaluate two algorithms detecting dependencies among attributes show sequential elimination algorithm provides improvement naive bayesian classifier domains improvement occurs domains naive bayesian classifier significantly less accurate decision tree learner suggests attributes used common databases independent class independence assumption affect accuracy classifier bayesian classifier probabilistic method classification used determine probability example class given values attributes example represented set attributevalue pairs form may estimated training data determine likely class test example probability class computed equation classifier created manner sometimes called simple naive bayesian classifier one important evaluation metric machine learning methods predictive accuracy unseen examples measured randomly selecting subset examples database use training examples used test examples case simple bayesian classifier training examples used estimate probabilities equation used detected training data
studies methods hidden markov model generation multiple sequence alignment related proteins remains challenge currently available alignment methods hidden markov model approach offers new flexible method generation multiple sequence results studies attempting infer appropriate parameter constraints generation hmms sequences methods described
gold extracting finite state machines recurrent network dynamics several recurrent networks proposed representations task formal language learning training recurrent network next step understand information processing carried network researchers extracting finite state machines internal state trajectories recurrent networks paper describes two conditions sensitivity initial conditions computational explanations due discrete measurements allow extraction methods return finite state descriptions
bias probability generalization order useful learning algorithm must able generalize inputs previously presented system bias necessary generalization shown several researchers recent years bias lead better generalization possible functions applications paper provides examples illustrate fact also explains bias learning algorithm better another practice probability functions taken account shows domain knowledge understanding conditions learning algorithm performs used increase probability accurate generalization identifies several conditions considered attempting select appropriate bias particular problem
hlearning reinforcement learning method optimize undiscounted average reward paper introduce modelbased reinforcement learning method called hlearning optimizes undiscounted average reward compare three reinforcement learning methods domain scheduling automatic guided vehicles robots used modern four methods differ along two dimensions either modelbased optimize discounted total reward undiscounted average reward experimental results indicate hlearning robust respect changes domain parameters many cases converges fewer steps better average reward per time step methods added advantage unlike methods parameters tune
smooth lyapunov theorem robust stability paper presents lyapunov function theorem motivated robust control analysis design result based upon generalizes various aspects wellknown classical theorems unified natural manner allows arbitrary bounded timevarying parameters system description deals global asymptotic stability results smooth lyapunov functions applies stability respect necessarily compact invariant sets introduction work motivated problems robust nonlinear stabilization one main
robotic control genetic neural networks technical report may abstract important often problem field artificial intelligence systems environment representations inherent meaning system since humans rely heavily semantics seems likely crucial development intelligent behavior study investigates use simulated robotic agents neural network processors part method ensure topology weights neural networks optimized genetic algorithms although optimization difficult empirical evidence shows method tractable quite experiments agents evolved control strategy able transfer novel environments behavior suggests also learning build cognitive maps
imperfect domain theories analysis explanationbased learning shown promise powerful analytical learning technique however ebl requirement complete correct domain theory successful learning occur clearly nontrivial domains developing domain theory nearly impossible task therefore much research devoted understanding imperfect domain theory extended system performance paper present characterization problem use analyze past research area past problem viewed types performance errors caused domain theory primary contrast focus primarily types knowledge present theory derive types performance errors result theory viewed search space possible domain theories variety knowledge sources used guide search examine types knowledge used variety past systems purpose analysis will indicate need universal weak method domain theory correction different sources knowledge theory correction combined
mapping bayesian networks boltzmann machines study task maximal map bayesian network variables given partial value assignment initial constraint problem known stochastic approximation algorithm simulated annealing stochastic algorithm realized sequential process set bayesian network variables one variable allowed change time consequently method become slow number variables increases present method mapping given bayesian network massively parallel machine neural network architecture sense instead using normal sequential simulated annealing algorithm use massively parallel stochastic process boltzmann machine architecture neural network updating process provably converges state solves given map task
parameterized heuristics intelligent adaptive network routing large communication networks parameterized heuristics offers powerful theoretical framework design analysis autonomous adaptive communication networks routing networks presents realtime instance optimization problem dynamic uncertain environment paper describes framework heuristic routing large networks effectiveness heuristic routing mechanism upon based described part simulation study within network grid topology formal analysis underlying principles presented incremental design set heuristic decision functions used guide along nearoptimal minimum path large network paper carefully derives properties heuristics set simplifying assumptions network topology load dynamics identify conditions guaranteed along optimal path paper concludes discussion relevance theoretical results presented paper design intelligent autonomous adaptive communication networks outline directions future research
principal curve clustering noise technical report department statistics university washington graduate research assistant adrian raftery professor statistics department statistics university washington box seattle email web research supported onr grants authors grateful helpful discussions
use expert advice extended abstract analyze algorithms predict binary value combining predictions several prediction strategies called experts analysis worstcase situations make assumptions way sequence bits predicted generated measure performance algorithm difference expected number mistakes makes bit sequence expected number mistakes made best expert sequence expectation taken respect predictions show minimum difference order square root number mistakes best expert give efficient algorithms achieve upper lower bounds matching leading constants cases give implications result performance batch learning algorithms pac setting improve best results currently known context also extend analysis case log loss used instead expected number mistakes
towards casebased reasoning synthesis paper presents novel approach structural similarity assessment adaptation casebased reasoning cbr synthesis approach presented implemented domain industrial building design relating approach existing theories provide foundation systematic evaluation appropriate cases primary repository knowledge represented using algebraic approach similarity relations provide structure preserving case modifications modulo underlying theory available representation modeled enables inference adapted solutions approach enables incorporate formally generalization abstraction transformation combinations cbr
learning automated training agent learning agent employing reinforcement learning receives sparse weakly training information present approach automated training agent may also provide instruction learner form actions learner perform learner access feedback instruction experiments vary level interaction learner allowing trainer learner almost every time step allowing trainer also vary parameter controls learner incorporates actions results show significant average number training trials necessary learn perform task
boosting weak learning algorithm majority published information computation present algorithm improving accuracy algorithms learning binary concepts improvement achieved combining large number hypotheses generated training given learning algorithm different set examples algorithm based ideas presented schapire paper strength weak learnability represents improvement results analysis algorithm provides general upper bounds resources required learning polynomial pac learning framework best general upper bounds known today show number hypotheses combined algorithm smallest number possible outcomes analysis results regarding representational power threshold circuits relation learnability compression method pac learning algorithms provide extensions algorithms cases concepts binary case accuracy learning algorithm depends distribution instances
computational model ratio paper proposes model ratio structure consisting series reasoning steps relate abstract predicates abstract predicates relate abstract predicates specific facts model satisfies important set characteristics ratio identified literature particular model shows theory case controls effect contrast purely model ratio fails account dependency effect theory decision
unifying measure topographic mappings paper abstract computational principles underlying topographic maps discussed give definition preserving map call topographic prove certain desirable properties argued topographic exist usual case many equally valid choices available quality map introduce particular measure several previous discuss relation work formulation problem sets within wellknown class quadratic assignment problems
pac learning algorithms tolerate random attribute noise paper studies robustness pac learning algorithms instance space examples corrupted purely random noise instances labels past results subject best agreement rule tolerate small amounts noise yet cases large amounts noise show lies two alternatives uniform attribute noise attribute independently random probability present algorithm pac learns unknown noise rate less positive result show product random attribute noise attribute randomly independently probability nearly malicious algorithm tolerate small amount noise supported part foundation grant nsf grant part research conducted author mit laboratory computer science supported nsf grant grant siemens corporation net address
learning roles behavioral diversity robot teams paper describes research investigating behavioral specialization learning robot teams agent provided common set skills motor behavioral builds strategy using reinforcement learning agents learn particular behavioral given current situation reward signal experiments conducted robot soccer simulations evaluate agents terms performance policy convergence behavioral diversity results show many cases robots will automatically choosing heterogeneous behaviors degree performance team depend reward structure entire team global reinforcement teams tend towards heterogeneous behavior agents provided feedback local reinforcement converge identical policies
product unit learning constructive algorithm introduced adds product units network product units provide method automatically learning higherorder input combinations required efficient synthesis boolean logic functions neural networks product units also higher information capacity sigmoidal networks however activation function received much attention literature possible reason one problems using standard backpropagation train networks containing units report examines problems evaluates performance three training algorithms networks type empirical results indicate error surface networks containing product units local minima corresponding networks units reason combination local global training algorithms found provide reliable convergence investigate added training algorithm extracting common frequency input weights training frequency separately show convergence order compare performance transfer functions product units implemented candidate units correlation system using candidate units smaller networks trained faster standard three sigmoidal types one gaussian transfer functions used superiority candidate units four different nonlinear activation functions used addition network extensive simulations showed problem implementing random boolean logic functions product units always chosen transfer functions
draft symbolic representation neural networks early version paper accepted
cognitive model learning goal develop cognitive model humans acquire skills complex cognitive tasks goal designing computational architectures navigation task requires coordination paper analyze navigation task depth use data experiments human subjects learning task guide constructing cognitive model acquisition task data box view provided execution traces inputs outputs computational experiments allow explore space alternative architectures task guided quality fit human performance data
logic iterated belief revision show paper postulates ensure rational conditional beliefs belief revision thus responses sequences observations four additional postulates sound relative qualitative version probabilistic conditioning contrary framework proposed postulates characterize belief revision process may depend elements state necessarily captured belief set also show simple modification framework allow belief revision function states establish modelbased representation theorem proposed postulates turn way may transformed iterated belief revision
strategy learning multilayer connectionist representations results presented demonstrate learning search strategies using connectionist mechanisms previous studies strategy learning within symbolic formalism addressed behavior connectionist system presented develops search weak strategy performance system applied simulated realtime task compare performance networks showing ability network discover new features thus enhance original representation critical solving balancing task
computational reinforcement learning following used adaptive control distinguish indirect learning methods learn explicit models dynamic structure system controlled direct learning methods compare existing indirect method uses conventional dynamic programming algorithm closely related direct reinforcement learning method applying methods infinite horizon markov decision problem unknown probabilities simulations show although direct method requires much less space dramatically less computation per control action learning ability task superior compares favorably complex indirect method although results address methods performances compare problems become difficult suggest given fixed amount computational power available per control action may better use direct reinforcement learning method augmented indirect techniques available resources computationally costly indirect method answers questions raised study depend many factors making context computation
knowledgebased framework belief change part foundations propose general framework study belief change begin defining belief terms knowledge plausibility agent true worlds considers plausible consider properties defining interaction knowledge plausibility show properties affect properties belief particular show assuming two natural properties belief becomes operator finally add time gives framework knowledge plausibility hence belief time extends framework modeling knowledge multiagent systems show framework quite expressive model natural way number different scenarios belief change example show capture prior probabilities updated conditioning related paper show two best studied scenarios belief revision belief update fit framework
adaptive markov chain monte carlo summary markov chain monte carlo mcmc used evaluating functions interest target distribution done calculating averages sample path markov chain stationary distribution computational efficiency markov chain rapidly mixing sometimes achieved design transition kernel chain basis detailed preliminary exploratory analysis alternative approach might allow transition kernel adapt new features encountered mcmc run however adaptation occurs often stationary distribution chain may describe framework based concept markov chain allows adaptation occur often stationary distribution chain consistency averages key words adaptive method bayesian inference gibbs sampling markov chain monte carlo
models multiple traditional model characterized choice applied choice noise model typically single regularization constant noise model single parameter ratio alone responsible determining globally attributes complexity flexibility smoothness characteristic scale length characteristic suggest models able capture just one simplicity complexity describe bayesian models smoothness varies spatially importance practical implementation concept conditional designing models many hyperparameters apply new models neuronal data demonstrate substantial improvement generalization error
learned industrial machine learning project author paper machine learning project project supported community main aim evaluate different learning algorithms using real industrial commercial applications industrial introduced different applications among fault diagnosis digit recognition prediction number learned lessons project application oriented research field machine learning distinguished especially research necessary handle real industrial commercial applications paper describe applications discuss shortcomings applied finally outline fields research necessary
improving performance using reinforcement learning paper describes application reinforcement learning difficult real world problem domain combination challenges seen research date systems operate continuous state spaces continuous time discrete event dynamic systems states fully observable nonstationary due changing rates addition use team agents responsible controlling one car team receives global reinforcement signal appears noisy agent due effects actions agents random nature incomplete observation state show results simulation best heuristic control algorithms results demonstrate power large scale stochastic dynamic optimization problem practical utility
category control navigation planning key words reinforcement learning exploration hidden state presentation paper presents exploration technique efficient exploration partially observable domains key idea applicable many exploration techniques keep statistics space possible shortterm memories instead agents current state space experimental results partially observable difficult driving task visual routines show performance improvements
improving policies without measuring performing policy iteration dynamic programming require knowledge relative rather absolute measures utility actions calls advantages actions states nevertheless existing methods dynamic programming including compute form absolute utility function smooth problems advantages satisfy two differential consistency conditions including requirement free show lead appropriate policy improvement solely terms advantages
protein structure prediction selecting features large candidate introduce parallel approach selecting features used inductive learning algorithms predict protein secondary structure able rapidly choose small feature sets containing potentially useful features building decision tree using features set training examples features included tree provide compact description training data thus suitable use inputs inductive learning algorithms empirical experiments protein task sets complex features chosen used standard artificial neural network representation yield surprisingly little performance gain even though features selected large feature discuss possible reasons result
basic extension development distributed genetic algorithms paper presents extension developed author sciences technology new university designed experimentation distributed genetic algorithms implemented extension basic system developed university primarily intended used research sequential serial genetic algorithms
selforganizing representation objects explore representation objects several distinct views stored object demonstrate ability network units support representations using unsupervised hebbian relaxation network learned recognize ten objects different training process led emergence compact representations specific input views tested novel views objects network exhibited substantial generalization capability simulated psychophysical experiments networks behavior similar human subjects
forward models supervised learning teacher internal models environment important role play adaptive systems general particular importance supervised learning paradigm paper demonstrate certain classical problems associated notion teacher supervised learning solved use learned internal models components adaptive system particular show supervised learning algorithms utilized cases unknown dynamical system actions desired outcomes approach applies supervised learning algorithm capable learning multilayer networks paper revised version mit center cognitive science paper thank michael helpful comments project supported part research support grant program division research resources national health grant atr auditory visual perception research laboratories grant siemens corporation grant human science program grant office naval research
improved algorithm incremental induction decision trees technical report updated paper will appear proceedings international conference machine learning abstract paper presents algorithm incremental induction decision trees able handle numeric symbolic variables order handle numeric variables new tree revision operator called introduced finally method given finding decision tree based direct metric candidate tree
learning physical descriptions functional definitions examples learning examples effect different conceptual
modelling images handwritten digits
weighted majority algorithm research primarily conducted author university santa support onr grant university supported onr grant grant current address research institute independence way email address supported onr grants part research done author computation laboratory partial support onr grants address department computer science university california santa email address
simple selection control rules speedup learning many recent approaches avoiding utility problem speedup learning rely sophisticated utility measures significant numbers training data accurately estimate utility control knowledge empirical results presented indicate simple selection strategy control rules derived training problem explanation quickly defines efficient set control knowledge training problems simple selection strategy provides alternative approaches improving speed problem solver
algorithm variable resolution reinforcement learning multidimensional new algorithm learning feasible trajectories goal regions high dimensional continuous high dimensions essential learning plan uniformly statespace maintains decisiontree partitioning statespace applies techniques computational efficiently adaptively high resolution critical areas current version algorithm designed find feasible paths trajectories goal regions high dimensional spaces future versions will designed find solution optimizes realvalued criterion many simulated problems tested ranging twodimensional including path planning nonlinear dynamics robots restricted spaces cases good solution found less ten trials
comparing predictive inference methods discrete domains predictive inference seen process determining predictive distribution discrete variable given data set training examples values problem domain variables consider three approaches computing predictive distribution assume joint probability distribution variables set distributions determined set parametric models simplest case predictive distribution computed using model maximum map posterior probability evidence approach predictive distribution obtained averaging individual models model family third case define predictive distribution using new definition stochastic complexity experiments performed family naive bayes models suggest using data available stochastic complexity approach produces accurate predictions sense however amount available training data evidence approach clearly outperforms two approaches map predictive distribution clearly sense two sophisticated approaches score map approach may still cases produce best results
bayesian casebased reasoning neural networks given problem casebased reasoning cbr system will search case memory use stored cases find solution possibly modifying retrieved cases adapt required input specifications paper introduce neural network architecture efficient casebased reasoning show rigorous bayesian probability propagation algorithm implemented feedforward neural network adapted cbr approach efficient indexing problem cbr naturally implemented parallel architecture heuristic matching probability metric allows cbr perform theoretically sound bayesian reasoning also show probability propagation actually offers solution adaptation problem natural way
casebased creative design designers across variety domains many creative activities since much creativity using old solutions novel ways believe casebased reasoning used explain many creative design processes
language dynamical system
prediction learning uniform convergence dimensions present new generalpurpose algorithm learning classes valued functions generalization prediction model prove general upper bound expected absolute error algorithm terms generalization dimension proposed give lower bounds upper bounds improved constant factor general apply result together techniques due obtain new upper bounds numbers terms notion dimension using different technique obtain new bounds numbers terms function show apply bounds obtain improved general bounds sample complexity agnostic learning establish sufficient stronger necessary conditions class valued functions learnable within uniform class
multiple network systems modules task division module discrimination widely considered connectionist objective incorporate neural networks intelligent systems systems intended possess functions interaction environment first step direction develop various neural network algorithms models second step combine networks modular structure might incorporated system paper consider one aspect second point namely processing reliability details architecture type neural expert module named consists number modules modules processing capabilities varies respect particular specialization aspects problem domain employs collection like experts expert highest confidence answer confidence levels system hierarchy
learning policies partially observable environments scaling partially observable markov decision processes pomdps model decision problems agent tries maximize reward face limited andor noisy sensor feedback study pomdps motivated need address realistic problems existing techniques finding optimal behavior appear scale find satisfactory policies problems states brief review pomdps paper discusses several simple solution methods shows capable finding nearoptimal policies selection extremely small pomdps taken learning literature contrast show able solve slightly larger problem based robot navigation find combination two novel approaches performs problems suggest methods scaling even larger complicated domains
self markov chain monte carlo summary propose new method construction markov chains given stationary distribution method based construction auxiliary chain stationary distribution elements auxiliary chain suitable number times proposed method many advantages easy implement provides simple analysis faster efficient currently available techniques also adapted course simulation make theoretical numerical comparisons characteristics proposed algorithm mcmc techniques
approximating optimal policies partially observable stochastic domains problem making optimal decisions uncertain conditions central artificial intelligence state world known times world modeled markov decision process mdp mdps studied extensively many methods known determining optimal action policies realistic case state information partially observable partially observable markov decision processes pomdps received much less attention best exact algorithms problems inefficient space time introduce smooth partially observable value approximation new approximation method quickly yield good approximations improve time method combined reinforcement learning methods combination effective test cases
parallel search neural network guidance
connectionist modeling fast mapping phenomenon
abduction plausible causes model belief update theory belief update proposed reasonable model revising beliefs changing world however semantics update relies information available describe alternative view update observations incorporated belief set explaining observation terms set plausible events might caused observation predicting consequences explanations also allow possibility conditional explanations show naturally induces update operator postulates certain assumptions however argue assumptions always reasonable restrict ability integrate update forms revision reasoning action parts report appeared preliminary form abductive model update proc
connectionist networks learn paper specifies main features neuronal connectionist models argues need usefulness appropriate larger structures examines recognition models perception perspective examples structures behavior development visual system briefly architecture networks perceptual recognition results presented simulations carefully recognition structures objects framework perceptual learning introduced including mechanisms new links nodes subject constraints local receptive fields global information processing discovered generation links preliminary results presented networks learn recognize simple objects generation show large improvements networks either lack structure learn links alone
decision tree induction based efficient tree ability decision tree efficiently enables variety approaches decision tree induction otherwise expensive two approaches described one incremental tree induction tree induction using measure tree quality instead test quality approaches several variants offer new computational classifier characteristics particular applications
variational approach bayesian logistic regression models extensions consider logistic regression model gaussian prior distribution parameters show accurate variational techniques used obtain closed form posterior distribution parameters given data thereby posterior predictive model results extended binary belief networks belief networks also derive closed form presence missing values finally show dual regression problem gives latent variable density model variational formulation leads exactly updates
improving mean field approximation via use mixture distributions mean field methods provide computationally efficient approximations posterior probability distributions graphical models simple mean field methods make completely approximation posterior accurate posterior multimodal indeed posterior multimodal one modes captured improve mean field approximation cases employ mixture models posterior approximations mixture component distribution describe efficient methods optimizing parameters models
pole balancing recurrent evolutionary networks success evolutionary methods standard control learning tasks created need new pole balancing problem longer difficult enough serve measuring learning efficiency systems paper present difficult version problem cart pole move plane demonstrate neuroevolution system solve difficult problem without velocity information
biases efficient learning spatial temporal spatiotemporal patterns paper introduces explores representational biases efficient learning spatial temporal spatiotemporal patterns connectionist networks massively parallel networks simple computing elements examines learning mechanisms build network structures encode information environmental stimuli higher resolutions needed tasks perceptual recognition network perform simple examples presented illustrate basic structures processes used networks ensure learned representations system focus minimal adequate resolution several extensions basic algorithm efficient learning using multiresolution representations spatial temporal spatiotemporal patterns discussed
fast online qlearning uses accelerate qlearning update complexity previous online implementations based bounded size space faster algorithms update complexity bounded number actions method based observation updates may needed
generative learning structures processes generalized connectionist networks massively parallel networks relatively simple computing elements offer attractive framework exploring variety learning structures processes intelligent systems paper briefly summarizes popular learning structures processes used networks outlines range potentially powerful alternatives inductive learning systems develops class new learning algorithms massively parallel networks simple computing elements call class learning processes generative offer set mechanisms constructive adaptive determination network architecture number processing elements connectivity among function experience generative learning algorithms attempt overcome limitations approaches learning networks rely modification weights links within otherwise fixed network topology rather slow learning need choice network architecture several alternative designs range control structures processes used form content internal representations learned networks examined empirical results study generative learning algorithms briefly several extensions algorithms directions future research
next generation based autonomous road use artificial neural networks domain autonomous vehicle navigation produced promising results shown neural system vehicle reliably many different types ranging paths even results several areas within neural paradigm autonomous road following still need addressed include navigation different type simultaneous use different sensors generalization road types neural system never seen system presented addresses issue modular neural architecture uses networks connectionist many types
cooperation alternative view system experts introduce constructive incremental learning system regression problems models data means locally linear experts contrast approaches experts trained independently data learning prediction query required experts individual predictions expert trained minimizing local cross error using second order methods way expert able find local distance metric size shape field predictions valid also detect relevant put features bias importance individual input dimensions derive asymptotic results method variety simulations properties algorithm demonstrated respect interference learning speed prediction accuracy feature detection task oriented incremental learning
dynamic decision making model agent faces repeated game incomplete information nature appropriate tool modeling general interactions model environment state controlled nature may change arbitrarily function initially unknown agent bayesian form prior probability neither state selection strategy nature reward function policy agent function assigns action every history observations actions two basic feedback structures considered one perfect monitoring case agent able observe previous environment state part feedback imperfect monitoring case available agent reward obtained settings refer partially observable processes current environment state unknown main result refers competitive ratio criterion perfect monitoring case prove existence efficient stochastic policy competitive ratio obtained almost stages arbitrarily high probability efficiency measured terms rate convergence shown optimal policy exist imperfect monitoring case moreover proved perfect monitoring case exist deterministic policy satisfies long run optimality criterion addition discuss criterion prove deterministic efficient optimal strategy exist imperfect monitoring case criterion finally show approach optimality viewed qualitative previous work area
pac learning rectangles respect product distributions multipleinstance examples describe polynomialtime algorithm learning rectangles respect product distributions multipleinstance examples pac model example consists elements together indicating whether points learned assume unknown product distribution instances independently drawn according accuracy hypothesis measured probability predict whether one points drawn learned algorithm achieves accuracy probability ffi
machine learning function decomposition present new machine learning method given set training examples induces definition target concept terms hierarchy intermediate concepts definitions effectively problem smaller less complex problems method inspired boolean function decomposition approach design digital circuits cope high time complexity finding optimal decomposition propose suboptimal heuristic algorithm method implemented program hierarchy induction tool experimentally evaluated using set artificial realworld learning problems shown method performs terms classification accuracy discovery concept hierarchies
bayesian approach treestructured regression context inductive learning bayesian approach successful estimating probabilities events learning examples estimate developed handle situations paper present estimate extension estimate estimation probabilities covers also estimation probability distributions focus application construction regression trees theoretical results incorporated system automatic induction regression trees results applying system several domains presented compared previous results
technical report august
learning incomplete data realworld learning tasks often involve highdimensional data sets complex patterns missing features paper review problem learning incomplete data two statistical bayesian goal place current neural network approaches missing data within statistical framework describe set algorithms derived framework handle clustering classification function approximation incomplete data principled efficient manner algorithms based mixture modeling make two distinct expectationmaximization principle estimation mixture components missing data report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology support center provided part grant national science foundation contract asc9217041 support laboratorys artificial intelligence research provided part advanced research projects agency department defense authors supported part grant atr auditory visual perception research laboratories grant siemens corporation grant national science foundation grant office naval research supported grant foundation michael jordan nsf young
implementation finitestate automata recurrent neural networks recently proven dynamics deterministic finitestate automata dfa states input symbols implemented sparse recurrent neural network state neurons weights sigmoidal discriminant functions investigate constructive algorithm extended neural dfa implementations analog implementation neurons weights affect desired network performance show weight achieved easily weight andor neuron however requires network resources result impact construction neural dense internal representation dfa states
detecting features spatial point processes via modelbased clustering technical report department statistics university washington graduate student department university washington box seattle email address adrian raftery professor statistics department statistics university washington box seattle email address research supported office naval research grant authors grateful peter helpful discussions
problem problem must decide arm machines play sequence trials maximize reward classical problem received much attention simple model provides tradeoff exploration trying arm find best one exploitation playing arm give best payoff past solutions problem almost always assumptions statistics machines work make statistical assumptions nature process generating machines give solution problem adversary rather stochastic process complete control sequence plays prove expected payoff algorithm approaches best arm rate give improved rate convergence best arm fairly low payoff also prove general matching lower bound best possible performance algorithm setting addition consider setting player team experts arm play give strategy will guarantee expected payoff close best expert finally apply result problem learning play unknown repeated matrix game adversary
alternative conditional probabilities bayesian belief networks show alternative way representing bayesian belief network probability distributions representation equivalent traditional representation conditional probabilities makes dependencies nodes apparent intuitively easy understand also propose matrix representation andor conditional probabilities efficient memory requirements computational speed traditional representation implementations probabilistic inference use show certain class binary networks computation time approximate probabilistic inference positive upper bound error result independent size network finally alternative traditional algorithms use conditional probabilities describe exact algorithm probabilistic inference uses updates probability distributions nodes network according
neural computation requirement train large neural networks quickly design new massively parallel using design features processing nodes network connected directly processor studies show peak performance range arithmetic operations per second paper presents case hardware combines neural features general machine architecture briefly describes design progress
active learning text categorization many realworld domains like text categorization supervised learning requires large number training examples paper describe active learning method uses committee learners reduce number training examples required learning approach similar query committee framework among committee members predicted input part example used signal need actual value experiments text categorization using committee learners demonstrate approach reduce number labeled training examples required used single learner orders magnitude paper review accepted another conference journal acknowledgements data manipulation analysis programs greatly research date
developments probabilistic modelling neural learning paper give review ensemble learning using simple example
smoothing spline anova exponential families application study
cancer diagnosis via machine learning
covering divideandconquer topdown induction logic programs covering used extensively work divideandconquer technique compared covering technique logic programming framework covering works repeatedly specializing general hypothesis iteration focusing finding clause high coverage positive examples divideandconquer works specializing general hypothesis focusing positive negative examples experimental results presented demonstrating cases accurate hypotheses found divideandconquer covering moreover since covering considers alternatives repeatedly tends less efficient divideandconquer never considers alternative hand covering searches larger hypothesis space may result compact hypotheses found technique divideandconquer furthermore divideandconquer contrast covering applicable learn ing recursive definitions
discovery algorithmic probability
studies machine learning using game journal
inductive learning approach prediction paper introduces recurrence surface approximation inductive learning method based linear programming predicts recurrence times using training examples examples available training output may lower bound right answer approach augmented feature selection method appropriate feature set within context linear programming computational results field breast cancer shown straightforward prediction method artificial neural network model also proposed
mml mixture modelling von gaussian distributions minimum message length mml invariant bayesian point estimation technique also consistent efficient provide brief overview mml inductive inference informationtheoretic bayesian interpretation outline mml used statistical parameter estimation mml mixture modelling program uses message various parameter estimates enable combine parameter estimation selection number components message length within constant posterior probability theory mml theory also theory highest posterior probability currently assumes variables permits multivariate data gaussian discrete von distributions
random approach motion planning technical report department computer science university
efficient computational model human visual attention one challenges models cognitive phenomena development efficient low level sensory information high level processes visual processing researchers long argued mechanism required perform many tasks required high level vision thesis presents connectionist model visual attention used vehicle studying interface model efficient biologically plausible complexity network linear number effective parallel strategies used minimize number iterations required resulting system able efficiently solve two tasks particularly difficult standard bottomup models vision computing spatial relations visual search simulations show networks behavior matches much known psychophysical data human visual attention general architecture model also closely matches known data human attention system various extensions discussed including methods learning component modules
minimax dynamics networks lyapunov function networks constructed construction assumes symmetric interactions within excitatory inhibitory populations neurons interactions populations lyapunov function yields sufficient conditions global asymptotic stability fixed points conditions limit cycles may stable relations lyapunov function optimization theory classical mechanics dynamics neural network symmetric interactions provably converges fixed points general mathematical result establish paradigm neural computation fixed point interactions neurons brain furthermore dynamical behaviors seen brain fixed point attractors also include complex behavior types dynamics realized networks may useful neural computation reasons important understand global behavior neural networks interaction excitatory neuron inhibitory neuron clearly consider class networks incorporates fundamental networks class distinct populations excitatory inhibitory neurons interactions minimax forms network dynamics
capacity report abstract efficient way reading memory presented accomplished using implicit information utilized find units thus removing noise reading memory
operations operation machine
feedback stabilization nonlinear systems paper surveys wellknown facts recent developments topic stabilization nonlinear systems
hierarchical selection models applications
estimating constants densities different dimensions bayesian inference bayes factor defined ratio posterior versus prior posterior simply ratio constants two posterior densities many practical problems two different dimensions cases current monte carlo methods bridge sampling method path sampling method ratio importance sampling method directly applied article extend importance sampling bridge sampling ratio importance sampling problems different dimensions find global optimal importance sampling bridge sampling ratio importance sampling sense minimizing asymptotic relative errors estimators implementation algorithms asymptotically achieve optimal simulation errors developed two examples also provided
massively parallel matching knowledge structures knowledge bases used systems increase size access relevant information factor cost inference especially true analogical casebased reasoning ability system perform inference dependent efficient flexible access large base cases likely relevant solving problem hand chapter discuss novel algorithm efficient associative matching relational structures large semantic networks structure matching algorithm uses massively parallel hardware search memory knowledge structures matching given structure algorithm built massively parallel knowledge representation system runs connection machine currently exploring utility algorithm casebased planning system
sequential pac learning consider use online stopping rules reduce number training examples needed rather large training sample proved sufficient eliminate hypotheses priori idea instead observe training examples decide online whether return hypothesis training primary benefit approach detect actually training standard bounds paper presents series sequential learning procedures paclearning pac paclearning respectively analyze worst case expected training sample size procedures show often smaller existing fixed sample size bounds still providing exact worst case also provide lower bounds show best involve constant possibly log factors however empirical studies show sequential learning procedures actually use many times fewer training examples practice
dimension recurrent neural networks dimacs technical report december
adaptive global optimization local search
learning evolution neural networks
structural similarity guidance casebased design paper presents novel approach determine structural similarity guidance adaptation casebased reasoning cbr advance structural similarity assessment provides single numeric value specific structure two cases common modification rules needed obtain structure two cases approach retrieval matching adaptation group dependent processes guarantees retrieval matching similar cases together overall problem solving performance cbr case selection adaptation considerably although approach theoretical nature restricted specific domain will give example taken domain industrial building design additionally will two implementations approach
modelbased approach design analyze task context design redesign physical devices identify three types tasks differ types information take input design achieve desired behavior device design results behavior specific structural element design describe modelbased approach solving task approach uses models capture designers way device works terms causal explanations structure results behaviors also address issue indexing models memory discuss three types tasks require different types indices models finally describe system implements evaluates modelbased approach assignment
knowledge acquisition reasoning design support systems present framework knowledge acquisition development design support systems different types knowledge knowledge base design support system defined illustrated formal knowledge acquisition point special emphasis used guide acquisition application knowledge starting knowledge planning steps design problemsolving knowledge supports design formal integrated model knowledge design constructed based notion knowledge acquisition incremental process give account possibilities problem solving depending knowledge system finally different kinds knowledge interact design support system research supported research technology within joint project contract project national research center computer science technical university university university
comparison unsupervised classifiers activity sorting like objects classes without help known unsupervised classification symbolic connectionist study classification statistical classifiers search theory best explain distribution given data whereas neural network classifiers networks use vector quantization principle classifying data previously many studies compared supervised classification algorithms challenging problem comparing unsupervised classifiers largely performed empirical comparison strengths weaknesses various classifiers overall statistical classifiers especially perform better neural network
explaining casebased reasoning research casebased reasoning led development many laboratory casebased systems move towards introducing systems work environments explaining processes casebased reasoning important issue paper describe notion explaining casebased reasoning contains trace processing problemsolving provides explanation problemsolving decisions partial solution language representing problemsolving trace depends model problem solving describe knowledge model problemsolving describe representation language illustrate scheme examples interactive kritik
protein families based statistical decision theory statistical decision theory provides principled way estimate frequencies positions protein family goal minimize risk function expected distance estimates true population frequencies estimates obtained adding optimal number observed data two formulas presented one based marginal frequencies one based observed data experimental results show constructed using estimates constructed using existing methods
artificial natural learning purpose paper propose refinement notion identify bias obtain poor notion since learning device relies bias makes choose given hypothesis instead another show better captured characteristic bias related models learning shown rely bias whereas bias models include specific priori knowledge learned necessarily socalled models however turn way learn forms learning ability learning always show two properties models may sometimes candidates cognitive modelling
genetic algorithm
selective attention visual monitoring control robot vehicle reliable control autonomous vehicle requires ability focus attention important features input scene previous work autonomous following system good results conditions paper presents artificial neural network based learning approach handling difficult scenes will system work presents mechanism achieving focus attention exploiting temporal map based upon computed expectation inputs next time step indicates regions input important performing task map used features important task
value function based production scheduling production scheduling problem meet critical problem requirement maintaining product face stochastic output makes standard scheduling models jobshop inadequate currently applied algorithms simulated annealing constraint propagation must employ methods cope uncertainty paper describe markov decision process mdp formulation production scheduling captures production solution mdp value function used generate optimal scheduling decisions online simple example illustrates theoretical superiority approach methods describe industrial application two reinforcement learning methods generating approximate value function domain results demonstrate deterministic noisy scenarios value function approximation effective technique
efficient learning probabilistic concepts paper investigate new formal model machine learning concept boolean function learned may exhibit uncertain probabilistic input may sometimes classified positive example sometimes negative example probabilistic concepts may arise situations prediction measured variables accuracy insufficient determine outcome model learning learning algorithms efficient general sense perform wide class distribution domain addition giving many efficient algorithms learning natural classes study develop detail underlying theory learning
learning using dynamic feature combination selection
filtering method reducing inherent learned knowledge field paper phenomenon causes learned knowledge used problem solving problem occurs deductive problem failure branch search tree mechanism problem will force program whole thus many nodes using learned rule using rules generated learned rule first place suggest approach called filtering solve problem learners use approach problem solver filter function together knowledge acquired function problem whether use learned knowledge part use tested idea context learning system filter uses probability decide whether turn experiments show improvement performance factor paper concerned particular type occurs deductive problem employ search procedure use learned knowledge accelerate search problem failure branches search tree mechanism problem solver forces exploration whole thus search procedure will many states using learned rule using search path produced rule first place
learning act using realtime dynamic programming authors thank rich relationships heuristic search control thank rich sutton williams sharing fundamental insights subject numerous discussions thank rich sutton first making research comments grateful independently error earlier version article finally thank whose insight interest class learning problems research supported grants national science foundation air force office scientific research
object selection based correlation technical report abstract one classical topics neural networks widely used unsupervised competitive learning cortical processing control global connectivity networks however encode spatial relations input thus support sensory perceptual processing spatial relations important propose new architecture maintains spatial relations input features selection network builds locally excitatory globally inhibitory oscillator networks dynamics slow input scene many objects patterns network selects largest object system easily select several largest objects alternate time show selection network gains efficiency combining selection parallel noisy regions network applied select object real images special case selection network without local gives rise new form
reinforcement learning algorithms markovian decision processes reinforcement learning become central paradigm solving problems robotics artificial intelligence researchers almost problems controller maximize discounted sum however many problems optimal behavior limit cycle natural computationally formulate tasks controllers objective maximize average payoff received per time step paper derive new algorithms stochastic approximation methods solving system equations associated policy evaluation optimal control questions tasks algorithms analogous popular qlearning algorithms already developed case one algorithms derived significant variation algorithm preliminary empirical results presented new algorithms
exactly learning automata small cover time present algorithms exactly learning unknown environments described deterministic finite automata learner performs walk target automaton step output state labeled edge next state learner means access teacher answers equivalence queries gives learner hypotheses present two algorithms first case outputs observed learner always correct second case outputs might corrupted random noise running times algorithms polynomial cover time underlying graph target automaton
power exploring mapping directed graphs exploring mapping unknown environment fundamental problem studied variety contexts many works focused finding efficient solutions restricted versions problem paper consider model makes limited assumptions environment solve mapping problem general setting model environment unknown directed graph consider problem robot exploring mapping assume labeled thus robot given means distinguishing reason provide robot device place use identify later paper show robot upper bound number learn graph efficiently one robot know upper bound number log necessary sufficient cases algorithms deterministic
sample complexity learning bayesian networks recent years increasing interest learning bayesian networks data one effective methods learning networks based minimum description length mdl principle previous work shown learning procedure asymptotically successful probability one will converge target distribution given sufficient number samples however rate convergence unknown work examine sample complexity mdl based learning procedures bayesian networks show number samples needed learn close approximation terms entropy distance confidence ffi log ffi log log means sample complexity polynomial error threshold confidence bound also discuss constants term depend complexity target distribution finally address questions asymptotic propose method using sample complexity results speed learning process
tutorial learning bayesian networks technical report
scaling average reward reinforcement learning approximating domain models value function almost work reinforcement learning far focused methods scale domains large state spaces paper propose two extensions modelbased method called hlearning address problem extend hlearning learn action models reward functions form bayesian networks approximate value function using local linear regression test algorithms several scheduling tasks simulated automatic guided vehicle show effective significantly reducing space requirement hlearning making converge faster best knowledge results first apply ing function approximation
bayesian methods adaptive models
highdimensional structure incremental grid growing neural network understanding highdimensional real world data usually requires learning structure data space structure may contain highdimensional clusters related complex ways methods clustering selforganizing maps designed aid visualization interpretation data however methods often fail capture critical structural properties input although selforganizing maps capture highdimensional topology represent cluster boundaries clustering clusters capture local global topology paper proposes algorithm combines characteristics selforganizing maps flexible adaptive structure learns cluster bound data
transfer learning solutions elemental sequential tasks although building sophisticated learning agents operate complex environments will require learning perform multiple tasks applications reinforcement learning single tasks paper consider class sequential decision tasks called composite sequential decision tasks formed temporally number elemental sequential decision tasks elemental decomposed simpler consider learning agent learn solve set elemental composite assume structure composite tasks unknown learning agent straightforward application reinforcement learning multiple tasks requires learning tasks separately computational resources memory time present new learning algorithm modular architecture learns decomposition composite achieves transfer learning sharing solutions elemental across multiple composite solution composite constructed computationally modifications solutions constituent elemental provide proof one aspect learning algorithm
evolving avoidance behavior robot arm existing approaches learning control robot arm rely supervised methods correct behavior explicitly given difficult learn avoid using methods however examples avoidance behavior hard generate paper presents alternative approach evolves neural network controllers genetic algorithms inputoutput examples necessary since neuroevolution learns single performance measurement entire task object approach tested simulation robot arm receives visual sensory input neural networks evolved effectively avoid various locations reach random target locations
reinforcement learning soft state aggregation widely accepted use compact representations tables crucial scaling reinforcement learning algorithms realworld problems unfortunately almost theory reinforcement learning assumes table representations paper address issue combining function approximation present function approximator based simple extension state aggregation commonly used form compact representation namely soft state aggregation theory convergence arbitrary fixed soft state aggregation novel intuitive understanding effect state aggregation online new heuristic adaptive state aggregation algorithm finds improved compact representations exploiting nature soft state aggregation preliminary empirical results also presented
machine learning learning predict methods temporal differences keywords incremental learning prediction article introduces class incremental learning procedures specialized using past experience known system predict future behavior whereas conventional methods assign credit means difference predicted actual outcomes new methods assign credit means difference temporally successive predictions although temporaldifference methods used player authors adaptive heuristic poorly understood prove convergence optimality special cases relate methods realworld prediction problems temporaldifference methods require less memory less peak computation conventional methods produce accurate predictions argue problems supervised learning currently applied prediction problems sort temporaldifference methods applied advantage
integrated architectures learning planning based approximating dynamic programming paper extends previous work class architectures intelligent systems based approximating dynamic programming methods architectures integrate reinforcement learning planning single process operating world learned model world paper present show results two architectures architecture based dynamic policy iteration method related existing ideas evaluation functions universal plans reactive systems using navigation task results shown simple system simultaneously learns trial error learns world model plans optimal using evolving world model architecture based qlearning new kind reinforcement learning uses less familiar set data structures simpler implement use show architectures easy adapt use changing environments
generalization reinforcement learning successful examples using sparse coarse coding large problems reinforcement learning systems must use parameterized function approximators neural networks order generalize similar situations actions cases strong theoretical results accuracy convergence computational results mixed particular reported last years series negative results attempting apply dynamic programming together function approximation simple control problems continuous state spaces paper present positive results control tasks one significantly larger important differences used function approximators whereas used mostly global function approximators learned online whereas learned others suggested problems encountered solved using actual outcomes classical monte carlo methods algorithm however experiments always substantially performance conclude reinforcement learning work conjunction function approximators little present avoiding case general
online learning random representations consider requirements online must done incrementally realtime results learning available new example acquired despite methods learning examples used effectively online learning components reinforcement learning systems including radial basis functions selforganizing maps developed paper share structure original input representation higher dimensional representation unsupervised way map representation final answer using relatively simple supervised learner perceptron rule structures learn rapidly reliably thought either scale poorly require extensive domain knowledge contrary researchers argued representation chosen largely random good results main contribution paper develop test hypothesis show simple methods perform methods suited online learning significantly better backpropagation find size random representation increase dimensionality problem required size reduced substantially using techniques results suggest useful role play online supervised learning constructive induction
decisiontheoretic generalization online learning application boosting rule consider problem dynamically resources among set options worstcase online framework model study interpreted broad abstract extension online prediction model general decisiontheoretic setting show multiplicative rule adapted model bounds slightly cases applicable considerably general class learning problems show resulting learning algorithm applied variety problems including prediction repeated games prediction points
new learning algorithm blind signal separation new online learning algorithm minimizes statistical dependency among outputs derived blind separation mixed signals dependency measured average mutual information outputs source signals mixing matrix unknown except number sources expansion instead expansion used evaluating natural gradient approach used minimize novel activation function proposed online learning algorithm property easily implemented neural network like model validity new learning algorithm verified computer simulations
central classifier bound error bound classifier chosen early stopping key
avoiding overfitting bpsom overfitting wellknown problem fields symbolic connectionist machine learning describes generalisation performance trained model paper investigate ability novel artificial neural network bpsom avoid overfitting bpsom hybrid neural network combines feedforward network selforganising maps training supervised backpropagation learning unsupervised learning finding adequate representations show bpsom outperforms standard backpropagation also backpropagation weight decay dealing problem overfitting addition show bpsom preserving generalisation performance pruning methods fail
iterated revision minimal change conditional beliefs describe model iterated belief revision extends theory revision account effect revision conditional beliefs agent particular model agent makes changes possible conditional component belief set test minimal conditional revision provides acceptance conditions arbitrary show problem determining acceptance nested conditional reduced acceptance tests thus iterated revision accomplished virtual manner using revision
learnability discrete distributions extended abstract
issues using function approximation reinforcement learning reinforcement learning techniques address problem learning select actions unknown dynamic environments widely use complex domains reinforcement learning techniques must combined generalizing function approximation methods artificial neural networks little however understood theoretical properties combinations many researchers encountered failures practice paper identify source systematic utility values using qlearning example give theoretical account phenomenon deriving conditions one may expected cause learning fail employing popular function approximators present experimental results support theoretical findings
approach blind separation blind derive new selforganising learning algorithm information network nonlinear units algorithm assume knowledge input distributions defined limit conditions information extra properties found linear case transfer function able higherorder input distributions perform true reduction units output representation enables network separate statistically independent components inputs higherorder generalisation principal components analysis apply network source separation problem successfully separating unknown mixtures ten also show variant network architecture able perform blind unknown speech signal finally derive dependencies information transfer time suggest information provides unifying framework problems blind signal processing please comments paper will appear neural computation reference version technical report institute neural computation san
operations learning graphical models decomposition techniques graphical models provide paper review empirical statistical learning graphical model perspective wellknown examples graphical models include bayesian networks directed graphs representing markov chain networks representing markov field graphical models extended model data analysis empirical learning using graphical operations simplifying problem provided including decomposition manipulation probability models exponential family two standard algorithm schemas learning reviewed graphical framework gibbs sampling expectation maximization algorithm using operations schemas popular algorithms synthesized graphical specification includes versions linear regression techniques feedforward networks learning gaussian discrete bayesian networks data paper concludes implications data analysis popular algorithms fall within framework presented
empirical approach solving general utility problem speedup learning utility problem speedup learning describes common behavior machine learning methods performance due increasing amounts learned knowledge shape learning curve cost using learning method number training examples several domains suggests parameterized model relating performance amount learned knowledge mechanism limit amount learned knowledge optimal performance many recent approaches avoiding utility problem speedup learning rely sophisticated utility measures significant numbers training data accurately estimate utility control knowledge empirical results presented indicate simple selection strategy control rules derived training problem explanation quickly defines efficient set control knowledge training problems simple selection strategy provides alternative approaches improving speed problem solver experimentation illustrates existence minimum representing least cost learning curve training examples controlling amount learned knowledge opposed knowledge attempt also made relate domain characteristics shape learning curve
comparison kernel estimators perceptrons functions speech classification compare kernel estimators single perceptrons functions problems classification handwritten digits speech taking two different applications employing many techniques report twodimensional study assessment learning methods possible consider feedforward network one hidden layer examples local methods use kernel estimators like knearest neighbor knn generalized knn grow learn nearest neighbor also considered fuzzy knn due similarity distributed networks use linear perceptron pairwise separating linear perceptron multilayer perceptrons sigmoidal hidden units also tested function network combination local distributed networks four criteria taken comparison correct classification test set network size learning time complexity found perceptrons architecture suitable generalize better local memorybased kernel estimators require longer training precise computation local networks simple learn quickly use memory
learning improve case adaptation introspective reasoning cbr current cbr systems case adaptation usually performed rulebased methods use rules system ability define rules depends knowledge task domain may available priori presenting serious cbr systems needed adaptation knowledge paper describes ongoing research method address problem acquiring adaptation knowledge experience method uses reasoning scratch based introspective reasoning requirements successful adaptation build library adaptation cases stored future reuse describe approach types knowledge requires initial computer implementation lessons learned open questions study
representing memory search position paper framework modeling introspective reasoning discusses relevance framework modeling introspective reasoning memory search argues effective flexible memory processing rich memories built five types explicitly represented knowledge information needs relationships different types information actual behavior information search process behavior representations relate actual performance approach modeling memory search general principles modeling introspective reasoning step towards addressing problem reasoner human acquire knowledge properties knowledge base
machine learning multistrategy approach vol macro multistrategy learning machine learning techniques great potential means acquisition knowledge nevertheless use complex engineering domains still machine learning techniques studied context knowledge acquisition defined tasks classification learning tasks handled relatively simple algorithms complex domains present difficulties combining strengths several learning techniques weaknesses providing alternative learning strategies study presents two macro issue multistrategy learning macro perspective deals decomposition overall complex learning task relatively learning tasks perspective deals designing multistrategy learning techniques supporting acquisition knowledge task two discussed context
introspective reasoning using multistrategy learning order learn effectively reasoner must possess knowledge world able improve knowledge also must reason performs given task particular knowledge needs improve performance current task requires declarative representations reasoning performed system performance task systems knowledge organization knowledge chapter presents taxonomy possible reasoning failures occur performance task declarative representations failures failures particular learning strategies theory based explanation structures help system identify failure types formulate learning goals choose appropriate learning strategies order avoid similar mistakes future theory implemented computer model introspective reasoner performs multistrategy learning story understanding task
mean field learning algorithm unsupervised neural networks introduce learning algorithm unsupervised neural networks based ideas statistical mechanics algorithm derived mean field approximation large sigmoid belief networks show approximately infer statistics networks without sampling done solving mean field equations relate statistics unit markov using statistics target values weights network adapted local rule evaluate strengths weaknesses networks problems statistical pattern recognition
investigation noisetolerant relational concept learning algorithms discuss types noise may occur relational learning systems describe two approaches addressing noise relational concept learning algorithm evaluate approach experimentally
neural learning chaotic dynamics error propagation algorithm neural network identify technical report abstract
nonparametric selection input variables connectionist learning
learning avoid reinforcement learning paradigm mobile robot navigation paper describes control system mobile robot based sensor information control system provide signal way avoided since case examples available system learns basis external reinforcement signal negative case zero otherwise describe adaptive algorithm used discrete coding state space adaptive algorithm learning correct mapping input state vector output signal
perception feedback partially supported advanced research projects agency partially supported air force office scientific research advanced research projects agency onr office naval research onr partially air force office scientific research office naval research onr onr
approximation spaces integer translates problem approximating smooth functions spaces integer translates symmetric function understood case points ffi approximation problem understood stationary setting work nonstationary setting assumption ffi small results similar many known results case ffi apply specifically examples kernel generalized
toward efficient agnostic learning paper investigation generalizations probably approximately correct pac learning model attempt significantly target function assumptions goal direction agnostic learning make assumptions target function derives fact designers learning algorithms give belief nature represented target function simple explanation give number positive negative results provide initial outline possibilities agnostic learning results include results obvious generalization pac model agnostic setting efficient general agnostic learning method based dynamic programming relationships loss functions agnostic learning algorithm learning problem involves hidden variables
separation visual cortex
design implementation casebased planning framework within planner
design implementation replay framework based partial order planner paper describe design implementation derivation replay framework based within partial order planner previous plan derivations first earlier decisions context new problem situation extending path obtain complete solution new problem path extended new solution explanationbased learning ebl techniques employed identify features new problem extension features added retrieval stored case keep retrieval costs low normally plan derivations individual goals one derivations solving problems cases covering multiple goals stored individual goals successfully aim constructing case library predict goal interactions store case set interacting goals provide empirical results demonstrating effectiveness improving planning performance problems drawn complex domain
learning control memories alternative dynamic recurrent networks neural computation previous algorithms supervised sequence learning based dynamic recurrent networks paper describes alternative class systems consisting two feedforward nets learn deal temporal sequences using fast weights first net learns produce context dependent weight changes second net whose weights may vary quickly method offers potential storage efficiency single weight instead unit may sufficient temporal information various learning methods derived two experiments unknown time illustrate approach one experiment shows system used adaptive variable binding
method tree reconstruction evolutionary tree reconstruction important step many biological research problems yet extremely difficult variety computational statistical scientific reasons particular reconstruction large trees containing significant amounts especially challenging present paper new tree reconstruction method call method used accurate evolutionary tree otherwise intractable datasets obtains decomposition input dataset small overlapping sets closely related trees subsets using base method choice combines one tree entire set analyzed smaller computationally expensive methods maximum likelihood estimation used without much cost time within subset closely related even simple methods much likely highly accurate result methods typically faster accurate compared naive use method paper describe basic ideas techniques demonstrate advantages experimentally simulating sequence evolution variety trees
learning semantic grammars constructive inductive logic programming construction semantic grammars difficult interesting problem machine learning paper shows acquisition problem viewed learning heuristics logic program appropriate control rules learned using new firstorder induction algorithm automatically useful syntactic semantic categories empirical results show learned generalize novel sentences outperform previous approaches based connectionist techniques
dynamic predication instruction set architectures conventional speculative architectures use branch prediction evaluate likely execution path program execution however certain branches difficult predict one solution problem evaluate paths following conditional branch predicated execution used implement form multipath execution predicated architectures issue instructions associated predicates predicates indicate instruction result branch reduces number branches executed eliminating branch cost executing additional instructions paper propose restricted form multipath execution called dynamic predication architectures little support predicated instructions instruction set dynamic predication dynamically predicates instruction sequences form branch executing paths branch branch short forward branch instructions form construct constructs stage sequence predicated instruction sequence dynamically execution results show dynamic predication speedups
model guided plasticity auditory spatial map selforganization auditory map space external strongly influenced vision nature interaction unknown paper biologically plausible model selforganization proposed receives learn signal based visual attention visual attention focused spatial location auditory input learn signal map allowed adapt twodimensional kohonen map used model simulations performed evaluate learn signal affect auditory map primary area visual attention different spatial locations auditory map corresponding location shift complete done early development partial done later similar results observed visual field modified therefore simulations suggest learn signal based visual attention possible explanation auditory plasticity
separating hippocampal maps spatial functions hippocampal formation place fields hippocampal cells old animals sometimes change animal removed environment ensemble correlation two sequential environment shows strong old animals near greater similar representation experiences strong young animals greater similar representation experiences one explanation hypothesis multiple maps encoded old animals may sometimes map theory proposed suggests experiment implies maps region offer alternative explanation properties region interact errors path environment produce
active recognition using partially observable markov decision processes mit laboratory perceptual computing section technical report appeared ieee conference pattern recognition abstract present recognition system active features based reinforcement learning paradigm using vision routines previously implemented interactive environment determine spatial location body parts user guide active obtain images expressions reinforcement learning paradigm based partially observable markov decision process used implement visual attention attention module selects targets based goal successful recognition uses new qlearning formulation given set target system learn discriminate particular
every method fitness sharing implicit sharing compared various extensions genetic algorithm attempt find optima search space containing several optima many natural coevolutionary learning range management control problems learning game strategies methods must find optima however suitable comparison studies compare two similar methods fitness sharing implicit sharing using realistic classification problem find advantages different circumstances implicit sharing covers optima population large enough species form optimum population large enough fitness sharing find optima larger bases implicit sharing easily indicates trying find many optima possible implicit sharing works population large enough requires prior knowledge many exist
knowledge compilation towards practical design support systems modern knowledge systems design typically employ multiple problemsolving methods turn use different kinds knowledge construction heterogeneous knowledge system support practical design thus two fundamental questions design information support heterogeneous design processing partial answers questions exist separately databases already contain amounts generalpurpose design information addition modern knowledge systems typically characterize kinds knowledge needed specific problemsolving methods quite precisely leads hypothesize compilation potential mechanism integrating heterogeneous knowledge systems databases design paper first outline general computational architecture called integration focus specific issue data database form appropriate problemsolving method used heterogeneous knowledge system describe experiment knowledge system called interactive kritik integrated oracle database using communication tool limited experiment indicates computational feasibility compilation also additional research issues
first experiments using mixture nonlinear experts time series prediction paper investigates advantages mixture experts model introduced connectionist community applied time series analysis two time series dynamics understood first series series consisting mixture noisefree process quadratic map noisy process noisy linear autoregressive three main results model produces significantly better results single networks discovers correctly also allows characterize due correct matching noise level model data avoids overfitting second series series used santa competition model also obtains excellent outofsample predictions allows analysis shows overfitting
learning viewpoint invariant representations faces attractor network natural visual experience different views object tend appear close temporal animal object around investigated ability attractor network acquire view invariant visual representations first neighbors pattern sequence pattern sequence contains successive views faces ten individuals change pose network dynamics developed multiple views given subject fall use independent component ica representation faces input patterns bell sejnowski ica representation advantages principal component representation pca recognition without attractor network suggesting ica better representation pca object recognition
analysis numerical effects parallelism parallel genetic algorithm paper examines effects synchronization numerical parallel efficiency parallel genetic algorithms gas describe structured parallel genetic algorithm experiments provide preliminary evidence asynchronous versions algorithms lower run time gas analysis shows improvement due synchronization costs high numerical efficiency fewer function evaluations asynchronous gas analysis includes utility traditional parallel performance measures parallel gas
support vector machine approach decision trees key ideas statistical learning theory support vector machines generalized decision trees support vector machine used decision tree optimal decision tree characterized dual space formulation constructing tree proposed result method generating simple decision trees multivariate linear nonlinear decisions preliminary results indicate method produces simple trees generalize respect decision tree algorithms single support vector machines
regularization theory neural networks architectures previously shown regularization principles lead approximation schemes equivalent networks one layer hidden units called regularization networks particular standard smoothness lead subclass regularization networks known radial basis functions approximation schemes paper shows regularization networks much range approximation schemes including many popular general additive models neural networks particular introduce new classes smoothness lead different classes basis functions additive splines product splines obtained appropriate classes smoothness furthermore generalization extends radial basis functions rbf basis functions also leads additive models ridge approximation models containing special cases functions forms projection pursuit regression several types neural networks propose use term generalized regularization networks broad class approximation schemes follow extension regularization probabilistic interpretation regularization different classes basis functions correspond different classes prior probabilities approximating function spaces therefore different types smoothness assumptions summary different multilayer networks one hidden layer call generalized regularization networks correspond different classes priors associated smoothness classical regularization principle three broad classes radial basis functions generalized basis functions product splines additive splines generalized schemes type ridge approximation functions several neural networks layer paper will appear neural computation vol pages earlier version
interactive segmentation medical images extended abstract
figure architecture kohonen network input neuron fully connected
theory networks approximation learning learning inputoutput mapping set examples type many neural networks constructed perform approximation multidimensional function solving problem reconstruction point view form learning closely related classical approximation techniques generalized splines regularization theory paper considers problems exact representation detail approximation linear nonlinear mappings terms simpler functions fewer variables theorem concerning representation functions several variables terms functions one variable turns almost irrelevant context networks learning develop theoretical framework approximation based regularization techniques leads class networks call generalized radial basis functions since mathematically related wellknown radial basis functions mainly used tasks networks equivalent generalized splines also closely related pattern recognition methods potential functions several neural network algorithms associative memory backpropagation topology preserving map also interesting interpretation terms prototypes synthesized combined learning stage paper introduces several extensions applications technique discusses data massachusetts institute technology paper describes research done within center biological information processing department brain cognitive sciences artificial intelligence laboratory research sponsored grant office naval research onr cognitive neural sciences division artificial intelligence center aircraft corporation foundation national science foundation support laboratorys artificial intelligence research provided advanced research projects agency department defense contract part onr contract
indexing refinement incremental learning cases article describes reasoner improve understanding understood domain application already novel problems domain casebased reasoning process using past experiences stored memory understand novel situations solve novel problems however process assumes past experiences understood provide good lessons used future situations assumption usually false one learning novel domain since situations encountered previously domain might understood completely furthermore reasoner may even case deals new situation may able access case using existing indices present theory incremental learning based revision previously existing case knowledge response experiences situations theory implemented casebased story understanding program learn new case situations case already exists learn index case memory incrementally refine understanding case using reason new situations thus evolving better understanding domain experience research work casebased reasoning providing mechanisms case library automatically built use casebased reasoning program
generalized hidden markov model recognition human genes dna present statistical model genes dna generalized hidden markov model provides framework describing grammar parse dna sequence probabilities assigned transitions states generation base given particular state machine learning techniques applied optimize probabilities using training set given new candidate sequence best parse model using dynamic programming algorithm identify path model maximum probability flexible modular new sensors additional states easily addition provides simple solutions integrating constraints reading frame constraints searching description results implementation model called presented sensor frequency model frequency two neural networks used splice site prediction show simple model performs quite standard test set genes human dna system identified bases correctly specificity exons exactly identified specificity shown perform favorably compared several systems
optimality repeated games bounded examine questions optimality repeated stage games one may strategies perhaps different computationally bounded sets also consider optimality bounded convergence rates infinite payoff develop notion period handle problem strategies
efficient algorithms learning play repeated games computationally bounded study problem efficiently learning play game unknown adversary chosen computationally bounded class contribute line research playing games finite automata scope research considering new classes introduce natural notions games recent history whose current action determined simple boolean formula recent history play games statistical whose current action determined simple function statistics entire history play cases give efficient algorithms learning play difficult game called contract also give powerful positive result date learning play finite automata efficient algorithm learning play game finite automata probabilistic actions low cover time
decision tree system finding genes dna morgan integrated system finding genes dna sequences morgan uses variety techniques task decision tree classifier decision tree system combined new methods identifying start donor sites acceptor sites together dynamic programming algorithm finds optimal segmentation dna sequence coding noncoding regions exons introns optimal segmentation dependent separate scoring function takes assigns score probability sequence scoring functions morgan sets decision trees combined give probability estimate experimental results database dna sequences show morgan excellent performance many different measures separate test set achieves overall accuracy correlation sensitivity specificity coding bases addition morgan identifies coding exons exactly beginning end coding regions predicted correctly paper describes morgan system including decision tree routines algorithms site recognition performance benchmark database dna
use neural networks support intelligent scientific computing paper report use backpropagation based neural networks implement phase computational intelligence process expert system supporting numerical simulation applications partial differential equations based reasoning system provides advice method parameters use simulation specified based application advice characteristics given model matched characteristics previously seen classes models performance various solution methods previously seen similar classes models used basis predicting method use thus major step reasoning process involves analysis categorization models classes models based characteristics study demonstrate use neural networks identify class predefined models whose characteristics match ones specified based application
inductive learning compact rule sets using efficient hypotheses reduction method described reduces hypotheses space efficient easily reduction criteria called reduction learning algorithm described based reduction analyzed using probability approximate correct learning results results obtained reducing rule set equivalent set formulas goal learning algorithm induce compact rule set describing basic dependencies within set data reduction based criterion gives semantic interpretation rules criteria comparison hypotheses reduction show reduction improves search smaller probability
variables
receptive field parameters affect neural learning identify three principle factors performance learning networks localized units unit noise sample density structure target function analyze effect unit receptive field parameters factors use analysis propose new learning algorithm dynamically receptive field properties learning
reinforcement learning methods markov decision problems decision problems continuous time generalizations discrete time markov decision problems number reinforcement learning algorithms developed recently solution markov decision problems based ideas asynchronous dynamic programming stochastic approximation among qlearning realtime dynamic programming decision problems optimality equation context propose algorithms similar named adapted solution decision problems demonstrate algorithms applying problem determining optimal control simple system conclude discussion circumstances algorithms may
classification using hierarchical mixtures experts recently interest use multiple models classification regression statistics neural networks hierarchical mixture experts successful number regression problems significantly faster training use expectation algorithm paper extend classification results reported three common classification benchmark tests parity two
statespace abstraction evaluation probabilistic networks one important factor determining complexity evaluating probabilistic network state spaces nodes varying state spaces one trade accuracy result computational efficiency present time procedure approximate evaluation probabilistic networks based idea application simple networks exhibits smooth improvement quality computation time increases suggests statespace abstraction one useful control parameter designing real time probabilistic
measuring difficulty specific learning problems existing complexity measures learning theory applied specific learning problems training sets moreover typically making assumptions way learner will operate lack satisfactory generic complexity measure learning problems difficulties researchers various areas present paper forward idea may help shows supervised learning problems fall two generic complexity classes one associated computational determining class particular problem thus effectively evaluate degree generic difficulty
statistical biases backpropagation learning keywords cognitive science pattern recognition paper investigates statistical effects may need exploited supervised learning notes effects classified according order proposes learning algorithms will typically form bias towards particular classes effect presents results empirical study statistical bias backpropagation study involved applying algorithm wide range learning problems using variety different internal architectures results study backpropagation specific bias general direction statistical rather relational effects paper shows existence bias effectively algorithms ability noise
rbf network function regression image segmentation preliminary results abstract radial basis function rbf networks increase number degrees freedom complexity inputoutput mapping estimated basis supervised training data set due superior expressive power gaussian rbf model supervised growing neural gas selected literature employs learning strategy capable generating removing hidden units synaptic connections slightly modified version tested function estimator training surface image signal whose size finite relationship generation learning system maps hidden units presence image homogeneous subsets segments investigated unfortunately examined version performs poorly function estimator image may due intrinsic learning strategy structural parameters output weights simultaneously consistently framework rbf networks studies investigate combination learning strategies generation criteria internal report paper image segmentation rbf networks feasibility study presented conference applications science neural networks fuzzy systems evolutionary computation part international symposium optical science engineering san
learning symbolic rules using artificial neural networks distinct advantage symbolic learning algorithms artificial neural networks typically concept representations form easily understood humans one approach understanding representations formed neural networks extract symbolic rules trained networks paper describe investigate approach extracting rules networks uses extraction algorithm network training method soft previously algorithm successfully applied knowledgebased neural networks experiments demonstrate extracted rules generalize better rules learned using c45 system addition accurate extracted rules also reasonably comprehensible
figure time complexity unit parallelism measured theoretical prediction time experience showed parallel algorithm simulating neural networks desirable even possible obtain efficient solution single training algorithm believe advantages clear easy understand program approaches allowing specific machine neural network algorithm currently investigate neural network models resulting parallel algorithms composed common basic building blocks logarithmic tree efficient communication structure connections connections hinton sejnowski learning algorithm boltzmann machines cognitive science implementing neural network models parallel computers computer journal vol latency message architectures international parallel processing symposium ieee computer press using designing massively parallel computers artificial neural networks journal parallel distributed computing vol efficient parallel learning algorithms neural networks advances neural information processing systems kohonen selforganization associative memory neural network simulation speed connections per second ieee neural networks dynamic selection training patterns neural networks new method control generalization technical report technical university hinton williams learning internal representations error propagation eds parallel distributed processing cognition vol press cambridge comparison optimized backpropagation algorithms proc symposium artificial neural networks learning backpropagation nets perspective science eds techniques applications neural networks implementation backpropagation learning large simd parallel computer parallel computing vol backpropagation algorithm grid architectures parallel computing vol
using introspective reasoning select learning strategies order learn effectively system must possess knowledge world able improve knowledge also must reason performs given task particular knowledge needs improve performance current task requires representation reasoning performed system performance task paper presents taxonomy possible reasoning failures occur task declarative representations particular learning strategies propose theory explanation structures help system identify failure types choose appropriate learning strategies order avoid similar mistakes future program called theory processes examples domain drug
input state parameterized families systems key words nonlinear stability robust control
extracting rules artificial neural networks distributed representations although artificial neural networks applied variety realworld scenarios success often low degree human comprehensibility techniques compact sets symbolic rules artificial neural networks offer promising perspective overcome obvious neural network representations paper presents approach extraction rules artificial neural networks key mechanism validity interval analysis generic tool extracting symbolic knowledge knowledge neural networks empirical studies robot arm domain illustrate proposed method extracting rules networks realvalued distributed representations
toward optimal feature selection paper examine method feature subset selection based information theory initially framework defining theoretically optimal computationally intractable method feature subset selection presented show goal eliminate feature gives little additional information beyond remaining features particular will case irrelevant redundant features give efficient algorithm feature selection computes approximation optimal feature selection criterion conditions approximate algorithm successful examined empirical results given number data sets showing algorithm effectively datasets large numbers features
chapter reinforcement learning planning control
decision trees abstract cases paper address problem casebased learning presence irrelevant features review previous work attribute selection present new algorithm greedy pruning decision trees effectively store set abstract cases memory hypothesize approach will efficiently identify relevant features even interact parity concepts report experimental results artificial domains support hypothesis experiments natural domains show improvement cases others discuss implications experiments consider additional work irrelevant features outline directions future research
learning boolean concepts presence many irrelevant features
robot developing agents learning learning plays role development agents paper explore use reinforcement learning shape robot perform predefined target behavior simulated real robots parallel implementation learning classifier system extended genetic algorithm classifying different kinds behaviors explore effects learning different types agents architecture flat hierarchical training strategies particular hierarchical architecture requires agent learn coordinate basic learned responses show best results achieved agents architecture training strategy match structure behavior pattern learned report results number experiments carried simulated real environments show results simulations real robots experiments deal simple reactive behavior one demonstrate use simple general memory mechanism whole experimental activity demonstrates classifier systems genetic algorithms employed develop autonomous agents
computational complexity reduction networks using similarity states although probabilistic inference general bayesian belief network problem inference computation time reduced practical cases exploiting domain knowledge making appropriate approximations knowledge representation paper introduce property similarity states new method approximate knowledge representation based property define two states node similar likelihood ratio probabilities depend nodes network show similarity states joint probability distribution exploited reduce computational complexity probabilistic inference networks multiple similar states example show two layer networks often used diagnostic reduced close network multiple similar states probabilistic inference new network done polynomial time respect size network results queries practical importance close results obtained exponential time original network error introduced reduction converges zero faster exponentially respect degree polynomial describing resulting computational complexity
changing rules approach theory refinement induction decision trees machine
bayesian unsupervised learning higher order structure multilayer architectures used bayesian belief networks machines provide powerful framework representing learning higher order statistical relations among inputs exact probability calculations models often intractable much interest finding approximate algorithms present algorithm efficiently discovers higher order structure using gibbs sampling model interpreted stochastic recurrent network states feedback higher levels demonstrate performance algorithm problems
learning presence malicious errors paper study extension model learning introduced also known probably approximately correct pac model allows presence malicious errors examples given learning algorithm errors generated adversary computational power access entire history learning algorithms computation thus study worstcase model errors results include general methods bounding rate error learning algorithm efficient algorithms nontrivial rates malicious errors problems learning errors standard combinatorial optimization problems
comparing bayesian model class selection criteria discrete finite mixtures investigate problem computing posterior probability model class given data sample prior distribution possible parameter settings model class mean group models share parametric form general posterior may hard compute highdimensional parameter spaces usually case realworld applications literature several methods computing posterior approximately proposed quality approximations may depend heavily size available data sample work interested testing methods perform realworld problem domains order conduct study chosen model family finite mixture distributions certain assumptions able derive model class posterior analytically model family report series model class selection experiments realworld data sets true posterior approximations compared empirical results support hypothesis techniques provide good estimates true posterior especially sample size grows large
constructing bayesian finite mixture models algorithm email report university department computer science abstract paper explore use finite mixture models building decision support systems capable sound probabilistic inference finite mixture models many properties computationally efficient prediction reasoning phase universal sense approximate problem domain distribution handle present formulation model construction problem bayesian framework finite mixture models describe bayesian inference performed given model model construction problem seen missing data estimation describe realization expectationmaximization algorithm finding good models prove feasibility approach report empirical results several available classification problem datasets compare results corresponding results obtained alternative techniques neural networks decision trees comparison based best results reported literature datasets question appears using theoretically sound bayesian framework suggested reported results outperformed relatively small effort
modeling casebased planning reasoning failures one application models reasoning behavior allow reasoner detect failures reasoning process address issues models versus specificity knowledge kinds knowledge needed knowledge structured evaluation introspective reasoning systems present system implements model planning processes improve planner response reasoning failures show hierarchical model model generality access details discuss qualitative quantitative measures used evaluating introspective component
reinforcement learning consider sequential decision making problems criteria ordered according importance structural properties problems reinforcement learning algorithms learn asymptotically optimal decisions derived computer experiments confirm theoretical results provide insight learning processes
bayesian model averaging model selection markov equivalence classes acyclic acyclic widely used describe among variables multivariate distributions particular likelihood functions models recursive often allow explicit maximum likelihood estimates suited building bayesian networks expert systems may however many determine dependence markov model thus family given set naturally classes class associated unique statistical model statistical procedures model selection model averaging fail take account equivalence classes may substantial computational recent results shown class determined single chain graph essential graph simultaneously equivalence class propose two stochastic bayesian model averaging selection algorithms essential graphs apply analysis three data sets
constructing computationally efficient bayesian models via unsupervised clustering probabilistic reasoning bayesian belief networks given set samples unknown probability distribution study problem constructing good bayesian network model probability distribution question task viewed search problem goal find maximal probability network model given data work make attempt learn arbitrarily complex bayesian network structures since resulting models practical purposes due exponential amount time required reasoning task instead restrict special class simple treestructured bayesian networks called bayesian prototype trees polynomial time algorithm bayesian reasoning exists show probability given bayesian prototype tree model evaluated given data evaluation criterion used stochastic simulated annealing algorithm searching model space simulated annealing algorithm provably finds maximal probability model provided sufficient amount time used
use simple example main ideas probability specifically show use acceptance rule naturally leads use intervals represent probabilities change due experience probabilities concerning experiments events computed given proper knowledge underlying distributions
stabilization stability
concept learning heuristic classification domains
learning use selective attention shortterm memory sequential tasks paper presents reinforcement learning algorithm uses selective attention shortterm memory simultaneously address problems large perceptual state spaces hidden state combining advantages work instancebased memorybased learning work robust statistical tests separating noise task structure method learns quickly creates state noise uses treestructured representation related work prediction trees variable resolution dynamic programming builds memory used shortterm memory selective perception algorithm demonstrated solving driving task agent around slower faster traffic agent uses active perception simulated eye movements environment hidden state time pressure world states environment sensory system agent uses distinction test build tree represents memory necessary just internal fewer states
monotonic measure optimal feature selection feature selection problem choosing subset relevant features general exhaustive search optimal subset monotonic measure exhaustive search avoided without optimality unfortunately error measures monotonic new measure employed work monotonic fast compute search relevant features according measure guaranteed complete exhaustive experiments conducted
hardware mechanism dynamic memory references
learning optimal dialogue strategies case study dialogue agent email paper describes novel method dialogue agent learn choose optimal dialogue strategy widely dialogue strategies formulated terms little work automatically optimizing agents choices multiple ways method based combination learning algorithms empirical evaluation techniques learning component method based algorithms reinforcement learning dynamic programming qlearning empirical component uses evaluation framework identify important performance factors provide performance function needed learning algorithm illustrate method dialogue agent named email interactive system supports access email show learn choose among alternate strategies agent reading email
incremental reduced error pruning paper outlines problems may occur reduced error pruning inductive logic programming efficiency new method incremental reduced error pruning proposed attempts address problems experiments show many noisy domains method much efficient alternative algorithms along gain accuracy however experiments show use algorithm domains specific concept description
determining mental state eeg signals using parallel implementations neural networks eeg analysis key role modeling cortical dynamics relatively little effort devoted developing eeg limited means communication several mental states reliably distinguished recognizing patterns eeg device like sequences mental states eeg pattern recognition difficult problem success finding representations eeg signals patterns distinguished article report study comparing three eeg representations signals representation using transform representation classification performed neural network implemented server processor simd architecture adaptive solutions execution time comparisons show speed best classification accuracy samples using representation
reinforcement learning survey paper surveys field reinforcement learning perspective written researchers familiar machine learning historical basis field broad selection current work reinforcement learning problem agent learns behavior interactions dynamic environment work described work psychology differs considerably details use word reinforcement paper discusses central issues reinforcement learning including trading exploration exploitation foundations field via markov decision theory learning delayed reinforcement constructing empirical models accelerate learning making use generalization hierarchy hidden state concludes survey implemented systems assessment practical utility current methods reinforcement learning
adding memory xcs add internal memory xcs classifier system test xcs internal memory named environments two four states experimental results show easily converge optimal solutions simple environments moreover performance stable respect size internal memory involved learning however results present evidence complex environments may fail evolve optimal solution results suggest exploration strategies currently employed xcs adequate guarantee convergence optimal policy complex environments
hill climbing learning abstraction genetic algorithm simple modification standard hill climbing optimization algorithm taking account learning features discussed basic concept approach socalled probability vector single determine probabilities vectors vector used random generation vectors form neighborhood specified given probability vector within neighborhood best solutions smallest functional values function feature learning introduced probability vector updated formal hebbian learning rule wellknown theory artificial neural networks process repeated probability vector close either zero one resulting probability vector determines vector may interpreted optimal solution given optimization task genetic algorithms discussed effectiveness proposed method illustrated example global minima highly multimodal function
trading spaces computation representation limits learning research paper supported research fellowship joint cognitive science human computer interaction one authors support order arbitrary
efficient algorithms identifying relevant features paper describes efficient methods exact approximate implementation bias consistent hypotheses features possible bias useful learning domains many irrelevant features present training data first introduce new algorithm exactly implements bias algorithm empirically shown substantially faster focus algorithm previously given introduce algorithms apply efficient heuristics approximating bias algorithms employ greedy heuristics trade optimality computational efficiency experimental studies show learning performance id3 greatly improved algorithms used training data eliminating irrelevant features particular algorithm provides excellent efficient approximation
statistical approach decision tree modeling statistical approach decision tree modeling described approach decision tree modeled process output generated input sequence decisions resulting model yields likelihood measure fit allowing map estimation techniques utilized efficient algorithm presented estimate parameters tree model selection problem presented several alternative considered hidden markov version tree described data sequences temporal dependencies
free energy minimization algorithm three binary vectors length binary matrix task infer given given assumptions statistical properties problem arises noisy communication using errorcorrecting code based parity original signal inference sequence linear feedback shift register noisy observation sequence assume aim find large exhaustive search possible sequences feasible one way combinatorial problem create related continuous optimization problem discrete variables real variables derive continuous representation terms free energy approximation posterior distribution
perceptual development learning behavioral evidence computational models intelligent system capable adapting changing environment therefore capable learning perceptual interactions requires certain amount plasticity structure attempt model perceptual capabilities living system construct synthetic system comparable abilities must therefore account plasticity variety developmental learning mechanisms paper examines results behavioral studies development visual perception integrates computational framework suggests several interesting experiments computational models yield insights development visual perception order understand development information processing structures brain one needs knowledge changes context normal environment however knowledge development settings also extremely useful reveals extent development function environmental experience opposed determined consider development visual system normal restricted conditions role experience early development sensory systems general visual system particular widely studied variety experiments involving carefully controlled manipulation environment presented animal extensive reviews results found examples manipulation visual experience total pattern selective certain class patterns animals vision etc extensive studies involving behavioral resulting total visual pattern indicate arise primarily result visual information processing brain results experiments suggest specific developmental learning mechanisms may operating various stages development different levels system will discuss working draft comments especially constructive suggestions improvement will introducing literature development helpful comments initial draft paper numerous researchers whose experimental work provided basis model paper research partially supported grants national science foundation university graduate school
diffusion context credit information markovian models paper studies problem transition probability matrices markovian models hidden markov models hmms makes difficult task learning represent longterm context sequential data phenomenon forward propagation longterm context information learning hidden state representation represent longterm context depends credit information time using results markov chain theory show problem diffusion context credit reduced transition probabilities approach transition probability matrices sparse model essentially deterministic results found paper apply learning approaches based continuous optimization gradient descent algorithm
requirements use neural networks industrial modern today needs flexible adaptive methods information processing several applications shown neural networks requirements paper application areas neural networks successfully used presented kind check list described different steps applying neural networks paper discussion neural networks projects done research group interactive planning research center computer science
neural network based system gas pipelines need regular intervals application developed special based based called research center computer science developed cooperation automatic system called task detect like loss kernel tool neural classifier trained using collected examples following paper focus aspects use learning methods industrial application
recognizing handwritten digits using mixtures linear models construct mixture locally linear generative models collection images digits use recognition different models given digit used capture different styles new images classified evaluating model use algorithm computationally straightforward principal components analysis pca incorporating information expected local requires adding vectors sample covariance matrices pca improves performance
nonlinear gated experts time series discovering avoiding overfitting international journal neural systems paper university computer science technical report analysis prediction realworld systems two key problems form switching overfitting particularly serious noisy processes article addresses problems using gated experts consisting nonlinear gating network several also nonlinear competing experts expert learns predict conditional mean expert adapts width match noise level regime gating network learns predict probability expert given input article focuses case gating network bases decision information inputs hidden markov models decision based previous states output gating network previous time step averaging several predictors contrast gated experts input space article discusses underlying statistical assumptions derives weight update rules compares performance gated experts standard methods three time series series obtained randomly switching two nonlinear processes time series santa time series competition light chaotic state daily realworld multivariate problem structure several time scales main results gating network correctly discovers different process associated expert important segmentation task used characterize less overfitting compared single networks homogeneous multilayer perceptrons since experts learn match local noise levels viewed matching local complexity model local complexity data
drug design machine learning modelling drug activity paper describes approach modelling drug activity using machine learning tools experiments modelling quantitative relationship using standard method machine learning system already reported literature paper describes results applying two machine learning systems assistant data results achieved machine learning systems better results method therefore machine learning tools considered promising solving kind problems given results also illustrate variations performance different machine learning systems applied drug design problem
rule based database integration heterogeneous intelligent processing engineering design paper describe one aspect research project called addressed problem performing design engineering devices heterogeneous databases end system interactive kritik multimodal reasoning system combined case based model based reasoning solve design problem paper focuses processing five types queries received end evaluated mapping appropriately using facts schemas underlying databases rules establish among data databases terms relationships equivalence set approach fact mapping process query received end evaluated respect large number possibilities possibilities encoded form rules consider various ways given query may match relation values underlying tables approach implemented using deductive database system rule processing engine
learning achieve goals temporal difference methods solve temporal credit assignment problem reinforcement learning important general reinforcement learning learning achieve dynamic goals although existing temporal difference methods learning applied problem take advantage special structure paper presents algorithm learns efficiently achieve dynamically changing goals exhibits good knowledge transfer goals addition paper shows traditional relaxation techniques applied problem finally experimental results given demonstrate superiority learning learning large synthetic nondeterministic domain
limitations learning boolean formulae finite automata paper prove learning several classes boolean functions model also called probably approximately correct pac model learning examples results representation independent hold regardless syntactic form learner represent hypotheses methods reduce problems number wellknown learning problems prove polynomialtime learning algorithm boolean formulae deterministic finite automata threshold circuits consequences number theory particular algorithm used factor composite numbers equivalent modulo detect quadratic results hold even learning algorithm required obtain advantage prediction random techniques used demonstrate interesting learning also apply results obtain strong results approximating graph coloring research conducted author university supported bell laboratories supported grants
increasing consensus accuracy dna incorporating trace representations present new method determining consensus sequence dna new method directly incorporates trace information consensus calculations via previously described representation classifications new method evidence representation determine consensus calls using method results automatically produced consensus sequences accurate less produced standard majority voting methods additionally improvements achieved less coverage required standard methods using coverage three error rates low coverage ten sequences
learning words time towards modular connectionist account acquisition receptive learned natural language capacity recognize produce words consisting novel combinations familiar recent work acquisition takes perspective production receptive comes first paper presents connectionist model acquisition capacity recognize complex words model takes sequences segments inputs maps onto output units representing grammatical consists simple recurrent network separate modules tasks recognizing root grammatical input word experiments artificial language stimuli demonstrate model generalizes novel words rules one major types found natural languages version network modules learn assign output recognition tasks efficient manner also argue rules involving portions root network requires separate recurrent sequences larger units network learn develop representations support recognition also provide basis learning produce recognize complex words model makes many detailed predictions learning difficulty particular rules
combining foil speedup logic programs paper presents algorithm combines traditional ebl techniques recent developments inductive logic programming learn effective clause selection rules prolog programs control rules incorporated original program significant speedup may achieved algorithm shown improvement competing ebl approaches several domains additionally algorithm capable automatically intractable algorithms ones run polynomial time
model invariant object recognition visual system neurons stream visual system exhibit responses images objects invariant respect natural transformations size view evidence suggests achieved series hierarchical processing areas attempt manner representations established constructed model cortical visual processing seeks parallel many features system specifically hierarchy constrained convergent connectivity stage constructed competitive network modified learning rule called trace rule incorporates previous current neuronal activity trace rule enables neurons learn invariant short time periods representation objects objects transform real world trace rule enables neurons learn statistical invariances objects transformations together representations occur close together time show using trace rule training algorithm model indeed learn produce transformation invariant responses natural stimuli faces
recurrent neural networks missing asynchronous data paper propose recurrent neural networks feedback input units handling two types data analysis problems one hand scheme used static data input variables missing hand also used sequential data input variables missing available different frequencies unlike case probabilistic models gaussian missing variables network attempt model distribution missing variables given observed variables instead discriminant approach fills missing variables purpose minimizing learning criterion minimize output error
algebraic transformations objective functions many neural networks derived optimization dynamics suitable objective functions show networks designed repeated transformations one objective another exhibit collection algebraic transformations reduce network cost increase set objective functions transformations include products expressions functions one two expressions sparse matrix products may interpreted transformations also minimum maximum set expressions transformations introduce new force network point rather minimum transformations allow control network dynamics formalism need apply transformations number structured neural networks beginning standard reduction network connections also random matching coordinate transformations sorting simulations show transformations may applied repeatedly example networks still converge
developing casebased reasoning structural design use casebased reasoning process model design involves previously known designs memory adapting design cases fit current design context development process model particular design domain parallel development representation cases case memory design knowledge needed addition specific designs selection particular representational paradigm types information details use particular problemsolving domain depend intended use information represented project information available nature domain paper describe development implementation four casebased design systems system described terms content source case memory implementation case recall case adaptation comparison systems considers relative advantages implementations
cortical mechanisms visual recognition learning hierarchical kalman filter model describe biologically plausible model dynamic recognition learning visual cortex based statistical theory kalman filtering optimal control theory model utilizes hierarchical network whose successive levels implement kalman filters operating larger spatial temporal scales hierarchical level network predicts current visual recognition state lower level adapts recognition state using residual error prediction actual state simultaneously network also learns internal model spatiotemporal dynamics input stream adapting synaptic weights hierarchical level order minimize prediction errors kalman filter model key data connections visual cortical areas assigns specific computational roles connections known exist neurons visual cortex previous work usefulness model explaining phenomena related receptive field effects paper addition providing detailed model present variety experimental results demonstrating ability model perform robust spatiotemporal segmentation recognition objects image sequences presence varying amounts background noise
effects learning rate evolution individual lifetime learning guide evolving population areas high fitness genotype space evolutionary phenomenon known baldwin effect baldwin hinton accepted rate evolution another interaction learning evolution will effect will argued depends measure evolutionary speed one effect shows learning reduce selection pressure individuals genetic differences thus tradeoff baldwin effect effect determine influence evolution two factors contribute tradeoff cost learning landscape investigated experimentally
memory based stochastic optimization validation tuning function approximators paper focuses optimization hyperparameters function approximators describe kind algorithm continuous optimization problems less time evaluating poor parameter settings time estimates promising regions parameter space algorithm able automatically optimize parameters function approximator less computation time demonstrate algorithm problem finding good parameters memory based learner show tradeoffs involved choosing right amount computation evaluation
feature selection algorithms based analysis experiments using realworld datasets find forward feature selection algorithms accuracy function approximation using selected input features improves efficiency significantly hence propose three algorithms order enhance efficiency feature selection processing provide empirical results linear regression locally weighted regression models also propose use algorithms develop offline recognition system local models
finding overlapping distributions mml paper considers aspect mixture modelling significantly overlapping distributions require data parameters accurately estimated separated distributions example two gaussian distributions considered significantly means within three standard insufficient data available single component distribution will estimated although data two component distributions consider much data required distinguish two component distributions one distribution mixture modelling using minimum message length mml criterion first perform experiments show mml criterion performs relative bayesian criteria second make two improvements existing mml estimates improve performance overlapping distributions
performance prediction large mimd systems neural network simulations paper present performance prediction model indicating performance range mimd parallel processor systems neural network simulations model total execution time simulation function execution times small number kernel functions measured one processor one physical communication link functions depend type neural network decomposition connection structure mimd machine using model execution time speedup efficiency large mimd systems predicted model applying two popular neural networks backpropagation kohonen selforganizing feature map decomposed system measurements taken network simulations decomposed via dataset network decomposition techniques agreement model measurements within estimates given performances expected new systems presented method also used application areas image processing
prototype feature selection sampling random mutation hill climbing algorithms goal reducing computational costs without accuracy describe two algorithms find sets prototypes nearest neighbor classification term prototypes refers reference instances used nearest neighbor computation instances respect similarity order assign class new data item algorithms rely stochastic techniques search space sets prototypes simple implement first monte carlo sampling algorithm second applies random mutation hill climbing four datasets show three four prototypes give predictive accuracy equal superior basic nearest neighbor algorithm whose runtime storage costs approximately times greater briefly investigate random mutation hill climbing may applied select features prototypes simultaneously finally explain performance sampling algorithm datasets terms statistical measure extent clustering target classes
growing cell structures selforganizing network unsupervised supervised learning present new selforganizing neural network model two variants first variant performs unsupervised learning used data visualization clustering vector quantization main advantage existing approaches kohonen feature map ability model automatically find suitable network structure size achieved controlled process also includes units second variant model supervised learning method results combination selforganizing network radial basis function rbf approach model possible contrast earlier approaches rbf units supervised training weights parallel therefore current classification error used determine new rbf units leads small networks generalize results benchmark classification problem presented better results previously published submitted
exploration model building mobile robot domains present first results autonomous mobile robot operates initially unknown structured environments task explore model environment efficiently avoiding uses instancebased learning technique modeling environment realworld experiences generalized via two artificial neural networks encode characteristics robots sensors characteristics typical environments robot assumed face trained networks allow knowledge transfer across different environments robot will face lifetime models represent expected reward confidence exploration achieved low confidence regions efficient dynamic programming method employed background find paths executed robot maximize exploration operates realtime operating successfully office building environment periods
genetic algorithms analyze performance genetic algorithm call variety algorithms problem refer additive search problem closely related several previously studied problems game additive fitness functions show problem learning perceptron noisy version efficient highly noise best known approach noisy first problem genetic type algorithm known standard gas contrast perform much poorly hillclimbing approaches even though schema theorem holds generalize study whether gas will achieve implicit parallelism problem many schemata gas fail achieve implicit parallelism describe algorithm call explicitly parallel search also compute optimal point selective turns independent fitness function population distribution also analyze mean field theoretic algorithm performing similarly many problems results provide insight gas competing methods
confidence instancebased learning many extensions proposed help instancebased learning algorithms perform better wide variety realworld applications however trivial decide parameters options use applying instancebased learning algorithm particular problem traditionally crossvalidation used choose parameters nearest neighbor classifier paper points cross validation often provide enough information allow classifier confidence levels used common crossvalidation used proposes fuzzy instance based learning algorithm uses voting parameters set via combination crossvalidation confidence levels experiments datasets higher average generalization accuracy using majority voting using crossvalidation alone determine parameters
reinforcement learning domain paper describes formulation reinforcement learning enables learning noisy dynamic complex concurrent learning domain methodology involves minimizing learning space use behaviors conditions dealing credit assignment problem reinforcement form heterogeneous reinforcement functions progress estimators experimentally group four mobile robots learning task
decision tree induction effective greedy heuristic existing decision tree systems use greedy approach induce trees locally optimal splits induced every node tree although greedy approach suboptimal produce reasonably good trees current work attempt belief quantify greedy tree induction empirically using popular decision tree algorithms c45 cart induce decision trees synthetic data sets compare corresponding optimal trees turn found using novel map coloring idea measure effect greedy induction variables underlying concept complexity training set size noise dimensionality experiments show among things expected classification cost induced tree consistently close optimal tree
facts input state stabilization facts input state stabilization ieee report abstract previous results input state shown hold even systems linear controls provided general type feedback allowed applications certain stabilization problems comparisons results input state stability also briefly discussed
attractor networks attractor networks map continuous input space discrete output space useful pattern noisy missing features input however designing net given set attractors training procedures often produce attractors attractor difficulties occur connection network encoding multiple attractors describe alternative formulation attractor networks encoding knowledge local distributed although attractor nets similar dynamics distributed much easier work propose statistical formulation attractor net dynamics yields convergence proof mathematical interpretation model parameters present simulation experiments explore behavior attractor nets showing produce presence attractor attractor attractors occur points symmetry state space
free lunch generalisation first principles according theorems generalisation absence domain knowledge necessarily good generalisation performance one situation always performance another notes theorems demonstrate effective generalisation logical learners bias assumption set key importance
networks grow learn learning limited modification parameters limited scope capability modify system structure also needed get wider range learnable case artificial neural networks learning iterative synaptic weights network appropriate network structure number hidden layers units size shape receptive fields paper view network structure usually done determined computed learning algorithm incremental learning algorithms modify network structure addition andor units andor links survey current connectionist literature given line thought grow learn new algorithm learns association due incremental using local representation socalled phase units previously stored longer necessary due recent modifications removed minimize network complexity incrementally constructed network later offline improve performance another method proposed greatly increases recognition accuracy train number networks responses algorithm variants tested recognition handwritten seem promising especially terms learning speed makes algorithm attractive online learning tasks robotics biological plausibility incremental learning also discussed briefly earlier part work realized supported national later part realized supported international computer science institute number people discussions questions peter
estimating dependency structure hidden variable paper introduces probability model mixture trees account sparse dynamically changing dependence relationships present family efficient algorithms use minimum tree algorithm find map mixture trees variety priors including dirichlet mdl priors
learning predict reading dna sequences two fundamental problems analyzing dna sequences regions dna sequence encode proteins determining reading frame region investigate using artificial neural networks anns find coding regions determine reading detect errors dna sequences describe adaptation approach used identify coding regions human dna compare performance anns several conventional methods predicting reading experiments demonstrate anns outperform conventional approaches
adaptive state space reinforcement learning navigation paper describes control system mobile robot based sensor information control system provide signal way avoided since case examples available system learns basis external reinforcement signal negative case zero otherwise rules temporal difference learning used find correct mapping discrete sensor input space signal describe algorithm learning correct mapping input state vector output signal algorithm used discrete coding input state space
transfer inductive work currently learning methods better able transfer knowledge one task another process knowledge transfer usually viewed separate inductive procedures ordinary learning however paper argues view leads number conceptual difficulties offers task analysis transfer process inside inductive argues transfer viewed within induction independent procedure knowledge learning trials
experiments transfer knowledge neural networks computational learning theory chapter describes three studies address question neural network learning improved via information extracted networks general problem call network transfer many types relationships source target networks focus weights source networks solve target network task goal learning target task demonstrate approach described improve learning speed ten times learning starting random weights
automated system autonomous vehicle neural net backpropagation trained neural network capable vehicle road environments although fairly robust one problems time takes train vehicle capable online learning car network capable autonomous operation one reason use report describe original system look three alternative training methods correlation run series trials using correlation compare baseline finally hidden unit analysis performed determine network learning applying advanced learning algorithms
vector associative maps unsupervised realtime learning control movement trajectories
evidence approximation predictive modeling data mining work discussed paper motivated need building decision support systems realworld problem domains goal use systems tool supporting bayes optimal decision making action maximizing expected utility respect predicted probabilities possible outcomes selected reason models used need probabilistic nature output model probability distribution just set numbers model family chosen set simple discrete finite mixture models advantage computationally efficient work describe bayesian approach constructing finite mixture models sample data approach based unsupervised learning process used exploratory analysis model construction first phase selection model class number parameters performed calculating approximation model class evidence second phase map parameters selected class estimated algorithm framework overfitting problem common many traditional learning approaches avoided learning process automatically complexity model paper focuses model class selection phase approach presenting empirical results natural synthetic data
technical report
large margin classification using perceptron algorithm introduce analyze new algorithm linear classification combines perceptron algorithm leaveoneout method like classifier algorithm takes advantage data linearly separable large compared algorithm however much simpler implement much efficient terms computation time also show algorithm efficiently used high dimensional spaces using kernel functions performed experiments using algorithm variants classifying images handwritten digits performance algorithm close good performance classifiers problem
parallelism parallelism via simultaneous multithreading version paper will appear transactions computer systems august permission make digital copies part work personal use provided copies made distributed commercial advantage copies bear full first components work others must credit otherwise lists requires prior specific permission andor abstract achieve high performance computer systems rely two forms parallelism parallelism ilp parallelism superscalar processors exploit ilp executing multiple instructions single program single cycle exploit executing different threads parallel different processors unfortunately styles partition processor resources thus adapting levels ilp program insufficient processors will insufficient ilp hardware superscalar paper explores parallel processing alternative architecture simultaneous multithreading allows multiple threads share processors resources every cycle reason running parallel applications processor ability use parallelism parallelism multiple threads share processors functional units simultaneously processor use ilp accommodate variations parallelism program single processors resources exists parallelism lack
models paper present framework building probabilistic automata parameterized probabilities gibbs distributions used model state transitions output generation parameter estimation carried using algorithm uses generalized iterative scaling procedure discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology
convergence new operations new method memory utilizing report abstract
improving using boosting techniques regression context boosting bagging techniques build committee may superior single use regression trees fundamental building blocks bagging committee machines boosting committee machines performance analyzed three nonlinear functions database cases boosting least equivalent cases better bagging terms prediction error
hybrid expert system current expert systems properly handle incomplete information hand neural networks perform pattern recognition operations even noisy environments background implemented neural expert system whose computational mechanism processes given information means approximate probabilistic reasoning
tracking measurements adaptive progress coevolutionary simulations coevolution give rise effect interacting populations others fitness landscapes effect significantly measurement coevolutionary progress introducing fitness improvements performance individuals appear usual measures evolutionary progress unfortunately appropriate measures fitness given effect developed artificial life theoretical biology population dynamics evolutionary genetics propose set appropriate performance measures based genetic behavioral data illustrate use simulation coevolution specified noisy recurrent neural networks generate pursuit behaviors autonomous agents
flexible parametric measurement error models inferences measurement error models sensitive modeling assumptions specifically model incorrect estimates reduce sensitivity modeling assumptions yet still efficiency parametric inference propose use flexible parametric models accommodate standard parametric models use mixtures purpose study two cases detail linear model model professor statistics department statistics university college associate professor professor department statistics university research supported grant national cancer institute research supported nsf grant research supported grant nsf grants
computer multiparent reproduction genetic algorithms paper investigate phenomenon multiparent reproduction study recombination mechanisms arbitrary number parents creating particular discuss crossover generalizes standard uniform crossover crossover generalizes point crossover study effects different number parents behavior conduct experiments function optimization problems observe multiparent operators performance gas enhanced significantly also give theoretical foundation showing operators work distributions
selection hierarchical models bayes factor approach technical report department statistics university washington seattle washington health policy institute health policy studies box university california san san adrian raftery professor statistics department statistics university washington seattle research supported national research service award national cancer institute authors grateful helpful discussions
draft brief introduction neural networks williams college university abstract artificial neural networks used increasing frequency high dimensional problems regression classification article provides tutorial overview neural networks focusing back propagation networks method approximating nonlinear functions explain point neural networks might attractive compare modern regression techniques keywords nonparametric regression function approximation backpropagation introduction networks way brain works computer programs actually learn patterns forecasting without know statistics just many artificial neural networks neural networks will term artificial need distinguish biological neural networks seem least able statistics without except software neural networks successfully used many different applications including robotics chemical process control speech recognition optical character recognition credit detection interpretation chemical vision autonomous navigation vehicles literature given end article article will attempt explain one particular type neural network feedforward networks sigmoidal activation functions backpropagation networks actually works trained compares known statistical techniques example use neural network consider problem recognizing hand written codes classification problem
information filtering selection mechanisms learning systems machine learning generalization search artificial
gaussian regression optimal finite dimensional linear models
estimation probability density function mode annual mathematical statistics apply algorithm classification assign class separate set gaussians set trained patterns single class trained gaussians set provides estimate probability function one class just window estimation take estimate pattern distribution average gaussians set classification pattern may now done calculating probability class sample point pattern class highest probability hence whole plays role classification patterns case regular classification schemes using tested classification scheme several classification tasks including two problem compared algorithm various classification algorithms second best algorithm applications window estimation however computing time memory window estimation compared algorithm hence practical situations algorithm developed fast algorithm combines attractive properties window estimation vector quantization scale parameter adaptively therefore set manner allows classification strategy vectors taken account yields better results standard vector quantization techniques interesting topic research use gaussians
predicting dynamically memory predictions dynamically objects used improve time space efficiency dynamic memory management computer programs used simple lifetime predictor demonstrated improvement variety computer programs paper use decision trees lifetime prediction programs show significantly better prediction method also advantage training use large number features let decision tree automatically choose relevant subset
learning representations evolutionary computation example domain twodimensional shape designs evolutionary systems used variety applications design scheduling problems basic algorithms similar applications representation always problem specific unfortunately search time evolutionary systems much depends efficient using problem specific domain knowledge reduce size search space paper describes approach user specifies general basic coding used larger variety problems system learns efficient problem specific coding evolutionary system variable length coding used system optimizes example problem process identifies successful combinations genes population combines higher level evolved genes extraction repeated iteratively allowing genes evolve high level complexity encode high number original basic genes results continuous search space allowing potentially successful solutions found much search time evolved coding used solve related problems potentially desirable solutions evolved coding makes knowledge example problem available new problem
study learning algorithms coverage learning algorithm number concepts learned algorithm samples given size paper whether good learning algorithms designed maximizing coverage paper extends previous upper bound coverage boolean concept learning algorithm describes two coverage approaches upper bound experimental measurement coverage id3 algorithms shows coverage far bound analysis shows although learns many concepts seem interesting concepts hence coverage maximization alone appear yield learning algorithms paper concludes definition coverage within bias suggests way coverage maximization applied weak preference biases
exploiting structure policy construction markov decision processes mdps recently applied problem modeling decisiontheoretic planning traditional methods solving mdps often practical small states spaces effectiveness large planning problems present algorithm called structured policy iteration constructs optimal policies without explicit state space algorithm fundamental computational steps commonly used modified policy iteration algorithm exploits variable propositional temporal bayesian network representation mdps principles behind applied structured representation stochastic actions policies value functions algorithm used conjunction approximation methods
asocs connectionist network guaranteed learning arbitrary mappings paper reviews features new class multilayer connectionist architectures known asocs adaptive selforganizing concurrent systems asocs similar decisionmaking neural network models attempts learn adaptive set arbitrary vector mappings however differs dramatically mechanisms asocs based networks adaptive digital elements using local information function specification incrementally use rules rather complete inputoutput vectors processing network able extract critical features large environment give output parallel fashion learning also uses parallelism selforganization new rule completely learned time linear depth network model guarantees learning arbitrary mapping boolean inputoutput vectors model also stable learning previously learned mappings except explicitly
maximum working likelihood inference markov chain monte carlo maximum working likelihood inference presence missing data quite challenging associated marginal likelihood problem number parameters involved large propose using markov chain monte carlo mcmc first obtain estimator working fisher information matrix second using monte carlo obtain remaining components correct asymptotic variance evaluation marginal likelihood needed demonstrate consistency asymptotic number independent distributed data clusters large likelihood may specified analysis ordinal data given example key words convergence posterior distributions maximum likelihood metropolis
natural image statistics efficient coding natural images contain characteristic statistical regularities set purely random images understanding regularities enable natural images coded efficiently paper describe forms structure contained natural images show related response properties neurons early stages visual system many important forms structure require higherorder linear pairwise statistics characterize makes models based linear hebbian learning principal components analysis inappropriate finding efficient codes natural images suggest good objective efficient coding natural scenes maximize representation show network learns sparse codes natural scenes developing localized oriented receptive fields similar cortex
using problem explore effects paper develop empirical methodology studying behavior evolutionary algorithms based problem describe three used study effects performance finally illustrate use ideas preliminary exploration effects simple gas
parameterized uniform crossover traditionally genetic algorithms upon point crossover operators many recent empirical studies however shown benefits higher numbers crossover points recent work focused uniform crossover involves average crossover points strings length theoretical results suggest view hyperplane sampling uniform crossover features however growing body experimental evidence suggests otherwise paper attempt views uniform crossover present framework understanding
complexity conditional conditional introduced utilized artificial intelligence capture broad range phenomena paper examine complexity several variants discussed literature show general satisfiability formulas arbitrary conditional npcomplete formulas bounded however provide several exceptions rule particular note results showing assuming worlds agree worlds possible decision problem becomes even formulas bounded assuming worlds agree conditional decision problem npcomplete arbitrary
learning sequential tasks incrementally adding higher orders incremental higherorder network combines two properties found useful learning sequential tasks higherorder connections incremental introduction new units network adds higher orders needed adding new units dynamically modify connection weights since new units modify weights next information previous step temporal tasks learned without use feedback thereby greatly simplifying training furthermore theoretically number units added reach arbitrarily past experiments grammar demonstrated speedups two orders magnitude recurrent networks
learning factorial codes minimization neural computation propose novel general principle unsupervised learning distributed internal representations input patterns principle based two forces representational unit adaptive predictor tries predict unit remaining units turn unit tries environment minimizes unit filter abstract concepts environmental input concepts statistically independent upon units focus discuss various simple yet potentially powerful implementations principle aim finding binary factorial codes codes probability particular input simply product probabilities corresponding code symbols codes potentially relevant segmentation tasks supervised learning detection methods finding factorial codes automatically implement occams razor finding codes using minimal number units unlike previous methods novel principle potential removing linear also nonlinear output experiments show algorithms based principle minimization feasible final part paper describes entirely local algorithm potential learning unique representations extended input sequences
statistical queries pac paper study learning pac model example oracle used learning may one two ways either example distribution examples first consider models examples recently showed efficient learning new model using statistical queries sufficient condition pac learning classification noise show efficient learning statistical queries sufficient learning pac model malicious error rate proportional required statistical query accuracy one application result new lower bound malicious error learning literals first bound independent number irrelevant attributes also use statistical query model give sufficient conditions using distribution specific algorithms distributions domains result class distributions weakly learn monotone boolean formulae also consider new models learning examples chosen according distribution learner will tested examine three variations distribution noise give necessary sufficient conditions polynomial time learning noise show various models finally examine hypothesis boosting algorithms context learning distribution noise show result regarding strength weak learnability sense tight requiring weak learner nearly distribution free
evolutionary learning abilities case study optimizing parameter values qlearning paper describes first stage study evolution learning abilities use simple exploration problem designed sutton task individual encode inherent learning parameters learning architecture use one step qlearning using table inherent parameters initial learning rate rate rewards exploration rate fitness measure number times achieves goal later life learners evolve genetic algorithm results computer simulation learning ability emerge environment changes every generation inherent map optimal path acquired environment change results suggest emergence learning ability needs environmental change faster alternate generation
efficient updates partially observable markov decision processes examine problem performing exact updates partially observable markov decision processes pomdps computational complexity viewpoint updates crucial operation wide range solution methods find intractable perform updates convex value functions general pomdps offer new algorithm called algorithm compute updated value functions efficiently restricted class pomdps number linear great compare algorithm existing algorithms analytically empirically find algorithm wide range sizes
limits instruction level parallelism applications paper examines limits instruction level parallelism found programs particular benchmark suite using recent version benchmark suite differs earlier studies removing true dependencies occur result compiler employing stack limitation parallelism appears true dependency stack methods used employ stack remove dependency paper show far parallelism seen previously refer type parallelism parallelism distance requires large instruction detection conclude two observations single instruction window characteristic superscalar machines inadequate detecting parallelism distance order take advantage parallelism compiler must involved separate threads must explicitly
models paper present framework building probabilistic automata parameterized probabilities gibbs distributions used model state transitions output generation parameter estimation carried using algorithm uses generalized iterative scaling procedure discuss relations certain classes stochastic feedforward neural networks geometric interpretation parameter estimation simple example statistical language model constructed using methodology
role constraints hebbian learning models unsupervised hebbian synaptic plasticity typically either synapses grow maximum allowed strength synapses decay zero strength common method avoiding outcomes use constraint limits total synaptic strength cell study dynamical effects constraints two methods constraint distinguished multiplicative otherwise linear learning rules multiplicative constraint results dynamics converge principal operator determining synaptic development contrast typically leads final state almost synaptic strengths reach either maximum minimum allowed value final state often weight configurations principal operator multiplicative yields receptive field correlated inputs represented whereas yields receptive field subset inputs two equivalent input populations two eyes common target multiplicative ocular dominance two populations weakly correlated whereas allows circumstances results may used understand constraints output cells input cells variety rules implement constrained dynamics discussed
convergence stochastic iterative dynamic programming algorithms recent developments area reinforcement learning number new algorithms prediction control markovian environments algorithms including algorithm sutton qlearning algorithm motivated approximations dynamic programming paper provide rigorous proof convergence learning algorithms relating powerful techniques stochastic approximation theory via new convergence theorem theorem general class convergent algorithms qlearning belong report describes research done brain cognitive sciences center biological computational learning artificial intelligence laboratory massachusetts institute technology support provided part grant nsf asc9217041 support laboratorys artificial intelligence research provided part advanced research projects agency defense authors supported grant foundation grant atr human information processing research laboratories grant siemens corporation grant national science foundation grant office naval research nsf grant support intelligent control mit michael jordan nsf young
empirical comparison gradient descent gradient descent supervised reinforcement learning technical report
objective functions active data selection learning made efficient select particularly data points within bayesian learning framework objective functions discussed measure expected candidate measurements three alternative specifications gain information lead three different criteria data selection criteria depend assumption hypothesis space correct may prove main
incremental grid growing encoding highdimensional structure twodimensional feature map knowledge clusters relations important understanding highdimensional input data unknown distribution ordinary feature maps fully connected fixed grid topology properly reflect structure clusters input cluster boundaries map incremental feature map algorithms nodes connections added map according input distribution overcome problem however far algorithms limited maps drawn case dimensional input space approach proposed paper nodes added incrementally regular dimensional grid times dimensionality input space process results map explicitly represents cluster structure highdimensional input
graphical characterization lattice conditional independence models lattice conditional independence models multivariate normal data recently introduced analysis missing data patterns dependent linear regression models shown class models subclass class graphical markov models determined acyclic namely subclass models explicit graph theoretic characterization markov equivalent obtained characterization allows one determine whether specific markov equivalent hence model polynomial time without exhaustive search exponentially large equivalence class results require existence joint densities
using casebased learning improve design optimization paper describe method improving optimization using casebased learning idea utilize sequence points explored search guide exploration proposed method particularly suitable continuous spaces expensive evaluation functions arise engineering design empirical results two engineering design domains across different representations demonstrate proposed method significantly improve efficiency reliability moreover results suggest modification makes genetic algorithm less sensitive poor choices tuning parameters rate
genetic algorithm continuous design space search genetic algorithms gas extensively used means performing global optimization simple yet reliable manner however realistic engineering design optimization domains simple classical implementation based binary encoding bit mutation crossover often inefficient reach global optimum paper describe continuous optimization uses new operators strategies structure properties engineering design domains empirical results domains aircraft demonstrate formulated significantly better classical efficiency reliability
references using neural networks identify kohonen self organized formation correct feature hinton williams learning internal representations error propagation eds parallel distributed processing cognition vol mit press
new look tree models multiple sequence alignment evolutionary trees frequently used underlying model design algorithms optimization criteria software multiple sequence alignment paper trees universal model light broad range biological questions used address tree model consists tree topology model accepted along branches major applications examples molecular biology literature used illustrate situations tree model fails occurs relationship described tree example structural functional applications also occurs situations lateral gene transfer entire gene modeled unique tree cases data convergent evolution may difficult find consistent model survey will dialogue computer leading biologically realistic research
transmission may allow combined associative memory function selforganization selective transmission feedback synapses learning proposed mechanism combining associative feedback selforganization feedforward synapses experimental data demonstrates synaptic transmission layer feedback synapses lack layer feedforward synapses network feature uses local rules learn mappings linearly separable learning sensory stimuli desired response simultaneously presented input feedforward connections form representations input feedback connections learn feedforward connectivity recall removed sensory input representation activity generates learned response
markov chain monte carlo methods based density function technical report department statistics university toronto abstract one way sample distribution sample uniformly region density function markov chain converges uniform distribution constructed uniform sampling direction uniform sampling slice defined current position variations slice sampling methods easily implemented univariate distributions used sample multivariate distribution updating variable turn approach often easier implement gibbs sampling may efficient versions metropolis algorithm slice sampling therefore attractive markov chain monte carlo applications use software automatically generates markov chain sampler model specification one also easily versions slice sampling sometimes greatly improve sampling efficiency random walk behaviour random also avoided slice sampling schemes simultaneously update variables
complexity solving markov decision problems markov decision problems mdps provide foundations number problems interest researchers studying automated planning reinforcement learning paper summarize results regarding complexity solving mdps running time mdp solution algorithms argue although mdps solved efficiently theory study needed practical algorithms solving large problems quickly future research alternative methods analysis rely mdps
machine learning creating reinforcement learners learning promising approach creating intelligent agents however reinforcement learning usually requires large number training episodes present evaluate design addresses allowing connectionist advice given time natural manner external approach learner makes suggestions expressed instructions simple programming language based techniques knowledgebased neural networks programs directly agents utility function subsequent reinforcement learning integrates refines advice present empirical evidence investigates several aspects approach show given good advice learner achieve statistically significant gains expected reward second experiment shows advice improves expected reward regardless stage training given another study demonstrates subsequent advice result gains reward finally present experimental results indicate method powerful naive technique making use advice
dirichlet mixtures method improving detection weak significant protein sequence paper presents mathematical foundations dirichlet mixtures used improve database search results sequences variable number sequences protein family domain known present method information protein database mixture dirichlet densities mixtures designed combined observed frequencies form estimates expected probabilities position profile hidden markov model statistical model estimates give statistical model greater generalization capacity related family members reliably model dirichlet mixtures shown outperform matrices methods computing expected distributions database search resulting fewer false false families tested paper previously published formula estimating expected probabilities contains complete derivations dirichlet mixture formulas methods optimizing mixtures match particular databases suggestions efficient implementation
analysis empirical studies analogy analogy technique problem solving experience improve problem solving performance research addresses issue common problem use analogy past experiences new problems reuse first research describes variety arise proposes new approach analogy uses appropriate adaptation strategies second compares approach others common domain empirical study shows analogy almost always efficient problem solving scratch amount depends ability overcome
analysis dynamical demonstrated recurrent neural networks act dynamical formal languages trained positive negative examples observed phase transitions learning state sets work focused mainly extraction minimization finite state automaton trained network however networks capable inducing languages regular therefore equivalent indeed may simpler small network fit training data inducing language networks language regular paper using low dimensional network capable learning data sets present empirical method testing whether language induced network regular also provide detailed machine analysis trained networks regular languages
linear machine decision trees technical report january abstract article presents algorithm inducing multiclass decision trees multivariate tests internal decision nodes test constructed training linear machine eliminating variables controlled manner empirical results demonstrate algorithm builds small accurate trees across variety tasks
simulator evolving universal simulator cover infinite computer evolution objects conference artificial life eds mit press knowledge program result familiar structures provided algorithm model physical purely fitness function thus measures feasibility way evolutionary process runs environment constrained added however requirement complex structures long simulations evaluate results encouraging evolved structures surprisingly look based common knowledge build instead computer found ways evolutionary search process able final designs confirm objectives introduced fitness functions background related problems describe physical simulation model twodimensional structures representation encoding applying evolution demonstrate feasibility work actual objects result particular finally discuss future work conclusions order evolve behavior autonomous devices one must simulator operates several constraints controller adaptive enough cover gap simulated real world space mechanisms simulation never perfect preserve margin efficient test simulation physical production test results real object computer evolution objects abstract idea coevolution popular little work done evolution physical structure lack general framework evolution simulation constrained gap implies objects usually work present takes step problem body evolution applying evolutionary techniques design structures parts evolution takes place simulator designed computes forces predicts failure dimensional structures final program assembly built demonstrate several different evolved
knowledge acquisition via knowledge integration paper concerned problem acquiring knowledge integration aim construct integrated knowledge base several separate sources need knowledge bases arise example knowledge bases acquired independently interactions several domain experts different domain experts may differ knowledge bases constructed way will normally differ similar problem also arise separate knowledge bases generated learning algorithms objective integration construct one system exploits knowledge available good performance aim paper discuss methodology knowledge integration describe implemented system present concrete results demonstrate advantages method
artificial evolution visual control systems robots many particularly exhibit sophisticated guided yet cases guided input eye inspired observation several years exploring possibilities guided robots vision rather design robot controllers hand use artificial evolution form extended genetic algorithm automatically generate architectures artificial neural networks generate effective coordination controlling mobile robots analytic techniques drawn dynamical systems theory allow understand evolved robot controllers function predict behaviour environments used evolutionary process initial experiments performed simulation techniques now successfully work variety real physical robot chapter reviews past work analysis evolved controllers gives overview current research conclude discussion application evolutionary techniques problems biological vision
compression algorithm probability transition matrices paper describes compression algorithm probability transition matrices matrix probability transition matrix general compression error appears small even high levels compression
bayesian statistics bayesian inference latent structure time series range
instancebased learning methods explicitly data receive usually training phase prediction time perform computation take query search database similar build online local model local average local regression predict output value paper review advantages instance based methods autonomous systems also note cost slow computation database grows large present evaluate new way database new algorithm maintains advantages instancebased learning earlier attempts combat cost instancebased learning explicit data applicable instancebased predictions based small number near neighbors explicit training phase form data structure approach builds multiresolution data structure summarize database experiences resolutions interest simultaneously permits query database conventional linear search greatly reduced computational cost
nearly learning standard online model learning algorithm tries minimize total number mistakes made series trials trial learner instance either instance appropriate response define natural variant model learner feedback instance accepted use two transformations relate model enhanced standard model false separately false present strategy trading false false standard model one perspective strategy exactly optimal including constants apply results obtain good general purpose algorithm nearly optimal algorithms variety standard classes conjunctions boolean variables also present analyze simpler transformation useful instances drawn random rather selected adversary
using errors create learnable paper describe algorithm exploits error distribution generated learning algorithm order domain approximated learnable traditionally error distribution error measure however important information error distribution algorithm exists ridge errors also partition space one part space will learning another algorithm builds variable tree whose leaves contain using tree new points predicted using correct partition tree algorithm using memory based learners crossvalidation
parallel research execution environment neural systems parallel research execution environment neural systems distributed networks systems current applications neural networks often contain large amounts data neural networks involved tasks vision large high requirements memory computational resources target execution executed distributed environment tools neural network simulation programs running machine via using approach larger tasks data examined using efficient coarse parallelism furthermore design allows neural networks running high performance mimd machine system paper different features design concepts discussed also used applications like image processing
genetic operators known standard learning classifier systems applied many different domains exhibit number problems payoff difficult reward system background genetic algorithm rule chains default hierarchies parallel version standard learning classifier system problems paper propose solutions problems introduce following original features new genetic operator used potentially useful classifiers energy introduced measure global convergence order apply genetic algorithm system close steady state dynamical classifiers set order speed performance phase algorithm present simulation results experiments run simulated twodimensional world simple agent learns follow light source
classifier system plays simple board game machine learning
keeping neural networks simple minimizing description length weights supervised neural networks generalize much less information weights output vectors training cases learning important keep weights simple amount information contain amount information weight controlled adding gaussian noise noise level adapted learning optimize tradeoff expected squared error network amount information weights describe method computing derivatives expected squared error amount information noisy weights network contains layer nonlinear hidden units provided output units linear exact derivatives computed efficiently without monte carlo simulations idea minimizing amount information required weights neural network leads number interesting schemes encoding weights
learning order things many applications desirable order rather classify instances consider problem learning order given feedback form preference effect one instance another outline approach one first learns conventional means preference function form indicates whether new instances ordered maximize learned preference function show problem finding ordering best preference function npcomplete even assumptions nevertheless describe simple greedy algorithm guaranteed find good approximation discuss online learning algorithm based algorithm finding good linear combination ranking experts use ordering algorithm combined online learning algorithm find combination search experts query expansion strategy search engine present experimental results demonstrate approach
dynamic conditional independence models markov chain monte carlo methods
relations search evolutionary algorithms technical report march abstract evolutionary algorithms powerful techniques whose operation principles inspired natural selection genetics paper discuss relation evolutionary techniques numerical classical search methods show methods instances single general search strategy call evolutionary computation combining features classical evolutionary methods different ways new instances general strategy generated new evolutionary classical algorithms designed one algorithm described
connectionist symbol manipulator discovers structure contextfree languages present neural net architecture discover hierarchical recursive structure symbol strings detect structure multiple levels architecture capability reducing symbols single symbols makes use external stack memory terms formal languages architecture learn parse strings contextfree grammar given training sets positive negative architecture trained recognize many different grammars architecture one layer weights allowing many cognitive domains involve complex sequences contain hierarchical recursive structure music natural language event perception illustrate containing embedded understanding structures requires reduced descriptions hinton string symbols states reduced single symbolic present neural net architecture learns encode structure symbol strings via reduction transformations difficult problem extracting structure complex extended sequences studied among others previous made straightforward interpretation behavior
selforganizing process based lateral synaptic resource selforganizing feature maps usually implemented neural parallel distributed processes external finds unit whose weight vector distance input vector determines neighborhood weight adaptation weights changed proportional distance biologically plausible implementation similarity measured product neighborhood selected lateral weights changed synaptic resources resulting selforganizing process quite similar abstract case however process somewhat boundary effects parameters need carefully evolved also necessary add redundant dimension input vectors
graphical models applied mathematical multivariate
reinforcement learning heterogeneous multiagent systems application decision making learning algorithms multiagent systems presents many challenges opportunities among ability agents learn act observing agents describe algorithm integrates qlearning roughly uses observations made expert agent bias exploration promising directions algorithm beyond previous work direction assumptions learner expert observed agent share objectives abilities preliminary experiments demonstrate significant transfer agents using many cases training time
face recognition hybrid neural network approach faces represent complex multidimensional visual stimuli developing computational model face recognition difficult present hybrid neural network solution compares favorably methods system combines local image sampling selforganizing map neural network neural network selforganizing map provides quantization image samples space inputs original space also output space thereby providing dimensionality reduction changes image sample neural network provides partial scale network larger features hierarchical set layers present results using transform place selforganizing map multilayer perceptron place network transform performs almost error versus multilayer perceptron performs poorly error versus method capable rapid classification requires fast approximate normalization preprocessing consistently exhibits better classification performance approach database considered number images per training database images per proposed method result error respectively provides measure confidence output classification error approaches zero examples use database images individuals contains quite high degree variability expression pose facial details analyze computational complexity discuss new classes added trained
asynchronous modified policy iteration updates present new algorithm solving markov decision problems extends modified policy iteration algorithm two important ways new algorithm asynchronous allows values states updated arbitrary order need consider actions state updating policy new algorithm converges general initial conditions required modified policy iteration specifically set initial function pairs algorithm guarantees convergence set modified policy iteration converges generalization obtained making simple easily change policy evaluation operator used updating value function asynchronous nature algorithm convergence general conditions range problems algorithm applied
causation action
markov chain monte carlo sampling evaluating multidimensional application bayesian computation recently markov chain monte carlo mcmc sampling methods become widely used determining properties posterior distribution alternative gibbs sampler sampler generalization sampling scheme generate markov chain posterior distribution proof convergence applications bayesian computation constrained parameter spaces provided comparisons mcmc samplers made addition propose importance weighted marginal density estimation method obtained averaging many dependent observations ratio full joint posterior densities weighting conditional density asymptotic properties guidelines choosing weighting conditional density also considered generalized version estimating marginal posterior densities full joint posterior density contains analytically intractable constants developed furthermore develop monte carlo methods based comparing marginal posterior density estimators article summary authors thesis presented award session
sample complexity noisetolerant learning paper characterize complexity noisetolerant learning pac model specifically show general lower bound number examples required pac learning presence classification noise combined result effectively show sample complexity pac learning presence classification noise furthermore demonstrate optimality general lower bound providing noisetolerant learning algorithm class symmetric boolean functions uses sample size within constant factor bound finally note general lower bound compares favorably various general upper bounds pac learning presence classification noise
monte carlo comparison unsupervised classifiers
evolution evolution recently realized caused adaptive selection particular level happen either level tradeoffs result competition paper describes simulations study effect genes changes mutation rate development investigates interaction simplified model
evolving visual routines architecture planning
qualitative reasoning physical systems deriving monotonic function observations much work qualitative physics involves constructing models physical systems using functional descriptions flow increases pressure methods improve model precision adding numerical monotonic functions methods normally used determine paper describes systematic method computing bounding multivariate monotonic function given stream data derived computed determining simultaneous confidence special neural network guaranteed produce monotonic functions complex systems simulated using methods
memorybased learning paper describe application memorybased learning problem compare memorybased learning examples memory generalizes using intelligent similarity metrics number recently proposed statistical methods suited large numbers features evaluate methods common benchmark dataset show method compares favorably previous methods incorporating various representations word patterns value difference metrics space
studies transmission analysis using hierarchical bayesian mixture models structured mixture models studied context data analysis inference neural synaptic transmission characteristics central systems mixture structures arise due uncertainties stochastic mechanisms responses individual sites models attempt capture scientific features sensitivity individual synaptic transmission sites stimuli extent responses done via structured classes prior distributions parameters describing features priors may structured permit assessment currently scientific hypotheses fundamental neural function posterior analysis implemented via stochastic simulation several data analyses described illustrate approach resulting insights recently generated experimental contexts developments open questions statistical research partially supported nsf grants work represents part project university medical center data provided university slightly revised version paper published journal statistical association vol modified title hierarchical mixture models transmission analysis author bayesian analysis concrete problem based work reported paper
rapid development modules memorybased learning need software modules performing natural language processing tasks growing modules perform efficiently accurately time rapid development often recent work machine learning techniques general memorybased learning particular offer tools meet present examples modules trained three tasks iii demonstrate three modules display high generalization accuracy argue applicable similarly large class tasks
learning boolean readonce formulas generalized bases readonce formula one variable appears single input give polynomial time algorithm uses membership equivalence queries identify exactly readonce boolean formulas basis goal work consider natural generalizations order develop exact identification algorithms powerful classes formulas show readonce formulas basis arbitrary boolean functions constant less exactly polynomial time using membership equivalence queries show readonce formulas basis arbitrary symmetric boolean functions also exactly polynomial time model given standard assumptions polynomial time identification algorithm formulas either bases using membership equivalence queries show basis class certain technical conditions polynomial time identification algorithm readonce formulas extended polynomial time identification algorithm readonce formulas arbitrary functions less result readonce formulas arbitrary symmetric arbitrary constant also exactly polynomial time using membership equivalence queries
hidden markov decision trees study time series model viewed decision tree markov temporal structure model intractable exact calculations thus utilize variational approximations consider three different distributions approximation one markov calculations performed exactly layers decision tree one decision tree calculations performed exactly time steps markov chain one assumption made single likely state sequence present simulation results artificial data accepted presentation
stochastic simulation algorithms dynamic probabilistic networks stochastic simulation algorithms likelihood weighting often give fast accurate approximations posterior probabilities probabilistic networks methods choice large networks unfortunately special characteristics dynamic probabilistic networks used represent stochastic temporal processes mean standard simulation algorithms perform poorly simulation trials process observed time paper present simulation algorithms use evidence observed time step set trials back towards first algorithm evidence reversal time slice evidence nodes slice become state variables second algorithm called survival sampling set trials time step using stochastic reproduction rate weighted likelihood evidence according trial compare performance algorithm likelihood weighting original network also investigate benefits combining methods combination appears maintain bounded error independent number time steps simulation
stochastic random probabilistic direction example people simulated annealing search technique single trial solution modified random energy defined represents good solution goal find best solution energy changes lead lower energy always accepted increase accepted probability given change energy constant temperature initially temperature high corresponding state large changes possible reduced using schedule allowing smaller changes system low energy solution
learning small disjuncts systems learn examples often create disjunctive concept definition disjuncts concept definition cover training examples small disjuncts problem small disjuncts error large disjuncts may necessary achieve high level predictive accuracy paper extends previous work done problem small disjuncts investigating reasons small disjuncts error large disjuncts evaluating impact small disjuncts inductive learning paper shows attribute noise missing attributes class noise training set size cause small disjuncts error large disjuncts paper also evaluates impact factors learning small disjuncts error rate shows two artificial domains low levels attribute noise applied training set ability learn correct noisefree concept evaluated small disjuncts primarily responsible making learning difficult
questions minimize errors number efficient learning algorithms achieve exact identification unknown function class using membership equivalence queries using standard transformation algorithms easily online learning algorithms use membership queries transformation number equivalence queries made query algorithm directly corresponds number mistakes made online algorithm paper consider several natural classes known learnable setting investigate minimum number equivalence queries minimum number mistakes online model made learning algorithm makes polynomial number membership queries uses polynomial computation time able reduce number equivalence queries used previous algorithms often prove matching lower bounds example consider class dnf formulas variables olog terms previously algorithm provided best known upper bound log minimum number equivalence queries needed exact identification greatly improve upper bound showing exactly needed learner priori exactly needed learner know priori exactly matches known lower bounds many results obtain complete characterization tradeoff number membership equivalence queries needed exact identification classes consider monotone dnf formulas horn sentences olog dnf formulas dnf formulas readonce formulas various bases deterministic finite automata
learning rectangles queries
survey evolution strategies
learning rules local exceptions present learning algorithm rulebased concept representations called rule sets rule sets allow deal exceptions rule separately introducing exception rules exception rules exception rule etc constant depth local exception rules contrast decision lists exception rules must global ordering rules localization exceptions makes possible represent concepts decision list representation hand decision lists constant number rules different classes represented constant depth rule sets polynomial increase size algorithm algorithm constant depth rule sets hence pac learning algorithm based repeatedly applying greedy approximation method weighted set cover problem find good exception rule sets
learning hierarchical rule sets present algorithm learning sets rules organized levels level contain arbitrary number rules class associated level concept given class basic concepts rules higher levels rules lower levels used represent exceptions basic concepts use boolean attributes infinite attribute space model certain concepts defined terms given sample examples algorithm runs polynomial time produces consistent concept representation size olog size smallest consistent representation levels rules implies algorithm learns pac model algorithm repeatedly applies greedy heuristics weighted set cover weights obtained approximate solutions previous set cover problems
characterizing dynamics using characterizing dynamics abstract
regularities random mapping semantics paper investigate representational methodological issues attractor network model mapping semantics based find contrary studies response time concrete words represented bits output pattern slower abstract words model also predicts response times words dense semantic neighborhood will faster words similar neighbors language consistent neighborhood effect seen mapping patterns many neighbors faster pathways since random mapping used clear cause effect different previous experiments also report rather finding time model measured time takes network presented new input criterion used determine network changed include testing hidden units results reported change direction effect abstract words now slower words dense semantic since independent reasons hidden units stopping criterion done common practice believe phenomenon interest mostly neural network however provide insight interaction hidden output units
composite geometric concepts polynomial
approach learning mixed casebased modelbased reasoning architecture casebased reasoning cbr used form solved problems speedup later problem solving using cases additional costs due retrieval time case adaptation time also storage space simply cases will result situation retrieving trying adapt old cases will take time average means must applied build case memory actually useful form utility problem approach taken construct cost model system used predict effect changes system paper describe utility problem associated cases construction cost model present experimental results demonstrate model used predict effect certain changes case memory
vector design using genetic algorithms genetic algorithmic approach vector design combines conventional generalized algorithm presented refer hybrid genetic generalized algorithm works briefly follows finite number called selected iterative cycles reproduction perform experiments various alternative design choices using processes speech image source data ratio performance measure cases showed performance improvements respect also compare results formula
massively parallel support casebased planning casebased planning previously generated plans stored cases memory solve similar planning problems future considerable time planning scratch generative planning thus potential heuristic mechanism handling intractable problems one systems need highly structured memory requires significant domain engineering complex memory indexing schemes enable efficient case retrieval contrast system based massively parallel language extremely fast retrieval complex cases large memory ability fast many advantages indexing large used memory numerous alternate ways allowing specific retrieval stored plans better fit target problem less adaptation preliminary version article ieee expert paper extended version
characterization computation nonparametric maximum likelihood estimator
optimal asymptotically optimal domains via decomposition present efficient method number processors tasks associated cells uniform grid load balancing constraints observed approximately minimizing total partition corresponds amount communication method based upon decomposition grid optimal prove assumptions problem size grows large parameters error bound associated feasible solution approaches zero also present computational results high level parallel genetic algorithm utilizes method make comparisons methods network algorithm solves within instances problem require one binary variables quadratic assignment formulation
exploration dual control finding bayesian balance exploration exploitation adaptive optimal control general intractable paper shows compute suboptimal estimates based equivalence approximation arising form dual control extends existing uses exploration reinforcement learning sutton approach two components statistical model uncertainty world way exploratory behaviour
critical points problems involving certain analytic functions applications sigmoidal nets paper deals nonlinear problems involving fitting data parameterized analytic functions generic regression data general result stronger assumptions set functions giving rise critical points quadratic loss function special case usually called layer neural networks built upon standard sigmoidal activation upper bound provided
role generic models conceptual change research part nsf grant part onr grant thank john use contribution developing constructive modeling interpretation helpful comments abstract hypothesize generic models central conceptual change science hypothesis two theoretical sources first source constructive modeling derives theory analyses historical conceptual changes science reasoning representation cognitive psychology theory constructive modeling generic mental models conceptual change second source adaptive modeling derives computational theory creative design theories situation independent domain abstractions generic models using constructive modeling interpretation reasoning exhibited collected john problem solving session involving conceptual change employ resources theory adaptive modeling develop new computational model describe analysis illustrate synthesis two theories used develop system testing results research show generic modeling plays central role conceptual change also demonstrate synthesis provide significant insights scientific reasoning
designing neural networks adaptive control paper discusses design neural networks solve specific problems adaptive control particular investigates influence typical problems arising realworld control tasks techniques solution exist framework based investigation systematic design method developed method development adaptive force controller robot manipulator
unsupervised discrimination data via optimization binary information gain present informationtheoretic derivation learning algorithm clusters data linear contrast methods try preserve information input patterns maximize information gained observing output robust binary implemented sigmoid nodes derive local weight adaptation rule via gradient objective demonstrate dynamics simple data sets relate approach previous work suggest directions may extended
dynamic logic module paper presents asocs adaptive selforganizing concurrent system model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control asocs adaptive network composed many simple computing elements operating parallel paper focuses adaptive algorithm details architecture learning algorithm significant memory knowledge maintenance advantages previous asocs models asocs operate either data processing mode learning mode learning mode asocs given new rule expressed boolean conjunction learning algorithm incorporates new rule distributed fashion short bounded time data processing mode asocs acts parallel hardware circuit
mixed memory markov models time series analysis paper presents method analyzing coupled time series using markov models domain state space make parameter estimation tractable large state space represented product smaller state spaces paradigm known factorial markov models transition matrix model represented mixture transition matrices underlying dynamical processes formulation know mixed memory markov models using framework analyze daily exchange rates five measured
robot learning parallel genetic algorithms computers work explores use machine learning methods extracting knowledge simulations complex systems particular use genetic algorithms learn rulebased strategies used autonomous robots evaluation given strategy may require several simulation produce estimate quality strategy consequence evaluation single individual genetic algorithm requires fairly substantial amount computation system suggests sort parallelism available network describe implementation parallel genetic algorithm present case studies resulting speedup two robot learning tasks
word perfect efficient transformation implementing feedforward neural networks artificial neural networks anns fixed topology learning often suffer number shortcomings result variations anns use dynamic topologies shown ability overcome many problems paper introduces transformations general strategy implementing distributed feedforward networks use dynamic topologies dynamic anns efficiently parallel hardware creates set nodes node computes part network output independent nodes using local information type transformation allows efficient support adding nodes dynamically learning particular paper presents dynamic backpropagation networks single hidden layer complexity learning execution algorithms single pattern number inputs number outputs number hidden nodes original network keywords neural networks backpropagation implementation design dynamic topologies architectures
application parallel genetic algorithm max problem hard combinatorial problems scheduling led recently research genetic algorithms canonical coding symmetric modified coding problem solution space different way show known genetic operators act coding scheme subset solutions contain probably best solutions respect objective every new problem needs determination necessary condition genetic algorithm work proof experiment implemented asynchronous parallel genetic algorithm computer network computational results new heuristic discussed
implementation parallel selforganizing learning model paper presents implementation priority adaptive selforganizing concurrent system learning model built using module many current hardware implementations neural network learning models direct implementations classical neural network large number simple computing nodes connected dense number weighted links one class asocs adaptive selforganizing concurrent system connectionist models whose overall goal classical neural networks models whose functional mechanisms differ significantly model potential application areas pattern recognition robotics logical inference dynamic control
genetic algorithm based scheduling dynamic environment application adaptive optimization strategies scheduling systems recently become research topic broad interest population based approaches scheduling static data models whereas realworld scheduling tends dynamic problem paper briefly outlines application genetic algorithm dynamic job shop problem arising production scheduling first genetic algorithm handle times second step simulation method used improve performance algorithm finally job shop nondeterministic optimization problem arising job temporal decomposition leads scheduling control simulation time genetic search
comparing adaptive connection pruning pure early stopping neural network pruning methods level individual network parameters connection weights improve generalization shown empirical study however open problem pruning methods known today selection number parameters removed pruning step pruning strength work presents pruning method automatically adapts pruning strength evolution weights loss generalization training method requires algorithm parameter user results statistical significance tests comparing static networks early stopping given based extensive experimentation different problems results indicate training pruning often significantly better significantly worse training early stopping without pruning furthermore often superior superior diagnosis tasks pruning early training process required
casebased similarity assessment estimating experience casebased problemsolving systems rely similarity assessment select stored cases whose solutions easily fit current problems however similarity assessment strategies evaluation semantic similarity poor predictors result systems may select cases difficult impossible adapt even easily cases available memory paper presents new similarity assessment approach similarity directly case library containing systems adaptation knowledge examines approach context casebased planning system learns new plans new empirical tests alternative similarity assessment strategies show approach enables better case selection increases benefits learned
learning integrate multiple knowledge sources casebased reasoning casebased reasoning process depends multiple overlapping knowledge sources provides opportunity learning exploiting opportunities requires determining learning mechanisms use individual knowledge source also different learning mechanisms interact combined utility paper presents case study examining relative contributions costs involved learning processes three different knowledge case adaptation knowledge similarity casebased planner demonstrates importance interactions different learning processes identifies promising method integrating multiple learning methods improve casebased reasoning
case study casebased cbr casebased reasoning depends multiple knowledge sources beyond case library including knowledge case adaptation criteria similarity assessment hand coding knowledge accounts large part knowledge acquisition developing cbr systems acquire learning cbr promising learning method apply observation suggests developing casebased cbr systems cbr systems whose components use cbr however despite early interest casebased approaches cbr method received little attention open questions include casebased components cbr system designed amount knowledge acquisition effort require effectiveness paper investigates questions case study issues addressed methods used results achieved casebased planning system uses cbr guide case adaptation similarity assessment paper discusses design presents empirical results support usefulness casebased cbr point potential problems tradeoffs directly demonstrate overlapping roles different cbr knowledge sources paper general lessons casebased cbr areas future research
nested networks robot control
data discrimination via linear support vector machines linear support vector machine formulation used generate fast algorithm two sets space number points orders magnitude larger algorithm creates sufficiently small linear programs separate data time key idea small number support vectors corresponding linear programming constraints positive dual variables carried successive small linear programs containing data prove procedure monotonic finite number steps exact solution leads globally optimal separating plane entire dataset numerical results fully dense available datasets points dimensional space confirm theoretical results demonstrate ability handle large problems
achieving machine learning sejnowski developed system english chapter describes machine learning approach builds upon extends initial work among many extensions system following different learning algorithm wider input window errorcorrecting output coding word results decision subsequent decisions addition several useful input features changes system performs much better original system training words system achieves correct individual correct whole words must exactly match correct based three human blind assessment study system estimated serious error rate whole words compared error rate
misclassification minimization problem minimizing number points plane attempting separate two point sets convex real space formulated linear program equilibrium constraints lpec general lpec exact penalty problem quadratic objective linear constraints algorithm proposed penalty problem stationary point global solution novel aspects approach include linear formulation step function exact penalty formulation without constraint assumptions iii exact solution extraction sequence penalty function finite value penalty parameter general lpec explicitly exact solution lpec constraints parametric quadratic programming formulation lpec associated misclassification minimization problem
strategies multiple case plan replay planning analogical reasoning learning method consists storage retrieval replay planning episodes planning performance improves reuse library planning cases retrieval driven similarity metrics based planning goals scenarios complex situations multiple goals retrieval may find multiple past planning cases similar new planning situation paper presents issues implications involved replay multiple planning cases opposed single one multiple case plan replay involves adaptation merging derivations planning cases several strategies replay introduced process various forms differences past new situations planning cases particular introduce effective merging strategy considers plan step choices especially appropriate planning plan execution illustrate discuss effectiveness merging strategies specific domains
mixedinitiative casebased planning mixedinitiative planning framework automated human planners interact construct plans satisfy specific objectives paper report work engineering robust mixedinitiative planning system human planners rely strongly past planning experience generate new plans casebased system supports human planning plans past plans several plan analysis automated planner combines generative casebased planning stored plans plan rationale reuse involves adaptation driven rationale system integrates realtime mixedinitiative planning system main technical approach consists allowing user specify link objectives enable system capture reuse plan rationale present concrete application domain force planning system increases planning efficiency human planners automated similar past plans plausible plan modifications
combining predictions multiple classifiers using competitive learning neural networks primary goal inductive learning generalize induce function accurately produces correct output future inputs showed certain assumptions combining predictions several separately trained neural networks will improve generalization one key assumptions individual networks independent errors produce standard way performing backpropagation assumption may standard procedure network weights region weight space near means search may reach small subset possible local minima paper present approach neural networks uses competitive learning create networks originally far weight space thereby potentially increasing set local minima report experiments two realworld datasets combinations networks method generalize better networks traditional way
two algorithms inducing structural equation models data present two algorithms inducing structural equation models data assuming latent variables models causal interpretation parameters may estimated linear multiple regression algorithms comparable rely conditional independence present algorithms empirical comparisons
approach connectionist theory refinement refining topologies knowledgebased neural networks
approximation neural networks local global approximation investigate neural network based approximation methods methods depend basis functions local global basis functions propose multiresolution hierarchical method various resolutions stored various levels tree root tree global approximation store learning samples intermediate nodes store intermediate representations order find optimal partitioning input space selforganising maps used proposed method problems encountered simulations will investigate parallel implementation method using parallel hierarchical simulations starting point
incremental learning feedforward network incremental learning new approach incremental training feedforward network single hidden layer based idea describe output weights hidden nodes set basis functions hidden nodes treated representation network output weights domain proved separate training hidden nodes conflict previously optimized nodes described special relationship backpropagation rule advantage existing algorithms extremely fast learning approach also easily extended incrementally arbitrary function linear functions necessarily tested net benchmark problems
beyond predictive accuracy potential users machine learning technology nontrivial problem choosing large number available tools one appropriate particular task assist often users desirable model selection process automated using experience base level learning researchers proposed possible solution predictive accuracy criterion work focusing discovery rules match applications models based accuracy although predictive accuracy clearly important criterion also case number criteria often considered learning model selection paper presents number criteria discusses impact approaches model selection
learning continuous attractors recurrent networks one approach invariant object recognition employs recurrent neural network associative memory standard networks state space memories objects stored attractive fixed points dynamics argue modification object continuous family represented continuous attractor idea illustrated network learns complete patterns perform task missing information network develops continuous attractor models patterns drawn statistical viewpoint pattern task allows formulation unsupervised approach invariant object recognition use recurrent neural network associative intuitive biological plausibility approach largely practical applications paper introduces two new concepts help object representation continuous attractors learning attractors pattern models associative memory memories stored attractive fixed points discrete locations state discrete attractors may appropriate patterns continuous variability like images object different object lie continuous pattern appropriate represent objects attractive fixed points continuous attractors make idea practical important find methods learning attractors examples naive method train network examples shortterm memory method network fixed points examples superior method train network examples corrupted learns complete patterns missing information learning terms regression rather density estimation
adaptive evolutionary graph coloring paper consider problem independent constraint handling mechanism stepwise adaptation weights show working graph coloring problems penalty function based approaches amounts modifying penalty function search show benefit first rather technical parameters thereby providing general problem independent way handle constrained problems second leads superior performance extensive series comparative experiments show outperforms powerful graph coloring heuristic algorithm graph instances linear behaviour
simple neuron models independent component analysis recently several neural algorithms introduced independent component analysis approach problem point view single neuron first simple learning rules introduced estimating one independent components data learning rules used estimate independent component negative others estimate component positive next system introduced estimate independent component results generalized estimate independent components raw mixtures separate several independent components system several neurons linear negative feedback used convergence learning rules proven without hypotheses distributions independent components
casebased acquisition place knowledge paper define task place learning describe one approach problem framework represents distinct using evidence probabilistic description place recognition relies casebased classification augmented process correct learning mechanism also similar casebased systems involving simple storage inferred evidence experimental studies physical simulated robots suggest approach improves place recognition experience handle significant sensor noise scales increasing numbers previous researchers studied evidence place learning combined two powerful concepts used experimental methods machine learning evaluate methods abilities
unsupervised constructive learning constructive induction learners problem representation modified normal part learning process useful initial representation inadequate inappropriate paper argue distinction constructive methods propose theoretical model allows distinction made process properly motivated also show although constructive induction used almost context supervised learning reason form part unsupervised regime
inductive database design designing deductive database decide predicate relation whether defined definition look like intelligent system presented assist task example database predicates defined tries compact database defined predicates defined ones intelligent system employs techniques area inductive logic programming
possible world partition sequences unifying framework uncertain reasoning work information multiple sources formalism employs handle uncertainty may uniform order able combine knowledge bases different need first establish common basis characterizing evaluating different provide semantics combined mechanism common framework provide building integrated system essential understand behavior present unifying framework based ordered partition possible worlds called partition sequences corresponds intuitive notion towards certain possible scenarios uncertain actual situation show existing namely default logic logic probabilistic conditioning thresholding generalized conditioning possibility theory incorporated general framework
signal separation nonlinear hebbian learning
using network solve problems large dimensionality paper investigates technique creating connected feedforward neural networks may capable producing networks large input output layers architecture appears particularly suited tasks involve sparse training data able take advantage reduce training time initial results presented based tests bit compression problem
bayesian inference graphical gaussian models paper propose method calculate posterior probability graphical gaussian model proposal based new device sample distributions conditional graphical constraints result methodology allows bayesian model selection within whole class graphical gaussian models including ones
metrics temporal difference learning markov chain reinforcement transition gives simple example function learned depends showed approximation optimal respect error value function approximation obtained method poor respect metric respect error values approximates function better td0 however respect error differences values td0 approximates function better better td0 respect former metric rather latter addition direct weights errors residual gradient methods weight errors equally case control simple markov decision process presented direct td0 residual gradient td0 learn optimal policy learns suboptimal policy results suggest example differences state values significant state values td0
locally weighted learning control lazy learning methods provide useful representations training algorithms learning complex phenomena autonomous adaptive control complex systems paper surveys ways locally weighted learning type lazy learning applied control tasks explain various forms control tasks take choice learning paradigm discussion section explores interesting impact explicitly previous experiences problem learning control
evolving compact solutions genetic programming case study genetic programming variant genetic algorithms data structures handled trees makes especially useful evolving functional relationships computer programs represented trees symbolic regression determination function dependence approximates set data points paper feasibility symbolic regression demonstrated two examples taken different domains furthermore several suggested methods literature compared intended improve performance solutions taking account introns occurs trees keeping size trees small experiments show useful tool derive complex functional dependencies numerical data
mixture models exploration relationships drug design report study mixture modeling problems arising assessment chemical relationships drug design discovery research laboratories developing test compounds many related candidate compounds together basic molecular building blocks known compounds tested biological activity analysis drug design tests also provide data relating activity chemical properties aspects structure associated focus studying relationships aid future selection level chemical activity compounds based chemical binding test compounds target binding sites compounds tests identify binding configurations hence potentially critical information missing natural latent variable resulting statistical models mixed respect missing information data analysis inference paper reports study site framework associated data build structured mixture models linear regression models predicting chemical effectiveness respect selection mechanisms discuss aspects modeling analysis including problems describe results analyses simulated real data set modeling real data led critical model extensions introduce hierarchical random effects components capture site binding mechanisms resulting levels effectiveness compounds bound comments current potential future directions conclude report
evolving guided robots version paper appears proceedings second international conference simulation adaptive behaviour mit press cambridge
bound error cross validation using approximation estimation rates give analysis generalization error cross validation terms two natural measures difficulty problem approximation rate accuracy target function approximated function number hypothesis parameters estimation rate training generalization errors function number hypothesis parameters approximation rate captures complexity target function respect hypothesis model estimation rate captures extent hypothesis model overfitting using two measures give rigorous general bound error cross validation bound clearly shows tradeoffs involved making fraction data testing large small optimizing bound respect argue combination formal analysis controlled experimentation following qualitative properties cross validation behavior quite robust significant changes underlying model selection problem
estimating expected error empirical model selection experimental theoretical comparison model selection considered problem choosing hypothesis language provides optimal balance low empirical error high structural complexity abstract discuss new efficient approach model selection approach inherently bayesian instead using priors target functions hypotheses priors error values leads new mathematical characterization expected true error setting classification learning learner given sample drawn according unknown distribution labeled instances returns empirical hypothesis least empirical error certain unknown true error process carried repeatedly true error empirical will vary run run empirical depends randomly drawn sample induces distribution true errors empirical possible samples drawn according unknown distribution distribution known one easily derive expected true error empirical model integrating distribution lead optimal model selection algorithm models calculate expected error model integrating error distribution select model least expected error pac theory framework provide worstcase bounds sample true error worstcase meaning hold distribution instances concept given class contrast focus determine distribution fixed given learning problem specified assumptions unlike worstcase bound depends size hypothesis space actual error distribution depends hypothesis space unknown distribution labeled instances however prove certain assumption independence hypotheses distribution true errors hence expected true error expressed function distribution empirical errors uniformly drawn hypotheses thought prior error values latter distribution always onedimensional estimated initial training data set randomly drawn hypotheses estimate distribution now leads estimate expected true error empirical model turn leads highly efficient model selection algorithm study behavior approach several controlled experiments results show accuracy error estimate least comparable accuracy estimate obtained crossvalidation provided prior error values estimated using least examples requires ten learner per model time algorithm requires assess model constant size model also study robustness algorithm independence assumptions observe bias predictions hypotheses space size four less hypothesis space size dependencies assumptions significant error full paper available
generalization clauses implication area inductive learning generalization main operation usual definition induction based logical implication recently interest clausal representation knowledge machine learning almost inductive learning systems perform generalization clauses use relation subsumption instead implication main reason wellknown simple technique compute least general generalizations subsumption implication however generalization subsumption inappropriate learning recursive clauses crucial problem since basic program structure logic programs note implication clauses therefore introduce stronger form implication called clauses show every finite set clauses exists least general generalization describe technique reduce generalizations implication clause generalizations subsumption call expansion original clause moreover show every clause exists expansion means every generalization clause reduced generalization subsumption expansion
computing distributions order statistics recurrence relationships among distribution functions order statistics independent distributed random quantities derived results extend known theory provide computationally algorithms variety problems
bayesian probability theory general method machine learning paper argues bayesian probability theory general method machine learning two theory capable learning tasks incremental supervised unsupervised learn different types data regardless whether noisy perfect independent facts behaviors unknown machine capabilities partially demonstrated paper uniform application theory two typical types machine learning incremental concept learning unsupervised data classification generality theory suggests process learning may many different types currently held method may best
bayesian models nonlinear
bias variance error output codes local learners paper focuses bias variance decomposition analysis local learning algorithm nearest neighbor classifier extended error output codes extended algorithm often considerably reduces classification error comparison nearest neighbor analysis presented reveals performance improvement obtained reducing bias cost increasing variance also show even classification problems classes extending codeword length beyond limit separation yields error reduction error reduction variance due voting mechanism used errorcorrecting output codes also bias
comparison random search versus genetic programming collective adaptation integrated distributed search genetic programming based systems collective memory form collective adaptation search method system significantly improves search problem complexity increased since pure approach scale problem complexity natural question two components actually search process investigate collective memory search utilizes random search engine find significantly outperforms based search engine examine solution space show problem complexity search space grow collective adaptive system will perform better collective memory search employing random search engine
hierarchical priors mixture models application regression density estimation
genetic algorithm tutorial technical report revised november
retrieve relevant information document presents approach relevance retrieved information based novel approach similarity assessment contrary systems define relevance measures context similarity query time necessary since without context similarity one guarantee similar items will also relevant
multistrategy casebased reinforcement learning approach reactive control systems autonomous robotic navigation paper presents reactive control system autonomous robotic navigation navigation module uses reactive control system perform navigation task learning module combines casebased reasoning reinforcement learning continuously tune navigation system experience casebased reasoning component systems environment appropriate case uses case tune parameters reactive control system reinforcement learning component refines content cases based current experience together learning components perform online adaptation resulting improved performance reactive control system environment online learning resulting improved library cases capture environmental regularities necessary perform online adaptation system extensively evaluated simulation studies using several performance metrics system configurations
learning model learning presented system learns hard patterns classifying easy ones model related filtering methods takes account addition filtering data cost simple policies introduced analyzed simple problem high low game addition algorithm suggested good approximator model space realworld domains results using algorithm synthesized problem realworld task using backpropagation network nearest neighbor classifier show learner perform classifier trained achieving significant cost reduction
study program response negative effects introns genetic programming standard method obtaining response genetic programming take value root node representations alternate methods explored one alternative specific location memory response value program purpose paper explore applicability technique treestructured programs explore effects studies light papers experimental results support finding memorybased program response technique improvement problems addition papers experimental results support finding contrary past research speculation addition even introns search performance genetic programming
defense c45 notes learning decision trees discuss implications article demonstrated commonly used data simple classification rules almost accurate decision trees produced c45 consider particular significance results future topdown induction decision trees extent sense research decision tree learning detail parts study try put results perspective argue absolute terms small difference accuracy c45 still significant claim c45 additional advantages addition discuss databases used compare empirically optimal decision trees observe significant differences point several classifiers
describe approach given set examples words associated representation language system automatically produced language takes input words produces output according rules implicit training data describe design system compare performance knowledgebased alternative approaches
empirical entropy manipulation realworld problems finite sample sufficient determine density therefore entropy signal directly assumption either functional form density smoothness necessary amount prior space possible density functions far common approach assume density parametric form contrast derive differential learning rule called optimizes entropy way kernel density estimation entropy calculated sampling density estimate resulting parameter update rule surprisingly simple efficient will show used detect correct images application beyond scope existing parametric entropy models
sparse representation function approximation derive new general representation function linear combination local correlation optimal sparse locations scales characterize relation pca regularization principles support vector machines
sample complexity finding good search strategies trials experiment search problem consists set probabilistic experiments performed order without configuration failures cost performing experiments depends order chosen earlier work finding optimal search strategies special cases model search trees andor graphs cost function success probabilities experiments given contrast study complexity learning approximately optimal search strategy success probabilities known working fully general model show number unknown probabilities maximum cost performing experiments
analyzing formation structure highdimensional selforganizing maps reveals differences feature map models present method calculating phase diagrams highdimensional variant selforganizing map method requires data space induced map explicit state map using method analyze two recently proposed models development orientation ocular dominance maps phase transition condition orientation map turns different form corresponding map
comparison neural statistical classifiers theory practice research reports january
adaptive load balancing study multiagent learning study process multiagent reinforcement learning context load balancing distributed system without use either central coordination explicit communication first define precise framework study adaptive load balancing important features stochastic nature purely local information available individual agents given framework show results basic adaptive behavior parameters effect system efficiency investigate properties adaptive load balancing heterogeneous populations address issue exploration exploitation context finally show naive use communication may improve might even system efficiency
efficient stochastic source coding application bayesian network source model hinton efficient stochastic source coding application bayesian network source model computer journal paper introduce new algorithm called bitsback coding makes stochastic source codes efficient given source code show algorithm actually efficient algorithm always shortest codeword optimal efficiency achieved chosen according boltzmann distribution based codeword turns commonly used technique determining parameters maximum likelihood estimation actually minimizes bitsback coding cost chosen according boltzmann distribution tractable approximation maximum likelihood estimation generalized expectation maximization algorithm minimizes bitsback coding cost presenting binary bayesian network model assigns exponentially many symbol show tractable approximation boltzmann distribution used bitsback coding illustrate performance bitsback coding using using data binary bayesian network source model produces possible input symbol rate bitsback coding nearly one obtained shortest codeword symbol
learning take agents learn agents exploit information possess distinct advantage competitive situations games provide environments study agent learning strategies researchers developed game playing programs learn play better experience developed learning program learn play better learns identify exploit weaknesses particular opponent repeatedly playing several games propose scheme learning opponent action probabilities utility maximization framework exploits learned opponent model show proposed expected utility maximization strategy generalizes traditional strategy allows benefit taking calculated avoided strategy experiments popular board game show learning player consistently outperforms player another automated player using heuristic though proposed mechanism improve level computer player improve ability play effectively opponent
factorial learning algorithm many real world learning problems best characterized interaction multiple independent causes factors discovering causal structure data focus paper based cooperative vector architecture unsupervised learning algorithm derived expectationmaximization framework due combinatorial nature data generation process exact computationally intractable two alternative methods computing proposed gibbs sampling approximation promising empirical results presented
blind identification separation technique via multilayer neural networks paper deals problem blind identification source separation consists estimation mixing matrix andor separation mixture stochastically independent sources without priori knowledge mixing matrix method propose estimates mixture matrix recurrent inputoutput identification using inputs nonlinear transformation estimated sources nonlinear transformation distortion consists inputs device constant contrast existing approaches covariance additive noise need modeled estimated regular parameter needed proposed approach implemented using multilayer neural networks order improve performance separation new associated online unsupervised adaptive learning rules also developed effectiveness proposed method illustrated computer simulations
performance source separation algorithms source separation consists recovering set independent signals observed mixtures signals possibly corrupted additive noise many source separation algorithms use second order information operation reduces non trivial part separation determining matrix show kind property exploited predict general results performance first contribution exhibit lower bound performance terms accuracy separation bound independent algorithm case distribution source signals second show performance invariant algorithms depends mixing matrix noise level specific way consequence low noise levels performance depend mixture distribution sources via function characteristic given source separation algorithm
local adaptive learning algorithms blind separation natural images paper neural network approach reconstruction natural highly correlated images linear additive mixture proposed multilayer architecture local online learning rules developed solve problem blind separation sources main motivation using multilayer network instead one improve performance robustness separation applying simple local learning rule biologically plausible moreover architecture learning relatively easy using circuits furthermore enables extraction source signals one starting signal one experimental part focuses separating highly correlated human faces mixture additive noise unknown number sources
learning solve markovian decision processes
using combining predictors study online learning algorithms predict combining predictions several prediction algorithms sometimes called experts simple algorithms belong multiplicative weights family algorithms performance algorithms number experts making particularly useful applications number experts large however applications text categorization often natural experts making predictions instances show transform algorithms assume experts always algorithms require assumption also show derive corresponding loss bounds method general applied large family online learning algorithms also give applications various prediction models including decision graphs switching experts
naive bayesian classifier within dealing classification problems current ilp systems often behind learners part much larger hypothesis space therefore explored however sometimes due fact ilp systems take account probabilistic aspects hypotheses classifying unseen examples paper proposes just developed naive bayesian classifier within first order learner learner uses relief based heuristic able detect strong dependencies within space dependencies exist conducted series experiments artificial realworld data sets results show combination together naive bayesian classifier sometimes significantly improves classification unseen instances measured classification accuracy average information score
dynamic simulation linear programming based methods process simulation valuable tool process design analysis operation work extend capabilities iterated linear programming dealing problems encountered dynamic process simulation previously developed method refined addition new descent strategy combines line search region approach adds stability efficiency method method advantage naturally dealing profile bounds demonstrated avoid computational difficulties arise regions new method treatment occurring dynamic simulation problems also presented paper method event within time interval detected one event occurs detected one indeed one specific class implicitly process simulation problems phase equilibrium calculations also new formulation introduced solve problems correspondence addressed
general method incremental multiagent learning
control parallel population dynamics behavior frequently observed difficulty application genetic algorithms domain optimization arises convergence order preserve genotype diversity develop new model behavior individuals model population active individual assumes behavior patterns different individuals living population assume different patterns moving hierarchy social states individuals change behavior changes social state controlled arguments plausibility arguments implemented rule set genetic algorithm computational experiments largescale job shop benchmark problems show results new approach ordinary genetic algorithm significantly
set neural network benchmark problems rules collection problems neural network learning pattern classification function approximation plus set rules benchmark tests similar problems contains data sets different domains datasets represent realistic problems called diagnosis tasks one consist real world data datasets presented simple using attribute representation directly used neural network training along datasets defines set rules conduct document neural network purpose problem rule collection give researchers easy access data evaluation algorithms networks make direct comparison published results feasible report describes datasets rules also gives basic performance measures indicating difficulty various problems measures used comparison
learning play game chess paper presents program learns play chess final outcome games learns chess board evaluation functions represented artificial neural networks integrates inductive neural network learning temporal variant explanationbased learning performance results illustrate strengths weaknesses approach
preprocessing model integrating cbr neural networks important factors play major role determining performances cbr casebased reasoning system complexity accuracy retrieval phase flat memory inductive approaches suffer serious drawbacks first approach search time increases dealing large scale memory base second one modification case memory becomes complex sophisticated architecture paper show construct simple efficient indexing system structure idea construct case hierarchy two levels memory lower level contains cases groups similar cases upper level contains prototypes prototype represents one group cases smaller memory used retrieval phase prototype construction achieved means incremental neural network show mode coupling preprocessing one neural network indexing system
pac learning onedimensional patterns developing ability recognize visual image robots current location fundamental problem robotics consider problem paclearning concept class geometric patterns target geometric pattern configuration points real line instance configuration points real line labeled according whether target pattern capture notion visual use metric two geometric patterns metric every point one pattern close point pattern relate concept class geometric patterns recognition problem present polynomialtime algorithm class onedimensional geometric patterns also present experimental results algorithm performs
model selection using measure functions concept measure functions generalization performance suggested concept provides alternative way selecting evaluating learned models classifiers addition makes possible state learning problem computational problem known prior problem domain captured measure function possible combination training set classifier assigns value describing good classifier computational problem find classifier maximizing measure function argue measure functions great value practical applications tool model selection force make explicit relevant prior knowledge learning problem hand provide understanding existing algorithms iii help construction algorithms illustrate last point suggesting novel algorithm based incremental search classifier optimizes given measure function
search attractors recurrent attractor networks offer many advantages feedforward networks modeling psychological phenomena dynamic nature allows capture time course cognitive processing learned weights may often easily interpreted soft constraints representational components perhaps significant feature networks however ability facilitate generalization constraints intermediate output representations attractor networks learn systematic regularities formed representations small number examples possess attractors paper investigates conditions attractors arise recurrent networks trained using variants backpropagation results computational experiments demonstrate structured attractors appear emergence appropriate error signal presented directly recurrent processing elements show however error signals weights pose serious problems networks kind present simulation results discuss reasons difficulty suggest directions future attempts
simplifying decision trees survey induced decision trees solution classification tasks many practical tasks trees produced algorithms comprehensible users due size complexity although many tree induction algorithms shown produce simpler comprehensible trees data structures derived trees good classification accuracy tree usually secondary relative accuracy attempt made survey literature perspective present framework approaches tree summarize approaches within framework purpose survey provide researchers overview approaches insight relative capabilities final discussion briefly describe empirical findings discuss application tree induction algorithms case retrieval casebased reasoning systems
markov samplers path simple diagnostic idea paper propose monitor markov chain sampler using path chosen dimensional summary argue path effectively sequential aspects markov sampler user quickly sampler moving around sample space direction summary proposal illustrated four examples represent situations path works moreover rigorous analysis given one examples conclude path effective tool convergence markov sampler comparing different markov samplers
bounding convergence time gibbs sampler bayesian image paper gives precise easy compute bounds convergence time gibbs sampler used bayesian image reconstruction sampling gibbs distribution without presence external field bounds number obtained constant easy calculate key words bayesian image convergence gibbs sampler model markov chain monte carlo
symmetry selforganizing orientation map development
coupled hidden markov models modeling interacting processes perceptual computing learning common sense technical report revised abstract present methods coupling hidden markov models hmms model systems multiple interacting processes resulting models multiple state variables temporally coupled via matrices conditional probabilities introduce deterministic approximation maximum posterior map state estimation enables fast classification parameter estimation via expectation maximization dynamic programming algorithm samples highest probability paths compact state minimizing upper bound cross entropy full dynamic programming problem complexity chains states observing data points compared naive product exact state clustering stochastic monte carlo methods applied inference problem several experiments examining training time model classification accuracy robustness initial conditions coupled hmms compared favorably conventional hmms approaches coupled inference chains demonstrate compare algorithms synthetic real data including interpretation
possible biases induced mcmc convergence
learning logical exceptions chess
bayesian analysis field experiments summary paper describes bayesian analysis field experiments topic received little previous attention despite literature bayesian paradigm interpretation results especially ranking selection also complex analyzed comparative using markov chain monte carlo methods key approach need spatial representations patterns discussed detail problems caused via may find use contexts paper includes three analyses variety trials yield one example involving binary data entirely straightforward comparisons analyses made datasets available
collection algorithms belief networks correspond portions report published proceedings annual symposium computer applications medical care november
discretization continuous markov chains mcmc convergence assessment show paper continuous state space markov chains finite markov chains idea continuous chain times related small sets control discretization finite markov chain derived mcmc output general convergence properties finite state spaces exploited convergence assessment several directions choice based criterion derived first evaluated parallel chains stopping time implemented efficiently two parallel chains using theorem stopping rules performance criterion illustrated three standard examples
parallel markov chain monte carlo sampling markov chain monte carlo mcmc samplers proved popular tools bayesian computation however problems arise application density interest high dimensional strongly correlated circumstances sampler may slow state space mixing poor article offer partial solution problem state space markov chain augmented accommodate multiple chains parallel updates individual chains based around genetic style crossover operator states drawn population chains process makes efficient use gradient information implicitly encoded within distribution states across population empirical studies support claim crossover operator parallel population chains improves mixing illustrated example sampling high dimensional posterior probability density complex predictive model latent variable approach methodology extended deal variable selection model averaging high dimensions illustrated example selection spline
database mit computational cognitive science technical report abstract describe variational approximation methods efficient probabilistic reasoning applying methods problem diagnostic inference database database largescale belief network based statistical expert knowledge internal size complexity network exact probabilistic diagnosis infeasible small set cases development network practical diagnostic tool researchers exploring diagnostic behavior paper describe variational approximation methods applied network resulting fast diagnostic inference evaluate accuracy methods set standard diagnostic cases compare stochastic sampling methods
rbf network effects neural networks topology performance known yet question finding optimal configurations automatically remains largely open paper proposes solution problem rbf networks self approach driven evolutionary strategy taken algorithm uses output information computationally efficient approximation rbf networks clustering process two parameters networks number positions empirical results demonstrate promise
evolution learning years baldwin effect using learning facilitate paper describes hybrid methodology integrates genetic algorithms decision tree learning order evolve useful subsets features recognizing complex visual concepts genetic algorithm used search space possible subsets large set candidate discrimination features candidate feature subsets evaluated using c45 decisiontree learning algorithm produce decision tree based given features using limited amount training data classification performance resulting decision tree unseen testing data used fitness underlying feature subset experimental results presented show increasing amount learning significantly improves feature set evolution difficult visual recognition problems involving facial image data addition also report extent aspects baldwin effect exhibited system
evaluating computational crisis response paper examine behavior system crisis response one instance crisis management describe task involving describe intelligent assistant planning scheduling domain relation human users focus strategy retrieving case case library initial schedule user adapt also present three hypotheses behavior mixedinitiative system experiments designed test results suggest approach leads faster response development schedules without solution quality
explanations empirically derived reactive plans given adequate simulation model task environment payoff function measures quality partially successful plans heuristics genetic algorithms develop high performance reactive rules interesting sequential decision tasks previously described implemented system called learning reactive plans shown system successfully learn rules laboratory scale problem paper describe method deriving explanations success empirically derived rule sets method consists plausible explaining reactive rules sequence actions satisfy
learning concepts sensor data mobile robot machine learning valuable tool improving flexibility efficiency robot applications many approaches applying machine learning robotics known approaches enhance robots highlevel processing planning capabilities approaches enhance processing control basic actions contrast approach presented paper uses machine learning link representations sensing action highlevel representation planning aim facilitate communication robot human user hierarchy concepts learned records mobile robot perception action combined every level concepts relational learning algorithm developed completely searches hypothesis space restricted rule schemata user defines terms grammars
convergence markov chain monte carlo algorithms use convergence diagnostic techniques markov chain monte carlo algorithms review various methods proposed mcmc literature common established method discussed particular emphasis issues possible extensions methods compared terms applicability provided particular classes problems
compositional modeling
memorybased time series recognition new methodology real world applications
visual tracking moving objects using neural network controller target tracking task manipulator track object moves arbitrarily table desired mapping approximated feedforward neural network use time derivatives position object manipulator controller inherently predict next position moving target object paper several controllers described successfully applied track moving object
machine learning
regression build predictive causal models covariance information help algorithm search predictive causal models estimate strengths causal relationships information conditional independence constraints identified usual causal induction algorithms algorithm combines covariance information effective heuristic build predictive causal models demonstrate accurate efficient one experiment assess ability find best predictors variables another compare performance using many measures pearl algorithm although based multiple linear regression evidence performs problems difficult regression algorithms
learning sequential decision rules using simulation models competition problem learning decision rules sequential tasks addressed focusing problem learning decision rules simple simulator learning method relies notion competition employs genetic algorithms search space decision policies several experiments presented address issues arising differences simulation model learning occurs target environment decision rules tested
utilizing prior concepts learning inductive learning problem consists learning concept given examples concept perform learning task inductive learning algorithms bias learning method discuss learning method use previously learned concepts domain learned concepts useful information concepts domain describe bias present horn clause relational learning algorithm utilizes bias learn multiple concepts provide preliminary empirical evaluation show effects previous information noisefree noisy data
statistical ideas selecting network architectures choosing architecture neural network one important problems making neural networks useful accounts applications usually details many hidden units needed weight decay used much type output units chosen address issues within framework statistical theory model paper concerned architecture selection issues feedforward neural networks also known multilayer perceptrons many issues arise selecting radial basis function networks recurrent networks widely problems occur much wider context within statistics applied selecting combining models two recent discussions references discuss neural networks statistical perspective choice provides number approximate answers
statistical semantics causation key words causality induction learning propose definition causation show contrary common causal influences distinguished following standard inductive reasoning also establish complete characterization conditions distinction possible finally provide procedure inductive causation show large class data structures effective algorithms exist direction causal influences defined
study deals determine lower bound run time present algorithm bound since study points network interface also analyze performance alternative interface designs analyses based run time model network
abstract automated decision making often complicated complexity knowledge involved much complexity arises contextsensitive variations underlying phenomena propose framework representing contextsensitive knowledge approach attempts integrate categorical uncertain knowledge network formalism paper outlines basic representation constructs examines efficiency discusses potential applications framework
comparison error estimates neural network models summary discuss number methods estimating standard error predicted values multilayer perceptron methods include method based bootstrap estimators estimator methods described compared number examples find bootstrap methods perform best capture variability due choice starting weights
practical bayesian inference using mixtures mixtures discrete mixtures normal distributions widely used modeling electrical synapses human animal systems usual framework independent data values arising means discrete prior unknown observed gaussian noise terms important development associated statistical methods issue noise terms often norm rather exception context recently developed models based dirichlet process mixtures problems explicitly model noise data values arising dirichlet process mixture addition modeling location prior dirichlet process induces dirichlet mixture mixtures whose analysis may developed using gibbs sampling techniques discuss models analysis illustrate context response analysis
predictive robot control neural networks neural controllers able position manipulator object arbitrary table desired mapping approximated feedforward neural networks however object moving manipulator behind required time visual information move manipulator use time derivatives position object manipulator controller inherently predict next position object paper several predictive controllers proposed successfully applied track moving object
generalizing adaptive discriminant network paper aa1 adaptive algorithm model asocs adaptive self concurrent systems approach also presents promising empirical generalization results aa1 actual data aa1 dynamic network grows fit problem learned aa1 generalizes selforganizing fashion network seeks find features discriminate concepts convergence training set guaranteed bounded linearly time
maximum likelihood source separation discrete sources communication deals source separation problem consists separation noisy mixture independent sources without priori knowledge mixture coefficients paper consider maximum likelihood approach discrete source signals known probability distributions important feature approach gaussian noise covariance matrix additive noise treated parameter hence necessary know model spatial structure noise another feature case discrete sources assumptions possible separate sources sensors paper consider maximization likelihood via expectationmaximization algorithm
system robust classifier robust statistical model developed classify health system wellknown series approximation technique forms basis procedure systems health fails procedure determines set causes health state used system action procedure applicable classifier known robust applied neural network traditional parametric pattern classifiers generated supervised learning procedure empirical measure optimized describe procedure mathematically demonstrate ability detect causes space complex california
towards improving case genetic algorithm case combination difficult problem case based reasoning often exhibit together previous work case combination representing case constraint satisfaction problem used minimum algorithm systematically global solution however also found instances problem minimum algorithm perform case combination efficiently paper describe situations initially retrieved cases easily propose method improve case genetic algorithm introduce fitness function maintains much retrieved case information possible also allow subsequent case combination efficiently
dynamic constraint satisfaction using casebased reasoning techniques dynamic constraint satisfaction problem formalism attention valuable often necessary extension static framework dynamic constraint satisfaction enables techniques applied extensively since applied domains set constraints variables involved problem evolves time time casebased reasoning cbr community working techniques reuse existing solutions solving new problems observed dynamic constraint satisfaction matches closely casebased reasoning process case adaptation observations previous work combining cbr achieve adaptation paper summarizes previous results describes similarity challenges case adaptation shows cbr together begin address
prior determination knowledge using pac learning model prior knowledge bias regarding concept speed task learning probably approximately correct pac learning mathematical model concept learning used quantify speed due different forms bias learning thus far pac learning mostly used analyze syntactic bias limiting concepts conjunctions boolean paper demonstrates pac learning also used analyze semantic bias domain theory concept learned key idea view hypothesis space pac learning consistent prior knowledge syntactic semantic particular paper presents pac analysis determinations type relevance knowledge results analysis relations among different determinations illustrate usefulness analysis based pac model
learning presence prior knowledge case study using model computational models natural systems often contain free parameters must set optimize predictive accuracy models process called viewed form supervised learning presence prior knowledge view fixed aspects model prior knowledge goal learn values free parameters report series attempts learn parameter values global model called system developed standard machine learning methods work constraints introduced structure model create difficult nonlinear optimization problem developed new divideandconquer approach subsets parameters others held constant approach possible select training examples portions model
virtual frequently used dataset paper considers situation learners testing set contains close approximations cases appear training set cases considered virtual since approximately seen learner generalisation measures take account frequency virtual may paper shows algorithm used derive baseline generalisation statistics process demonstrated though application study generalisation performance algorithm tested c45 commonly used datasets
music structure recognition tend know knowledge
learning refine case libraries initial results abstract conversational casebased reasoning cbr systems incrementally extract query description use however designing large case libraries good performance precision efficiency difficult cbr provide guidelines designing libraries guidelines difficult apply describe automated inductive approach conversational case libraries increase design guidelines revision increased performance three conversational case libraries
mixture model diagnosis system diagnosis process identifying machine patient considering history starting possible initial information new information sequential manner diagnosis made precise thus missing data problem since known model joint probability distribution data case database mixture models model parameters estimated algorithm gives additional benefit missing data database also handled correctly new information refine diagnosis performed using maximum utility principle decision theory since system based machine learning domain independent example using disease database presented
backpropagation give rise local minima even networks without hidden layers give example neural net without hidden layers sigmoid transfer function together training set binary vectors sum squared errors function weights local minimum global minimum example consists set training instances four weights threshold know substantially smaller binary examples exist
majority classifiers theory applications
learning accurate representational system multiple extension problem arises default theory use different subsets defaults propose different answers queries paper presents algorithm uses set observations learn version default theory essentially accurate detail associate given default theory set related theories uses total ordering defaults determine single answer return query goal select theory highest expected accuracy expected accuracy probability answer produces query will correspond correctly world unfortunately expected accuracy depends distribution queries usually known moreover task identifying optimal even given distribution information intractable paper presents method problems using set samples estimate unknown distribution hillclimbing local optimum particular given parameters ffi produces whose expected accuracy probability least ffi within local optimum appeared workshop theoretical foundations knowledge representation reasoning
qlearning technical report
complexity compression evolution compression information important concept theory learning argue hypothesis inherent compression pressure towards short general solutions genetic programming system variable length evolutionary algorithms pressure becomes size complexity solutions measured without code segments called introns built pressure effects complex fitness functions crossover probability generality maximum depth length solutions explicit fitness function depth length effects positive negative work provide basis analysis effects suggestions overcome negative implications order obtain balance needed successful evolution empirical investigation supports hypothesis also presented
cbr explanation results computer science technical report number technical report number
xcs classifier system reliably evolves accurate complete minimal representations boolean functions complex recent xcs classifier system forms complete mappings payoff environment reinforcement learning accuracy based fitness according generalization hypothesis xcs towards generalization xcs optimality hypothesis suggest xcs systems evolve optimal populations representations populations accurately map pairs payoff predictions using smallest possible set classifiers ability xcs evolve optimal populations boolean problems demonstrated using technique evolutionary search setting crossover mutation rates zero automatically performance statistics entire learning process combined techniques allow classifier system evolve optimal representations boolean functions without form
rise system without separating current rule induction systems cn2 typically rely separate strategy learning rule examples results number examples available learning successive rules systems accuracy alternative learn rules simultaneously using entire training set approach implemented rise system empirical comparison rise cn2 suggests without separating performs similarly simple domains achieves substantial gains accuracy domain difficulty grows
genetic programming minimal neural nets using occams razor genetic programming method investigated optimizing architecture connection weights multilayer feedforward neural networks genotype network represented tree whose depth width dynamically adapted particular application specifically defined genetic operators weights trained hillclimbing search new fitness function proposed principle occams razor makes optimal tradeoff error fitting ability network discuss results two problems complexity study convergence scaling properties algorithm
simple neural network models categorical perception facial expressions performance neural network facial expressions compared human subjects set experiments using experiments human subjects neural networks make use facial expressions facial affect database difference used human subjects experiments young manner images constructed versus averages nevertheless neural network accurately captures categorical nature human responses showing transitions images along sequence categorical perception model shows highest discrimination transition images crossover point model also captures shape time curves human subjects along sequences finally network matches human subjects expressions mixed images main model responses transitions seen human subjects attribute difference difference average stimuli image quality stimuli results show simple neural network classifier access biological constraints human processor whose access category labels subjects facial expressions nevertheless simulate fairly human responses expressions
signal path oriented approach generation dynamic process models article hand discusses tool automatic generation structured models complex dynamic processes means genetic programming contrast techniques use genetic programming find appropriate arithmetic expression order describe inputoutput behaviour process tool based block oriented approach description signal paths short survey techniques computer based system identification given basic concept structured model described furthermore extensions system presented detail including automatically defined fitness criteria
hyperplane ranking simple genetic algorithms examine role hyperplane ranking search performed simple genetic algorithm also develop metric measuring degree ranking exists respect static measurements taken directly function measurement dynamic ranking hyperplanes genetic search show degree dynamic ranking induced simple genetic algorithm highly correlated degree static ranking inherent function especially initial search
genetic programming methodology applications
crossover mutation genetic algorithms rely two genetic operators crossover mutation although exists large body conventional concerning roles crossover mutation roles captured theoretical fashion example never theoretically shown mutation sense less powerful crossover paper provides answers questions theoretically demonstrating important characteristics operator captured
bayesian network classification continuous attributes best discretization parametric fitting recent paper introduced classifier based bayesian networks called tree augmented naive bayes outperforms naive bayes performs c45 methods classifier several advantages including robustness polynomial computational complexity one limitation classifier applies discrete attributes thus continuous attributes must paper extend deal continuous attributes directly via parametric gaussians mixture gaussians conditional probabilities result classifier represent combine discrete continuous attributes addition propose new method takes advantage modeling language bayesian networks order represent attributes discrete continuous form simultaneously use versions classification process form attribute relevant classification task also avoids either form since different attributes may better one version empirical results show latter method usually achieves classification performance good better either purely discrete purely continuous models
structured representation complex stochastic systems paper considers problem representing complex systems evolve stochastically time dynamic bayesian networks provide compact representation stochastic processes unfortunately often since explicitly model complex structure many real life systems fact processes typically composed several interacting turn decomposed propose structured representation language extends dynamic bayesian networks objectoriented bayesian network framework show language allows describe systems natural modular way language supports natural representation certain system characteristics hard capture using traditional example allows represent systems processes evolve different rate others systems processes interact provide simple inference mechanism representation via bayesian networks suggest ways inference algorithm exploit additional structure encoded representation
constructive learning recurrent neural networks limitations recurrent correlation simple solution often difficult predict optimal neural network size particular application constructive methods add neurons layers connections etc might offer solution problem prove one method recurrent correlation due topology fundamental limitations representation thus learning capabilities represent monotone sigmoid activation functions certain finite state automata give preliminary approach get around limitations simple constructive training method adds neurons training still preserving powerful structure illustrate approach simulations learn many examples regular grammars
optimal weighting criterion case indexing numeric symbolic attributes indexing cases important topic memorybased one key problem assign weights attributes cases although several weighting methods proposed methods handle numeric attributes directly necessary numeric values classification furthermore existing methods theoretical background little optimality propose new weighting method based statistical technique called method handle numeric symbolic attributes framework generated attribute weights optimal sense maximize ratio variance classes variance cases experiments several benchmark tests show many cases method obtains higher weighting methods results also indicate distinguish relevant attributes irrelevant ones tolerate noisy data
general result stabilization linear systems using bounded controls abstract present two controllers globally linear systems subject control saturation allow essentially arbitrary saturation functions conditions system obvious necessary ones namely system positive real part standard condition hold one terms neuralnetwork type layer architecture one terms linear maps
classifying signals integrating ensembles neural networks paper proposes classification scheme based integration multiple ensembles anns demonstrated classification problem signals natural must distinguished signals artificial redundant classification environment consists several ensembles neural networks created trained bootstrap sample sets using various data representations architectures anns within ensembles bagging ensembles integrated signal adaptive manner using posterior confidence measure based agreement variance within ensembles proposed integrated classification machine achieved correct classifications test data cross validation evaluations comparisons indicate integration collection anns ensembles robust way handling high dimensional problems complex nonstationary signal space current classification problem
model selection generalized linear models via application first draft chapter bayesian adrian raftery professor statistics department statistics university washington seattle research supported onr contract raftery latter two part chapter written authors grateful excellent research david helpful discussions
using casebased reasoning acquire user scheduling preferences change time scheduling typically involves acquisition user optimization preferences problem space desired objectives make practical scheduling problems difficult costly solve especially problem configurations user optimization preferences change time paper incremental revision framework improving schedule quality incorporating user dynamically changing preferences casebased reasoning implemented system called records tradeoffs consequences result schedule revision guide schedule improvement preliminary experimental results show able effectively capture user static dynamic preferences known system exist implicitly manner case base
qualitative probability
behavior hierarchy autonomous mobile robots evolution realization autonomous behavior mobile robots using fuzzy logic control requires formulation rules responsible necessary levels intelligence collection rules decomposed efficiently implemented hierarchy article describes done using architecture behavior hierarchy mechanisms control decisionmaking described addition approach behavior coordination described emphasis evolution fuzzy coordination rules using genetic programming paradigm conventional applied evolve usefulness behavior hierarchy partial design performance results simulated autonomous navigation
unsupervised learning distributions binary vectors using two layer networks present distribution model binary vectors called influence combination model show model used basis unsupervised learning algorithms feature selection model closely related model defined first part paper analyze properties distribution representation scheme show arbitrary distributions binary vectors approximated combination model show weight vectors model interpreted high order correlation patterns among input bits compare combination model mixture model principle component analysis second part paper present two algorithms learning combination model examples first algorithm based gradient give closed form gradient significantly easier compute corresponding gradient general boltzmann machine second learning algorithm greedy method creates hidden units computes weights one time method variant projection pursuit density estimation third part paper give experimental results learning methods synthetic data natural data handwritten digit images
separating formal bounds practical performance learning systems
modeling distributed search via social complex group behavior arises social integration actions simple redundant individual furthermore act information center apply lessons natural systems model collective action memory computational agent collective action search combinatorial optimization problems collective memory improve learning multiagent systems collective adaptation integrates simplicity collective action pattern detection collective memory significantly improve processing knowledge test role information center examine ability task allocation without control
annealed theories learning study annealed theories learning boolean functions using concept class finite naive annealed theory used derive universal learning curve bound zero temperature learning similar inverse square root bound vapnikchervonenkis theory learning curve bounds also derived refined annealed theory leads still bounds cases similar results previously obtained using symmetry
evolution memory mental models using genetic programming build internal representations paper applies genetic programming successive actions results show evolution intelligent agents
numerical techniques efficient range searching near field using genetic algorithms article describes numerical method may used efficiently track targets range estimation case large approach used requirement priori knowledge source uses limited information array shape role sensor position uncertainty consequence targets always problems associated manipulation large matrices inherent conventional type algorithms simpler numerical approach presented reduces problem search optimization using method location target corresponds finding position maximum weighted sum output sensors since search procedure using modern stochastic optimization methods genetic algorithm requirement acceptable accuracy achieved real time usually array studied consists elements along flexible behind sensors giving effective long array far field assumption used algorithms longer appropriate targets considered rather plane shown simulated data significant noise
proceedings first international workshop intelligent adaptive systems constructive learning agents paper introduces new type intelligent agent called constructive learning agent agent differs adaptive agents ability learn assist user task also incrementally adapt knowledge representation space better fit given learning task agents ability make modifications originally given representation space due constructive induction learning method selective induction learning methods agents based methods rely good representation space good representation space misclassification noise attributes irrelevant attributes proposed methods problems agent domains poor representations learning agent will learn accurate rules useful learning agent paper gives architecture learning agent gives empirical comparison set six abstract domains involving disjunctive normal form descriptions
domain independent implementing classifier systems arbitrary environments
using decomposition get parallel selforganising map propose method computational complexity selforganising maps method uses partitioning neurons clusters teaching neurons occurs instead teaching network samples computational complexity decreases log furthermore introduce measure amount order selforganising map show introduced algorithm original algorithm
cooperation datadriven modelbased induction methods relational learning inductive learning relational domains shown intractable general many approaches task suggested nevertheless way restrict hypothesis space roughly two groups datadriven restriction encoded algorithm modelbased restrictions made less explicit form declarative bias paper describes inductive learner seeks combine aspects approaches initially datadriven using examples background knowledge put hypotheses based connectivity data hand hypotheses rule models used control decisions datadriven phase induction key words inductive learning relational domains cooperation datadriven methods implicit declarative bias
learning competition effects noise differences training model target environment problem learning decision rules sequential tasks addressed focusing problem learning plans simple simulator plane must avoid learning method relies notion competition employs genetic algorithms search space decision policies experiments presented address issues arising differences simulation model learning occurs target environment decision rules tested specifically either model target environment may contain noise experiments examine effect learning plans without noise testing plans noisy environment effect learning plans noisy simulator testing plans noisefree environment empirical results show best result obtained training model closely matches target environment using training environment noisy target environment better using using training environment less noise target environment
improving plans genetic algorithms
using genetic algorithm learn strategies avoidance local navigation navigation fields important capability autonomous vehicles one way produce robust behavior perform planning however realtime performance critical requirement navigation needed autonomous vehicle robust reactive rules perform wide variety situations also achieve realtime performance work learning system based genetic algorithms used learn reactive strategies navigation avoidance
rigorous learning curve bounds statistical mechanics paper introduce investigate mathematically rigorous theory learning curves based ideas statistical mechanics advantage theory vapnikchervonenkis theory bounds considerably many cases also true behavior functional form learning curves behavior often exhibit properties phase transitions power law explained theory theory application requires knowledge input distribution limited far finite function classes illustrate results many concrete examples learning curve bounds derived theory
using prior knowledge learn contextfree languages although considerable interest shown language inference automata induction using recurrent neural networks success models mostly limited regular languages previously demonstrated neural network automaton model capable learning deterministic contextfree languages languages examples however learning task computationally paper discuss ways priori knowledge task data used efficient learning also observe knowledge often experimental learning nontrivial languages
learning stochastic feedforward networks connectionist learning procedures presented sigmoid stochastic feedforward network networks class belief networks used expert systems represent probability distribution set variables using hidden variables express correlations conditional probability distributions exhibited stochastic simulation use tasks classification learning empirical data done via method analogous used boltzmann machines due feedforward nature connections negative phase boltzmann machine learning experimental results show result learning sigmoid feedforward network faster boltzmann machine networks advantages boltzmann machines pattern classification decision making applications provide link work connectionist learning work representation expert knowledge
generality versus size genetic programming genetic programming uses variable size representations programs size becomes important interesting emergent property structures evolved size programs controlling controlled factor search size influences efficiency search process related generality solutions paper analyzes size generality issues standard using addresses question whether analysis help control search process relate size generalization issues programs evolved control agent dynamic nondeterministic environment game
decisiontheoretic foundations causal reasoning present definition cause effect terms decisiontheoretic thereby provide principled foundation causal reasoning definition traditional view causation causal may vary set decisions available argue approach provides added notion cause also paper examine encoding causal relationships directed acyclic graphs describe special class influence diagrams canonical form show relationship representation cause effect finally show canonical form facilitates counterfactual reasoning
genetic programming fuzzy rulebased systems intelligent control fuzzy logic evolutionary computation proven tools handling realworld uncertainty designing control systems respectively approach presented combines attributes paradigms purpose developing intelligent control systems potential genetic programming paradigm learning rules use fuzzy logic controllers evaluated problem discovering controller mobile robot path tracking performance results incomplete compare favorably complete designed usual approach constrained syntactic representation supported genetic operators also introduced
interval survival data review recent progress review estimation interval models including nonparametric estimation distribution function estimation regression models nonparametric setting describe computational procedures asymptotic properties nonparametric maximum likelihood estimators regression setting focus proportional hazards proportional failure time regression models particular emphasis given calculation fisher information regression parameters also discuss computation regression parameter estimators via profile likelihood maximization likelihood results maximum likelihood estimators estimation asymptotic problems open questions also reviewed
balancing accuracy genetic programming genetic programming distinguished evolutionary algorithms uses tree representations variable size instead linear strings fixed length flexible representation scheme important allows underlying structure data discovered automatically one primary difficulty however solutions may grow without improvement generalization ability paper investigate fundamental relationship performance complexity evolved structures problem demonstrated empirically analyzing error landscapes programs evolved neural network synthesis consider genetic programming statistical inference problem apply bayesian framework introduce class fitness functions error complexity terms adaptive learning method presented automatically factor evolve programs without diversity population needed achieving desired training accuracy effectiveness approach empirically shown induction neural networks solving realworld medical diagnosis problem benchmark tasks
state reconstruction determining driven nonlinear systems
inference dynamic probabilistic networks dynamic probabilistic networks useful tool modeling complex stochastic processes simplest inference task monitoring computing posterior distribution state variables time step given observations time recursive algorithms wellknown monitoring models paper concerned computing posterior distribution given past future observations essential learning models data existing algorithms use space time total length observation sequence state space size time step therefore complex models long observation sequences paper presents log space log time algorithm demonstrates effectiveness algorithm two realworld learning problems also discuss possibility time algorithm
induction learning methods vary learned knowledge implicit hypothesis testing conclusions experimental evidence however paper demonstrates utility derived rules may bias learning systems examine naive naive context decision tree learner rules based weak tests significance experimental results indicate cases particularly cases sparse training data high noise work generalizes earlier findings fisher discuss relevance unsupervised learning small disjuncts issues
hierarchical recurrent neural networks longterm dependencies already shown extracting longterm dependencies sequential data difficult deterministic dynamical systems recurrent networks probabilistic models hidden markov models hmms inputoutput hidden markov models practice avoid problem researchers used domain specific knowledge give meaning hidden state variables representing past context paper propose use general type knowledge namely temporal dependencies structured implies longterm dependencies represented variables long time scale principle applied recurrent network includes multiple time scales experiments confirm advantages structures similar approach proposed hmms
flat minima neural computation present new algorithm finding low complexity neural networks high generalization capability algorithm searches flat minimum error function flat minimum large connected region error remains approximately constant bayesian argument suggests flat minima correspond simple networks low expected overfitting argument based gibbs algorithm variant novel way splitting generalization error overfitting error unlike many previous approaches require gaussian assumptions depend good weight prior instead prior inputoutput functions thus taking account net architecture training set although algorithm requires computation second order derivatives order complexity automatically effectively units weights input various experiments feedforward recurrent nets described application stock market prediction flat minimum search outperforms conventional weight decay optimal brain optimal brain damage also provide code algorithm
topics neural networks control
reactive plans paper describes method improving comprehensibility accuracy generality reactive plans reactive plan set reactive rules method involves two phases formulate explanations execution traces generate new reactive rules explanations since explanation phase previously described primary focus paper rule generation phase latter phase consists taking subset explanations using explanations generate set new reactive rules add original set particular subset explanations chosen yields rules provide new domain knowledge handling knowledge gaps original rule set original rule set manner provides expertise gaps domain knowledge provided new rules incomplete
evolutionary neural networks value ordering constraint satisfaction problems technical report may abstract new method developing good strategies constraint satisfaction search presented using evolutionary technique called individual neurons evolve form neural network knowledge discovered results better decisions based heuristics neural network evolved search decide ordering assembly line network required random ordering maximization future options heuristic approach extend domains heuristic information either difficult discover
refining conversational case libraries conversational casebased reasoning cbr inferences cbr express successful tools supporting development help related applications contrast rulebased expert systems capture knowledge cases rather rules incrementally extended however rather eliminate knowledge engineering case engineering task carefully cases according library design guidelines ensure good performance designing complex libraries according guidelines difficult software needed assist users case describe approach revising case libraries according design guidelines implementation empirical results showing conditions approach improve conversational cbr performance
computational account movement learning impact tradeoff present computational model movement learning types skills addressed class trajectory following movements involving multiple changes direction skills acquired observation improved practice also review robust phenomena human motor behavior present two tradeoff experiments models performance human behavior quite
combining symbolic connectionist learning methods refine
improving accuracy combining rulebased casebased reasoning
density estimation technical report information computer science department university california
comparison qlearning classifier systems reinforcement learning class problems autonomous agent given environment improves behavior maximizing function calculated just basis responses received environment qlearning classifier systems two methods among used solve reinforcement learning problems shared goal past often considered two different models paper first show classifier system restricted called discounted max simple classifier system max qlearning follows max converges optimal policy proved results experimental theoretical works improve qlearning facilitate use concrete applications second part paper show three restrictions need deriving equivalence qlearning internal states care symbols structural changes turn essential recently qlearning similarities among ongoing work within research contexts main contribution paper therefore make explicit strong similarities existing qlearning classifier systems show experience gained research within one domain useful direct future research one
finding compact sparse distributed representations visual images recent work investigated compact coding using dimensionality reduction sparse distributed coding context understanding biological information processing introduce artificial neural network self basis simple hebbian learning negative feedback activation show capable compact data distributions also identifying filters sensitive sparse distributed codes network extremely simple biological relevance investigated via response set images typical life however analysis networks identification filter sparse coding reveals coding may globally optimal exists limiting factor
sets points general position requires parameters classes concepts defined certain classes analytic functions depending parameters open sets samples length result also proved functions special case neural networks discussed
systematic evaluation design decisions cbr systems two important goals evaluation theory model assess design decisions performance implemented computer system analyze impact performance system faces problem domains different characteristics particularly difficult casebased reasoning systems systems typically complex tasks domains operate present methodology evaluation casebased reasoning systems systematic empirical experimentation range system configurations environmental conditions coupled rigorous statistical analysis results experiments methodology enables understand behavior system terms theory design computational model select best system configuration given domain predict system will behave response changing domain problem characteristics case study multistrategy casebased reinforcement learning system performs autonomous robotic navigation presented example
adapting abstract knowledge casebased reasoner use knowledge must powerful case casebased reasoner cope variation form problems given extent cases memory efficiently adapted fit wide range new situations paper address task adapting abstract knowledge planning fit specific planning situations first show adapting abstract cases requires representations planning situations next describe representation system memory organization adaptation process requirement approach implemented planner takes abstract advice
efficient estimation model interval maximum likelihood estimator proportional hazards model current data studied shown regression parameter asymptotically normal rate achieves information bound even though baseline function converges rate estimation asymptotic variance matrix regression parameter also considered prove main results also establish general theorem showing finite dimensional parameter class models asymptotically efficient even though infinite dimensional parameter converges rate slower results illustrated applying data set study introduction many survival analysis problems interested
role stories box new technical report abstract people often give advice stories stories course action general conditions appropriate computational model advice taking using stories must address two related problems determining conditions showing obtain new situation paper present efficient solution second problem based results first proposal implemented planner takes abstract advice
evolving team
issues mixture modelling mcmc algorithms increasing need efficient estimation mixture distributions especially following use modelling tools many applied fields propose paper bayesian approach estimation normal mixtures relies secondary components mixture terms main component providing intuitively representation modelling stage important prior distribution performance mcmc algorithms compare two possible extending show link secondary components together associated poor convergence properties mcmc algorithms
common server web server implemented common order facilitate exploratory programming global domain provide access complex research programs particularly artificial intelligence systems server initially used provide document retrieval email advanced applications include systems inductive rule learning question research seeks fully generalize automatic techniques developed email operate web conclusions argue sophisticated form processing order reduce load provide advanced interaction models users
model uncertainty survival analysis improves predictive performance survival analysis concerned finding models predict survival patients assess treatment key part process selection predictor variables standard use stepwise procedure guided series significance tests select single model make inference selected model however model uncertainty substantial review standard bayesian model averaging solution problem extend survival analysis introducing partial bayes factors proportional hazards model two examples taking account model uncertainty predictive performance extent useful
method model averaging selection propose method model averaging selection focuses training points individual bootstrap samples information used estimate optimal weighting factors combining estimates different bootstrap samples also finding best subsets linear model setting provide alternatives bayesian approaches model averaging selection requiring less computation fewer choices
prediction games arcing algorithms technical report december statistics department university california berkeley abstract theory behind success adaptive combining algorithms arcing adaboost freund schapire others reducing generalization error understood prediction classification regression game one player makes selection instances training set convex linear combination predictors finite set existing arcing algorithms shown algorithms finding good game strategies optimal game strategy finds combined predictor minimizes maximum error training set bound generalization error combined predictors terms maximum error proven bounds date arcing algorithms described converge optimal strategy schapire explanation adaboost works terms ability reduce margin comparing adaboost optimal arcing algorithm shows explanation valid answer lies situation bounds empirical results given explore situation
analogical problem solving always analogical case second draft analogical problem technical report
modelbased approach supporting dialogue conversational casebased reasoner conversational casebased reasoning ccbr form interactive casebased reasoning users input partial problem description text ccbr system solution display lists solutions stored cases whose problem descriptions best match users question display lists questions cases users interact either refining problem description selected questions selecting solution apply ccbr systems support dialogue infer answers questions implied problem description otherwise questions will user already standard approach dialogue allows case library designers rules define implications problem description questions however approach substantial knowledge engineering requirements introduce alternative approach intelligent assistant defining model case library implication rules derived detail approach benefits explain supported integration fast relational database system will evaluate approach context ccbr system named paper appeared symposium multimodal reasoning introduce integrated reasoning approach modelbased reasoning component performs important role conversational casebased reasoning ccbr system named figure ccbr form casebased reasoning users text queries describing problem system cases three components
learning conjunctions horn clauses
learning readonce formulas queries readonce formula boolean formula variable occurs formulas also called formulas boolean trees paper problem exactly identifying unknown readonce formula using specific kinds queries main results polynomial time algorithm exact identification monotone readonce formulas using membership queries polynomial time algorithm exact identification general readonce formulas using equivalence membership queries based notion adequate teacher results improve previous results readonce formulas also show polynomial time algorithm using membership queries equivalence queries exactly identify readonce formulas
distributed patterns hierarchical structures recursive memory structures show promise general representation vehicle uses distributed patterns however training often difficult explains least part relatively small networks studied show technique collection hierarchical structures set training patterns sequential effectively trained using simple recurrent network produces set distributed patterns corresponding structures
learning probabilistic automata variable memory length propose analyze distribution learning algorithm variable memory length markov processes processes described subclass probabilistic finite automata probabilistic finite automata learning algorithm motivated real applications interaction speech recognition used fixed memory markov hidden markov models either practical theoretical drawbacks though general results known learning distributions generated sources similar structure prove algorithm indeed efficiently learn distributions generated restricted sources particular show distribution generated target source distribution generated hypothesis made small high confidence polynomial time sample complexity demonstrate applicability algorithm learning structure natural english text using correction corrupted text
applications logical discovery engine clausal discovery engine presented discovers regularities data representative inductive logic programming paradigm represents data regularities means first order clausal theories search space clausal theories larger attribute value representation also input declarative specification language bias determines set regularities whereas papers semantics logical problem specification discovery algorithm paclearning aspects paper illustrate power resulting technique order achieve aim show used learn integrity constraints databases functional dependencies determinations properties sequences mixed quantitative qualitative laws engineering classification rules
induction decision trees using relieff context machine learning examples paper deals problem estimating quality attributes without dependencies greedy search current inductive machine learning algorithms detect significant dependencies attributes recently developed relief algorithm estimating quality attributes able detect dependencies attributes show strong relation estimates functions usually used heuristic guidance inductive learning algorithms propose use relieff extended version relief instead functions assistant system induction decision trees using relieff estimator attributes selection step algorithm tested several artificial several real world problems results show advantage presented approach inductive learning open wide possibilities using relieff
investigation dynamics genetic programming applied chaotic time series prediction reported interesting characteristic adaptive search techniques ability perform many problem domains others genetic flexible tree structure particular problem represented forms representations effects search performance therefore aspect fundamental engineering significance find representation upon genetic programming operators optimizes search performance discover case chaotic time series prediction representation commonly used domain yield optimal solutions instead find population converges onto one accurately tree trees explored correct convergence make simple modification crossover operator paper review previous work time series prediction result related report improvement modified crossover operator
linear space induction first order logic relieff current ilp algorithms typically use variants extensions greedy search detect significant relationships training objects instead functions propose use heuristic based relief guidance ilp algorithms step system heuristic used determine candidate literals used exhaustive search potentially good conjunction literals efficiency point view introduce interesting declarative bias enables keep training set introducing new variables within linear bounds linear respect clause length bias variables variable dependency tree resulting system tested various artificial problems advantages approach discussed
relieff estimation discretization attributes classification regression ilp problems instead functions propose use relieff heuristic guidance inductive learning algorithms basic relief developed able efficiently solve classification problems involving highly dependent attributes parity problems however sensitive noise deal incomplete data multiclass regression problems continuous class extended relief several directions extended algorithm relieff able deal noisy incomplete data used multiclass problems variant deal regression problems another area application inductive logic programming ilp instead measures relieff used estimate utility literals theory construction
combining temporal difference learning search paper present variation algorithm enables used conjunction minimax search present experiments chess demonstrate utility provide comparisons another less variant particular chess program used learn evaluation function playing free chess server improved just games discuss reasons success relationship results results
sequential metropolishastings algorithm paper deals asymptotic properties metropolishastings algorithm distribution interest unknown approximated sequential estimator density prove simple conditions rate convergence metropolishastings algorithm sequential estimator latter introduced reversible measure metropolishastings kernel problem natural extension previous work new simulated annealing algorithm sequential estimator energy
viewpoint invariant face recognition using independent component analysis attractor networks explored two approaches recognizing faces across changes pose first developed representation face images based independent component analysis ica compared principal component analysis pca representation face recognition ica basis vectors data set spatially local pca basis vectors ica representation greater changes pose second present model development viewpoint invariant responses faces visual experience biological system temporal natural visual experience incorporated attractor network model hebbian learning following temporal filter unit activities combined temporal filter basic hebbian update rule generalization temporally input patterns system acquired faces largely independent pose
bayesian curve fitting using multivariate normal mixtures problems regression smoothing curve fitting addressed via predictive inference flexible class mixture models multidimensional density estimation using dirichlet mixture models provides theoretical basis regression methods regression functions may means conditional predictive distributions bayesian regression functions features similar kernel regression estimates formal analysis addresses problems multivariate smoothing parameter estimation assessment uncertainties regression functions naturally computations based multidimensional versions existing markov chain simulation analysis univariate dirichlet mixture models
formal analysis role crossover genetic algorithms basis early theoretical empirical studies genetic algorithms typically used point crossover operators standard mechanisms implementing recombination however number recent studies primarily empirical nature shown benefits crossover operators involving higher number crossover points traditional theoretical point view surprising new results relate uniform crossover involves average crossover points strings length paper extend existing theoretical results attempt provide predictive theory role crossover genetic algorithms particular extend traditional analysis include two general forms crossover crossover uniform crossover also analyze two aspects crossover operators namely recombination potential exploratory power results analysis provide much view role crossover genetic algorithms implications results implementation issues performance discussed several directions research suggested
using neural networks statistical analysis data paper discuss methodological issues using class neural networks called mixture density networks discriminant analysis models advantage rigorous probabilistic interpretation proven alternative classification procedure discrete domains will address classification aspects discriminant analysis compare approach traditional method linear implemented standard statistical show approach performs aspects many observations made restricted particular case hand applicable applications discriminant analysis research
simulated annealing hard satisfiability problems satisfiability refers task finding assignment makes arbitrary boolean expression true paper compares simulated annealing algorithm greedy algorithm solving satisfiability problems solve problem instances extremely difficult traditional satisfiability algorithms results suggest scales better number variables increases solving least many hard problems less effort paper presents study helps explain relative advantage next improvement basic algorithm examined based random walk implemented finally examine performance test suite satisfiability problems produced dimacs challenge
support management automated reasoning technology service instancebased learning algorithms machine
discretization continuous features present comparison methods discretization continuous features study includes extensive empirical comparison analysis scenarios error minimization may inappropriate discretization criterion present discretization method based c45 decision tree algorithm compare existing discretization algorithm employs minimum description length principle recently proposed technique evaluate discretization methods respect c45 classifiers datasets repository analyze computational complexity method results indicate mdl heuristic outperforms error minimization average analyze shortcomings approaches comparison methods
system observable inputoutput behavior show similar construction systems modifications let discretetime system state space outputs perform change pair observable pair minimal sense system inputoutput behavior dimension least observable hence canonical let find another system necessarily inputoutput behavior canonical let relative degree markov sequence let initial state difference case smallest relative degree greater equal case roughly outputs system give information first outputs use inputs outputs learn first components may able use controls learn last components time finally two states proof case equations first output terms exactly terms satisfied first output terms input first components equivalent first output terms since output initial state example either case may use control identify using
applying second equation conclude get obtain see side bounded since system now since using obtain ffi note side trivial since know ffi ffi however see still holds established cases get taking side ffi ffi get ffi ffi take follows complete proof need deal general case inputs done induction proof will large stability control systems linear controllers control new design constrained controllers linear systems ieee transactions design linear systems linear control bounded states ieee control design tracking systems subject saturation control controllability linear systems constrained controls control feedback stabilization linear control system space control signals systems applied nonlinear control algebraic approach bounded controllability linear systems control stabilization stability proc ieee ieee mathematical control theory deterministic finite dimensional systems new york nonlinear output feedback design linear systems controls proc ieee ieee multiple means bounded feedback controls proc ieee ieee global stabilization restricted tracking multiple bounded controls systems control stabilization linear systems bounded controls proc global stabilization linear systems bounded feedback thesis department university
data error detection dynamic systems error detection plays role parameter estimation data dynamic steady state systems particular recent advances process optimization now allow data dynamic systems appropriate problem need considered data errors due either sensors just random events underlying statistical distribution induce biases parameter estimates data paper robust estimators exploratory statistical methods allow detect errors data performed robust methods property statistical distributions therefore presence regression done detected using exploratory statistical techniques important feature performance optimization algorithm data ability classify variables according properties observable variable estimated measured variables physical model variable measured variable estimated measurements variable classification used aid design schemes
learning hierarchical classifications many significant realworld classification tasks involve large number categories hierarchical structure example classifying documents subject categories library scheme classifying documents topic hierarchies investigate potential benefits using given hierarchy base classes learn accurate classifiers domains first consider possibility exploiting class hierarchy prior knowledge help one learn accurate classifier explore benefits learning hard topdown fashion compare soft approach training data among categories hierarchies potential improve prediction accuracy argue reasons sometimes improvement using hierarchy hypothesis class appropriate manner however various controlled experiments show cases performance advantage associated using hierarchy seem due prior knowledge
machine learning predicting nearly best pruning decision tree many algorithms decision tree data involve process first large decision tree typically overfitting data reduce overfitting second phase tree using one number available methods final tree output used classification test data paper suggest alternative approach pruning phase using given decision tree present new method making predictions test data prove algorithms performance will much worse precise technical sense predictions made best reasonably small pruning given decision tree thus procedure guaranteed competitive terms quality predictions pruning algorithm prove procedure efficient highly robust method viewed synthesis two previously studied techniques first apply results predicting using expert advice view pruning expert obtain algorithm provably low prediction loss computationally infeasible next generalize apply method developed derive efficient implementation procedure
noisetolerant learning general geometric concepts research laboratory abstract present efficient algorithm paclearning general class geometric concepts fixed specifically let set let arbitrary point associate boolean function concept class study consists concepts formed boolean function set concept class study much general geometric concept class known special cases main result obtain learning algorithms several new geometric concept classes consider geometric concepts defined necessarily convex defined faces formed combining thus algorithm also learn boolean combination polynomial number polynomial number faces even results easily extended efficiently learn boolean combination polynomial number concepts selected concept class given polynomial thus constant constant polynomial time algorithm determine concept consistent given set examples also present statistical query version algorithm tolerate random classification noise noise rate less finally present generalization standard net result apply give alternative noisetolerant algorithm
decision tree pruning based tree size work develop new criteria perform decision tree pruning method theoretically sound based theoretical concepts uniform convergence vapnikchervonenkis dimension show criteria motivated theory side performs practice accuracy new criteria comparable current method used c45
neural networks function determines form
integration functions peak
solving combinatorial problems using evolutionary algorithms
protein secondary structure modelling probabilistic networks extended abstract paper study performance probabilistic networks context protein sequence analysis molecular biology specifically report results initial experiments applying framework problem protein secondary structure prediction one main advantages probabilistic approach describe ability perform detailed experiments experiment different models easily perform local measure effect global structure methods support experimentation method efficient training prediction important order able perform many experiments different networks believe probabilistic methods comparable methods prediction quality addition predictions generated methods precise quantitative semantics shared classification methods specifically causal statistical independence assumptions made explicit networks thereby allowing study experiment different causal models manner
algorithmic stability bounds leaveoneout crossvalidation paper prove bounds error leaveoneout crossvalidation estimate generalization error bounds showing worstcase error estimate much worse training error estimate refers fact although often expect leaveoneout estimate perform considerably better training error estimate performance will considerably worse perhaps surprisingly given limited cases prior literature crossvalidation nontrivial bound error leaveoneout must rely notion algorithmic stability previous bounds rather strong notion hypothesis stability whose application primarily limited local algorithms introduce new notion error stability apply obtain bounds leaveoneout classes learning algorithms including training error minimization procedures bayesian algorithms also provide lower bounds demonstrating form error stability proving bounds error leaveoneout estimate fact training error minimization algorithms worst case bounds must still depend vapnikchervonenkis dimension hypothesis class
observation generalisation simulated robot world paper describes program behaviour simulated world uses observations experiments experiment sequence actions carried order support case generalisation concept generalisation program state world similar previous state partial matching algorithm used find enable two states unified generalisation two states
distributed genetic programming system genetic programming techniques problem solving tool depends good underlying software structure system presented conceptual system consisting collection software components interface definitions roles system flexibility effort applied new problem domain
empirical investigation multiparent recombination operators evolution strategies
adaptive behavior competing species coevolution competitive species provides interesting study role adaptive behavior provides dynamic environments paper experimentally investigate arguments coevolution different adaptive behaviors competing species species implemented simulated mobile robots sensors additional vision module whereas maximum speed set different types variability life architecture genetic length compared shown simple forms affect coevolutionary dynamics rather exploit noisy controllers generate random trajectories whereas benefit controllers improve pursuit behavior
recurrent neural networks report abstract obtain characterization class nonlinear systems appear neural networks research
brief papers computing second derivatives feedforward networks review calculation second derivatives required recent training analysis techniques connectionist networks elimination weights estimation confidence intervals weights network outputs review develop exact approximate algorithms calculating second derivatives networks weights simply full matrix second derivatives requires operations networks radial basis units sigmoid units exact calculation necessary intermediate terms requires order number hidden units network also review compare three approximations components second numerical scoring algorithms apply arbitrary activation functions networks error functions instance connections layers radial basis functions error units etc
functional programming analogy paper describe principles problem solving analogy applied domain functional program synthesis reason programs structures discuss two different methods handle structures graph metric determining distance two program schemes structure mapping engine existing system examine analogical processing furthermore show experimental results discuss
learning examples heuristic switching
potential prototype styles generalization many ways learning system generalize training set data paper presents several generalization styles using prototypes attempt provide accurate generalization training set data wide variety applications generalization styles efficient terms time space massively parallel architectures empirical results generalizing several realworld applications given results indicate prototype styles generalization presented potential provide accurate generalization many applications
recurrent neural networks aspects paper provides recent research regarding aspects recurrent dynamic neural networks sigmoidal activation functions class systems introduced discussed result regarding universal approximation properties known controllability parameter reviewed result facts regarding computational power recurrent nets also supported part air force grant
complete controllability recurrent neural networks paper presents characterization controllability class control systems commonly called recurrent neural networks characterization involves simple condition input matrix proved activation function
word perfect transformation asocs adaptive algorithm artificial neural networks anns fixed topology learning often suffer number shortcomings result anns use dynamic topologies shown ability overcome many problems adaptive self concurrent systems asocs class learning models inherently dynamic topologies paper introduces transformations general strategy implementing learning models use dynamic topologies efficiently parallel hardware creates set nodes node computes part network output independent nodes using local information type transformation allows efficient support adding nodes dynamically learning particular paper presents location independent asocs model asocs adaptive algorithm description gives formal definitions algorithms implements basic asocs mechanisms definitions provide formal description basic asocs mechanisms general addition
solutions bellman equation reinforcement learning algorithms often work finding functions satisfy bellman equation yields optimal solution prediction markov chains controlling markov decision process mdp finite number states actions approach also frequently applied markov chains mdps infinite states show case bellman equation may multiple solutions many lead predictions policies algorithms conditions presented guarantee single optimal solution bellman equation
modelbased learning structural indices design cases major issue retrieving appropriate cases memory solve given problem implies case appropriately stored memory casebased system dynamic cases reuse needs learn indices new knowledge system designers knowledge type indexing structural functional hierarchical organization case memory two distinct related issues index learning learning indexing learning right level generalization paper show sbf models help learning structural indices design cases domain physical devices sbf model design provides functional causal explanation structure design function describe sbf model design provides structural indexing design cases inductive biases index generalization discuss modelbased learning integrated learning uses prior design cases learning level index generalization
modelbased approach analogical reasoning learning design
learning classify observed motor behavior present representational observed movements representation temporal structure relating components single complex movement also present unsupervised learning system constructs classes movements empirical results indicate system builds abstract movement concepts appropriate component structure allowing predict latter portions partially observed movement
machine learning inference constructive induction problem learning inductive hypothesis two searches best representation space best hypothesis space datadriven constructive induction learning system searches better representation space analyzing input examples data presented datadriven constructive induction method combines learning algorithm two classes representation space improvement operators implemented system experimentally applied prediction problem using world database results show decision rules learned outperformed rules learned original representation space predictive accuracy rule simplicity
extracting support data given task report novel possibility extracting small subset data base contains information necessary solve given classification task using support vector algorithm train three different types handwritten digit classifiers observed types classifiers construct decision surface strongly overlapping small subsets data base finding possibility data bases significantly data important solution given task addition show theory allows predict classifier will best generalization ability based solely performance training set characteristics learning machines finding important cases amount available data limited
neuronal population activity reconstruction unified framework application hippocampal place cells physical variables orientation line visual field location body space coded activity levels populations neurons reconstruction inverse problem physical variables estimated observed neural activity reconstruction useful first much information physical variables present population second providing insight brain might use distributed representations solving related computational problems visual object recognition spatial navigation two classes reconstruction methods namely probabilistic bayesian methods basis function methods discussed include important existing methods special cases population vector coding optimal linear estimation matching representative example reconstruction problem different methods applied train data hippocampal place cells moving reconstruction accuracy trajectories compared different methods bayesian methods especially accurate constraint best errors within factor two informationtheoretic limit accurate reconstruction comparable intrinsic experimental errors position tracking addition reconstruction analysis interesting aspects place cell activity trajectory animal running general theoretical values minimal reconstruction errors quantify accurately physical variable encoded neuronal population sense mean square error regardless method used reading information one related result theoretical accuracy independent width gaussian tuning function two dimensions finally reconstruction methods considered paper implemented unified neural network architecture brain use solve related problems
representation spatial orientation intrinsic dynamics cell ensemble theory cells found system moving represent direction animal plane regardless location animal internal direction represented cells uses information based updating familiar visual model dynamics cell ensemble presented stability localized static activity profile network dynamic shift mechanism explained naturally synaptic weight distribution components even symmetry respectively symmetric weights symmetric connections stable activity profile close known tuning curves will emerge adding weights activity profile will shift continuously without shape shift speed accurately controlled strength component generic formulation shift mechanism determined within current theoretical framework attractor dynamics system internal representation facilitates correction error model offers specific onedimensional example computational mechanism representation derived sensory inputs integrating information
bias plus variance decomposition loss functions present decomposition expected misclassification rate commonly used loss function supervised classification learning decomposition quadratic loss functions known important tool analyzing learning algorithms yet decomposition commonly used misclassification loss functions recent work decomposition major shortcomings though potentially negative variance decomposition avoids show practice naive estimation decomposition terms biased show correct bias illustrate decomposition various algorithms datasets repository
rapidly fitness evaluation genetic programming convergent design component computational solving trivial evolutionary algorithms task measuring fitness individual generation evolving population idea evolvable hardware individual evolving population hardware purpose fitness evaluation task paper demonstrates parallelism rapidly exploited accelerate computationally fitness evaluation task genetic programming work done virtual computing expansion board type computers step evolved two fewer steps sorting network described sorting networks number steps minimal subsequent
least norm approximation theoretically fast finite successive linear approximation algorithm proposed obtaining solution corrupted linear system due noise error measurement proposed algorithm finds solution minimizing number elements error numerical tests example indicate proposed method comparable method minimizes norm solution error methods superior orders magnitude solutions obtained least choosing optimal solution specific number elements
learning viewpoint invariant face representations visual experience temporal association natural visual experience different views object face tend appear close temporal set simulations presented demonstrate viewpoint invariant representations faces developed visual experience temporal relationships among input patterns simulations explored interaction temporal smoothing activity signals hebbian learning feedforward system recurrent system recurrent system generalization network temporal filter unit activities following training sequences images faces changed pose multiple views given face system acquired representations faces approximately viewpoint invariant
submitted future generation computer systems special issue data mining using neural networks neural networks successfully applied wide range supervised unsupervised learning applications neuralnetwork methods commonly used tasks however often produce models require long training times article describe neuralnetwork learning algorithms able produce comprehensible models require training times specifically discuss two classes approaches data mining neural networks first type approach often called rule extraction involves extracting symbolic models trained neural networks second approach directly learn simple networks argue given current state neuralnetwork methods place tool boxes
statistical theory crossvalidation asymptotically effective statistical theory proposed analysis stochastic neural networks trained loss asymptotic case shown asymptotic gain generalization error small perform early stopping even access optimal stopping time considering crossvalidation stopping answer question ratio examples training testing sets order obtain optimum performance region early stopping always decreases generalization error large scale simulations done agreement analytical findings
mathematical programming data mining mathematical programming approaches three fundamental problems will described feature selection clustering robust representation feature selection problem considered two sets recognizing irrelevant redundant features creates model often generalizes better new unseen data computational results real data confirm improved generalization models clustering unsupervised learning patterns clusters may exist given database useful tool knowledge discovery databases mathematical programming formulation problem proposed theoretically computationally finite number steps resulting algorithm utilized discover useful survival curves breast cancer patients medical database robust representation concerned minimizing trained model applied new problems novel approach proposed small error training process order avoid overfitting data may contain errors examples applications concepts given
overview genetic algorithms part research topics
stochastic search inductive concept learning concept learning viewed search space concept descriptions hypothesis language determines search space standard inductive learning algorithms structure search space determined operators algorithms perform locally optimal search using hillclimbing andor strategy overcome limitation concept learning viewed stochastic search space concept descriptions proposed stochastic search method based simulated annealing known successful means solving combinatorial optimization problems stochastic search method implemented rule learning system based compact efficient representation problem appropriate operators search space furthermore heuristic pruning search space method enables also handling imperfect data paper introduces stochastic search method describes learning algorithm gives results experiments
exponentially many local minima single neurons show single neuron logistic function transfer function number local minima error function based square loss grow exponentially dimension
analysis effects neighborhood size shape local selection algorithms increasing parallel architectures variety evolutionary algorithms population spatially distributed local selection algorithms operate parallel small overlapping effects design choices regarding particular type local selection algorithm size shape neighborhood particularly understood generally tested empirically paper extend techniques used formally analyze selection methods sequential apply local neighborhood models resulting much understanding effects neighborhood size shape
incremental tradeoff resolution qualitative probabilistic networks qualitative probabilistic reasoning bayesian network often reveals tradeoffs relationships due competing qualitative influences present two techniques combine qualitative numeric probabilistic reasoning tradeoffs qualitative relationship nodes bayesian network first approach incrementally nodes contribute qualitative relationships second approach evaluates approximate bayesian networks bounds probability distributions uses bounds qualitative relationships question approach also incremental algorithm refines state spaces random variables bounds qualitative relationships approaches provide systematic methods tradeoff resolution potentially lower computational cost application purely numeric methods
survey parallel genetic algorithms report may
gaussian distribution simple powerful modification standard gaussian distribution studied variables gaussian constrained use energy functions two multimodal examples competitive cooperative distributions illustrate representational power gaussian since cooperative distribution represent pattern demonstrates potential gaussian modeling pattern
fast algorithm independent component analysis paper will appear neural computation abstract introduce novel fast algorithm independent component analysis used blind source separation feature extraction shown neural network learning rule transformed iteration provides algorithm simple depend parameters fast converge accurate solution allowed data algorithm one time independent components regardless probability distributions computations performed either batch mode manner convergence algorithm proven convergence speed shown comparisons gradient based algorithms made showing new algorithm usually times faster sometimes giving solution just iterations
neuronal goals efficient coding detection work minimal entropy codes unsupervised learning particular need probability events put practical neuronal framework detecting events variant bcm learning rule presented together mathematical results suggesting optimal minimal entropy coding
extended selection mechanisms genetic algorithms
genetic algorithm tutorial technical report revised november
machine learning inference constructive induction problem learning inductive hypothesis two searches best representation space best hypothesis space datadriven constructive induction learning system searches better representation space analyzing input examples data presented datadriven constructive induction method combines learning algorithm two classes representation space improvement operators implemented system experimentally applied prediction problem using world database results show decision rules learned outperformed rules learned original representation space predictive accuracy rule simplicity
nonlinear pca learning rule signal separation mathematical analysis
adaptation relief attribute estimation regression heuristic measures estimating quality attributes mostly assume independence attributes domains strong dependencies attributes performance poor relief extension relieff capable correctly estimating quality attributes classification problems strong dependencies attributes exploiting local information provided different contexts provide global view present analysis relieff lead adaptation regression continuous class problems experiments artificial realworld data sets show relieff correctly estimates quality attributes various conditions used learning regression trees relieff relieff provide unified view estimating attribute quality regression classification
inductive logic programming new research area inductive logic programming various positive characteristics subjects logic programming machine learning new area will overcome many limitations background present developments within area discussed various goals increasing body researchers identified inductive logic programming needs based sound principles logic statistics side statistical hypotheses discuss possible relationship algorithmic complexity theory pac learning terms logic provide unifying framework inverse resolution relative least general generalisation terms leads discussion feasibility extending framework allow invention new predicates previously discussed within context
bayesian learning forward neural networks bayesian methods applicable complex modeling tasks review principles bayesian inference presented applied neural network models several approximate implementations discussed advantages conventional model training selection argued bayesian methods traditional approaches although empirical evidence still sparse
learning belief networks data information theory based approach paper presents efficient algorithm learning bayesian belief networks databases algorithm takes database input constructs belief network structure output construction process based computation mutual information attribute pairs given data set large enough algorithm generate belief network close underlying model time time data set normal see section probability distribution algorithm guarantees structure perfect map pearl underlying dependency model generated evaluate algorithm present experimental results three versions wellknown network database attributes records results show algorithm accurate efficient proof analysis complexity conditional independence tests
search space analysis job shop scheduling problem computational study job shop scheduling problem presented thereby emphasis put structure solution space appears adaptive search statistical analysis search spaces reveals inherent properties problem adaptive heuristics
algorithm bayesian belief network construction data paper presents efficient algorithm constructing bayesian belief networks databases algorithm takes database attributes ordering causal attributes attribute appear earlier order input constructs belief network structure output construction process based computation mutual information attribute pairs given data set large enough probability distribution algorithm guarantees perfect map underlying dependency tests evaluate algorithm present experimental results three versions wellknown network database attributes records proof analysis computational complexity also presented also discuss features work relate previous works model generated time time complexity conditional independence
nonlinear prediction chaotic time series using support vector machines novel method regression recently proposed technique called support vector machine mathematical point view seems provide new insight function approximation implemented tested data base chaotic time series used compare performances different approximation techniques including polynomial rational approximation local polynomial techniques radial basis functions neural networks performs better approaches presented also study particular time series variability performance respect free parameters
module implementation neural network requirement dense artificial neural network systems led researchers paper reports implementation using modules specific system described selforganizing parallel dynamic learning model requires dense technology effective implementation requirement exploiting technology ideas presented paper regarding implementation artificial neural networks adapted apply neural network connectionist models
specialization recursive predicates specializing recursive predicate order set negative examples without set positive examples may possible remove clauses negative example without positive previously proposed solution problem apply program transformation order obtain target predicates recursive ones however application method recursive found work present algorithm limited specializing predicates key idea upon algorithm based enough remove clauses negative examples order obtain correct sometimes necessary clauses appear positive examples contrast new algorithm limited specializing clauses defining one predicate may clauses defining multiple predicates furthermore positive negative examples longer required instances predicate proven algorithm produces correct specialization positive examples logical consequences original program finite number derivations positive negative examples positive negative examples sequence input clauses
specialization logic programs pruning program positive negative examples viewed problem pruning negative examples positive examples shown actual pruning performed applying clause algorithm presented based idea input algorithm logic program positive negative examples computation rule determines shape shown generality resulting specialization dependent computation rule experimental results presented using three different computation rules experiments indicate computation rule formulated number applications low possible algorithm uses divideandconquer method also compared covering algorithm experiments show higher predictive accuracy achieved focus positive negative examples rather achieving high coverage positive examples
inference cognitive maps cognitive mapping qualitative decision modeling technique developed years see use social science applications paper show cognitive maps viewed context recent qualitative decision modeling latter provide semantic foundation facilitate development powerful inference procedures extensions models sort
continuous casebased reasoning casebased reasoning systems traditionally used perform highlevel reasoning problem domains described using discrete symbolic representations however many realworld problem domains autonomous robotic navigation better characterized using continuous representations problem domains also require continuous performance continuous interaction environment continuous adaptation learning performance task introduce new method continuous casebased reasoning discuss applied dynamic selection modification acquisition robot behaviors autonomous navigation systems conclude general discussion casebased reasoning issues addressed work
reports machine learning inference face challenge challenge title second international competition machine learning programs organized fall david university goal competition solve problems discover simplest classification rules structured objects rule complexity prolog program number various components rule expressed prolog horn clauses several submitted competition teams generated three members family learning programs induce paper analyses results obtained programs compares obtained learning programs also presents ideas research inspired competition one ideas challenge machine learning community develop measure knowledge complexity capture cognitive complexity knowledge preliminary measure cognitive complexity called different used competition briefly discussed authors thank david challenge competition machine learning programs provided challenge learning programs inspired new ideas improving authors also thank help suggestions solve problems competition research conducted center machine learning inference university centers research supported part advanced research projects agency grant office naval research grant air force office scientific research part office naval research grant part national science foundation grants
construction bayesian network structures data brief survey efficient algorithm previous algorithms network structures data either highly dependent conditional independence tests required ordering nodes supplied user present algorithm integrates two approaches tests used generate ordering nodes database used underlying bayesian network structure using non test based method results evaluation algorithm number databases led presented also discuss algorithm performance issues open problems
covariance criterion adaptive model selection propose new criterion model selection prediction problems covariance criterion training error average covariance predictions responses prediction rule applied versions dataset criterion applied general prediction problems example regression classification general prediction rules example stepwise regression models neural nets obtain measure effective number parameters used adaptive procedure relate covariance criterion model selection procedures illustrate use regression classification problems also conditional bootstrap approach model selection
supervised competitive learning finding positions radial basis functions paper introduces neural gas algorithm extends unsupervised competitive learning class information improve radial basis functions basic idea discover heterogeneous clusters clusters data different classes additional neurons towards discovery associated neuron guided introducing kind effect performance tested number data sets including data set results demonstrate promise
modeling analogical problem solving production system architecture research supported national science foundation fellowship office naval research grant john views conclusions contained document authors interpreted representing policies either expressed implied national science foundation office naval research states government
inference dynamic problems efficient algorithms developed estimating model parameters measured data even presence errors addition point estimates parameters however uncertainty needed linear approximations provide standard errors applied models substantially nonlinear overcome difficulty methods developed case variables error free paper extend methods models use method integrate parameters associated measurement errors apply methods obtain approximate confidence parameters approach computationally efficient requiring function evaluations applied large scale problems useful certain measurement errors input variables relatively small small
implicit learning object recognition importance temporal context novel architecture set learning rules cortical selforganization proposed model based idea multiple information channels one plasticity features learned bottomup information sources thus influenced learned contextual pathways maximum likelihood cost function allows scheme implemented biologically feasible hierarchical neural circuit simulations model first demonstrate utility temporal context plasticity model learns representation faces according independent viewpoint taking advantage temporal image sequences second set simulations add plasticity contextual stream explore variations architecture case model learns representation starting coarse clustering clustering specific stimulus features model provides account people may perform object recognition hierarchical bottomup fashion
pruning adaptive boosting final draft boosting algorithm adaboost developed freund schapire exhibited performance several benchmark problems using c45 weak algorithm like ensemble learning approaches adaboost constructs composite hypothesis voting many individual hypotheses practice large amount memory required store hypotheses make ensemble methods hard applications paper shows selecting subset hypotheses possible obtain nearly levels performance entire set results also provide insight behavior adaboost
role afferent excitatory lateral inhibitory synaptic plasticity visual cortical ocular dominance
plasticity cortical neuron properties modeling effects bear primary visual cortex shifts ocular dominance toward closed eye cortical region near site ocular dominance shift previously modeled variants covariance synaptic plasticity rule bear showed primary visual cortex changes ocular dominance distribution reduces reduces orientation direction paper presents novel account effects based exin synaptic plasticity rules include afferent excitatory lateral inhibitory rule exin plasticity rules enhance efficiency discrimination neural networks representation perceptual patterns exin model decreases lateral neurons site control regions neurons inside region model plasticity afferent pathways neurons affected assumed opposed previous models bear afferent pathways open eye neurons region proposed model consistent results suggesting longterm plasticity bear bear since role plasticity lateral inhibitory pathways producing cortical plasticity received much attention several predictions made based exin lateral inhibitory plasticity rule
learning boxes membership equivalence queries present two algorithms use membership equivalence queries exactly identify concepts given axisparallel boxes euclidean space coordinate discrete values first algorithm receives uses time membership queries polynomial log constant equivalence queries made formulated log axisparallel boxes next introduce new complexity measure better captures complexity boxes simply number boxes dimensions new measure number segments target maximum one lies entirely inside entirely defining present improvement first algorithm uses time queries polynomial log hypothesis class used decision trees show time queries used algorithm polynomial log constant thus generalizing exact learnability dnf formulas constant number terms fact single algorithm efficient either constant
pruning backpropagation neural networks using modern stochastic optimization techniques approaches combining genetic algorithms neural networks received great deal attention recent years result much work reported two major areas neural network design training topology optimization paper focuses key issues associated problem pruning multilayer perceptron using genetic algorithms simulated annealing study presented considers number aspects associated network training may behavior stochastic topology discussed improve topology searches simulation results two stochastic optimization methods applied nonlinear system identification presented compared simple random search
belief propagation revision networks loops local belief propagation rules sort proposed pearl guaranteed converge optimal beliefs connected networks recently number researchers empirically demonstrated good performance algorithms networks loops theoretical understanding performance yet achieved foundation understanding belief propagation networks loops networks single loop derive analytical relationship steady state beliefs network true posterior probability using relationship show category networks map estimate obtained belief update belief revision proven optimal although beliefs will incorrect show nodes use local information receive order correct steady state beliefs furthermore prove networks single loop map estimate obtained belief revision convergence guaranteed give globally optimal sequence states result independent length cycle size state space networks multiple loops introduce concept network show simulation results comparing belief revision update networks show code structure present simulations code problem indicating obtained belief revision convergence significantly likely correct report describes research done center biological computational learning department brain cognitive sciences massachusetts institute technology support center provided part grant national science foundation contract asc9217041 also supported
scheduling maintenance electrical power transmission networks using genetic programming previous work showed combination genetic algorithm using order permutation combined hand coded greedy produce optimal schedule four node test problem following used find low cost schedules region high power network paper describes evolution best known schedule base problem using genetic programming starting hand coded used
methodology processing problem constraints genetic programming search mechanisms artificial intelligence combine two elements representation determines search space search mechanism actually explores space unfortunately many searches may explore redundant andor solutions genetic programming refers class evolutionary algorithms based genetic algorithms utilizing parameterized representation form trees algorithms perform searches based simulation nature face problems subspaces problems just recently addressed systematic manner paper presents methodology domain genetic programming tool methodology uses data semantic information representation space valid possibly unique solutions will explored user constraints transformed normal set set feasibility subsequently used limit space explored constraints determine valid possibly unique space moreover also used subspaces user considers using knowledge simple example followed illustrate constraint language transformations normal set experiments boolean illustrate practical applications method limit redundant space exploration utilizing knowledge supported grant
linear systems outputs paper present necessary sufficient conditions class systems linear systems whose output saturation function measured
reasoning data
automated refinement firstorder domain theories knowledge acquisition difficult task task automatically improving existing knowledge base using learning methods addressed class systems performing theory refinement paper presents system firstorder revision theories examples refines firstorder theories integrating variety different revision techniques coherent whole uses techniques within hillclimbing framework guided global heuristic identifies possible errors theory calls library operators develop possible revisions best revision implemented process revisions possible operators drawn variety sources including propositional theory refinement firstorder induction inverse resolution demonstrated several domains including logic programming qualitative modelling
adaptive computation techniques time series analysis
feature generation sequence categorization problem sequence categorization generalize labeled sequences procedures accurately future sequences choice representation sequences major impact task absence background knowledge good representation often known straightforward representations often far optimal propose feature generation method called creates boolean features check presence absence selected show empirically representation computed improves accuracy two commonly used learning systems c45 new features added existing representations sequence data show superiority across range tasks selected three domains dna sequences sequences english text
pac learning membership queries extended abstract
genetic algorithms combinatorial optimization assembly line balancing problem genetic algorithms one example use random element within algorithm combinatorial optimization consider application genetic algorithm particular problem assembly line balancing problem general description genetic algorithms given specialized use problems discussed extensive computational testing find appropriate values various parameters associated genetic algorithm experiments importance correct choice scaling parameter mutation rate ensure good performance genetic algorithm also describe parallel implementation genetic algorithm give comparisons parallel serial implementations versions algorithm shown effective producing good solutions problems type appropriately chosen parameters
possible contribution avoidance using cbr methods paper presents application casebased reasoning methods data base international casebased reasoning tool used classification various outcome variables like outcome solution conflict addition case retrieval algorithms presented interactive tool searching conflict data base cases
confidence higher order uncertainty proposed handling higher order uncertainty including bayesian approach
inductive bias casebased reasoning systems order learn behaviour casebased learning systems simple casebased learner pac learning algorithm using casebased representation first consider naive casebased learning algorithm learns available cases similarity number features two problem descriptions agree present results concerning consistency learning algorithm give partial results regarding sample complexity able weak general learning algorithm consider sample complexity casebased learning reduced specific classes target concept application inductive bias prior knowledge class target concepts following recent work demonstrating casebased learning improved choosing similarity measure appropriate concept define second casebased learning algorithm learns using best possible similarity measure might inferred chosen target concept learning strategy since chosen similarity measure defined terms priori knowledge actual target concept allows assess limit maximum possible contribution approach casebased learning also addition role inductive bias definition general problem functions might represented form reasoning casebased representation special case therefore little straightforward general case allowing substantial results regarding functions sample complexity presented results conclude casebased learning best approach learning chosen concept space space functions discuss however study demonstrated context casebased learning operation concepts known machine learning inductive bias tradeoff computational complexity sample complexity
state evolutionary computation past years evolutionary computation landscape rapidly changing result increased levels interaction various research groups new ideas challenge old effect simultaneously field activity structure common agreement important open issues attempt summarize emergent properties paper
towards better understanding memorybased reasoning systems quantify experimentally analytically performance memorybased reasoning algorithms start insight capabilities algorithms compare algorithm using value difference metric popular bayesian classifier two approaches similar make certain independence assumptions data however whereas uses specific cases perform classification bayesian methods summarize data demonstrate particular system called works wide range domains using real artificial data respect artificial data consider distributions concept classes separated functional data generated markov models varying complexity finally show formally learn limit natural concept classes bayesian classifier learn will perfect accuracy
flexible metric nearest neighbor decision rule assigns object unknown class class among labeled training objects usually terms metric distance euclidean space input measurement variables metric chosen distance strongly performance optimal choice depends problem hand characterized class distributions input measurement space within given problem location unknown object space paper new types procedures described estimate local relevance input variable linear combinations individual point information used separately metric used distance object nearest neighbors procedures hybrid regular methods treestructured recursive partitioning techniques popular statistics machine learning
hybrid genetic search data data interpretation problems typically solved using computationally local search methods often result solutions traditional hybrid genetic algorithm compared different hybrid genetic algorithms static problem traditional hybrid genetic algorithm used applied local search every produced genetic search hybrid genetic algorithms designed temporally separate local genetic search components distinct phases minimize interference two search methods results show hybrid genetic algorithms produce higher quality solutions using significantly less computational time problem
using genetic algorithms explore pattern recognition system comments describe system model based binary strings model directed understanding pattern recognition processes learning take place individual species levels system genetic algorithm central component model paper study behavior two pattern recognition problems relevant natural systems finally compare model explicit fitness sharing techniques genetic algorithms show model implements form implicit fitness sharing
locally linear nested network robot manipulation present method accurate representation highdimensional unknown functions random samples drawn input space method builds representations function splitting input space smaller subspaces subspaces linear approximation computed representations function levels tree learning process good generalisation available accurate representations therefore fast accurate learning combined method
equivalence linear boltzmann chains hidden markov models sequence several authors made link hidden markov models time series models williams jordan jordan discuss linear boltzmann chain model transition state state symbol probability entire state hmm written linear boltzmann chain setting linear boltzmann chains represented hmms jordan however difference two models minimal precise final hidden state linear boltzmann chain constrained particular end state distribution sequences identical hidden markov model
coevolutionary approach learning sequential decision rules present coevolutionary approach learning sequential decision rules appears number advantages approaches coevolutionary approach formation stable representing simpler evolutionary direction controlled independently providing alternative evolving complex behavior using intermediate training steps results presented showing significant learning rate speedup approach simulated robot domain addition results suggest coevolutionary approach may lead problem
adapting bias gradient descent incremental version appropriate bias widely viewed key efficient learning generalization present new algorithm incremental algorithm learning appropriate biases based previous learning experience algorithm developed case simple linear learning rule separate parameter input algorithm parameters important form bias system bias approach adapted based previous learning experience appropriate nonstationary learning tasks particular tasks type show algorithm performs better ordinary fact finds optimal learning rates algorithm extends improves prior work fully incremental single free parameter paper also extends previous work presenting derivation algorithm gradient descent space parameters finally offer novel interpretation algorithm incremental form cross validation
adaptive parameter pruning neural networks neural network pruning methods level individual network parameters connection weights improve generalization open problem pruning methods known today selection number parameters removed pruning step pruning strength paper presents pruning method automatically adapts pruning strength evolution weights loss generalization training method requires algorithm parameter user results extensive experimentation indicate often superior superior diagnosis tasks pruning early training process required results statistical significance tests comparing new method backpropagation early stopping given different problems
objectoriented connectionist simulator gives overview simulator main concepts connectionist net simulator developed written objectoriented meet requirements flexibility reuse homogeneous structured connectionist nets allow user efficient implementations perhaps running hardware nets composed combining library classes necessary specializing behaviour general user interface classes allow uniform presentation nets modeled
generic mechanisms use case adaptation casebased reasoning new problems solved retrieving adapting solutions similar problems encountered past important issue reasoning identify different types knowledge reasoning useful different classes tasks paper examine class tasks involve new elements old solutions describe modelbased method solving task context design physical devices method uses knowledge generic mechanisms old designs adapted meet new functional specifications appropriate system evaluates computational feasibility method design adaptation
casebased reasoning comparative utility analysis casebased reasoning learning systems utility problem learning systems occurs knowledge learned attempt improve systems performance performance instead present methodology analysis utility problem uses computational models problem solving systems root causes utility problem detect threshold conditions problem will arise design strategies eliminate present models casebased reasoning learning systems compare respect utility problem analysis suggests cbr systems utility problem systems
model retrieval present model retrieval attempts capture three psychological phenomena people extremely good similarity analogy given items compare much structural people sometimes experience use purely structural analogical model called many called chosen consists two stages first stage uses computationally filter candidates memory items encode structured representations content vectors whose product yields estimate corresponding structural representations will match second stage uses sme compute true structural match output first stage fully implemented show capable modeling patterns access found psychological data
online learning linear functions present algorithm online learning linear functions optimal within constant factor respect bounds sum squared errors worst case sequence trials bounds logarithmic number variables furthermore algorithm shown robust respect noise data within constant factor key words machine learning computational learning theory online learning linear functions worstcase loss bounds adaptive filter theory subject classifications
constructive similarity assessment using stored cases define new situations fundamental issue casebased reasoning similarity assessment determining similarities differences new retrieved cases many methods developed comparing input case descriptions cases already memory however success methods depends input case description sufficiently complete reflect important features new situation casebased explanation events story understanding arises current situation understood consequently similarity assessment based matches known current features old cases likely fail gaps current cases description solution problem gaps new cases description approach call constructive similarity assessment constructive similarity assessment similarity assessment simple comparison fixed new old cases process types features investigated new situation features knowledge added description current case constructive similarity assessment compare new cases old using prior cases guide dynamically augmented descriptions new cases memory
towards computer model memory search strategy learning much recent research modeling memory processes focused identifying useful indices retrieval strategies support particular memory tasks another important question concerning memory processes however retrieval criteria learned paper examines issues involved modeling learning memory search strategies discusses general requirements appropriate strategy learning presents model memory search strategy learning applied problem retrieving relevant information adapting cases casebased reasoning discusses implementation model based lessons learned implementation points towards issues directions refining model
recombination operator correlation fitness landscape search performance author association thesis except provided neither thesis substantial may otherwise material form without authors prior written permission
structured variational approximations problem approximating probability distribution occurs frequently many areas applied including statistics communication theory machine learning theoretical analysis complex systems neural networks jordan recently proposed powerful method efficiently approximating probability distributions known structured variational approximations structured variational approximations exact algorithms probability computation tractable combined variational methods handle interactions make system whole intractable note present mathematical result derivation variational approximations exponential family distributions
selforganizing binary decision tree incrementally defined rule based paper presents asocs adaptive selforganizing concurrent system model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control asocs adaptive network composed many simple computing elements operating parallel paper focuses adaptive algorithm details architecture learning algorithm advantages previous asocs models simplicity cost asocs operate either data processing mode learning mode data processing mode asocs acts parallel hardware circuit learning mode rules expressed boolean conjunctions incrementally presented asocs asocs learning algorithms incorporate new rule distributed fashion short bounded time
dynamic hill climbing optimization techniques paper describes novel search algorithm called dynamic hill climbing ideas genetic algorithms hill climbing techniques unlike genetic hill climbing algorithms dynamic hill climbing ability dynamically change coordinate frame course optimization furthermore algorithm moves search search function space changing mutation rate uses distance metric ensure searches new regions space dynamic hill climbing empirically compared traditional genetic algorithm using wellknown five function test suite shown performance genetic algorithm often finding better solutions using many function evaluations
applying genetic algorithms testing intelligent controllers autonomous vehicles require sophisticated software controllers maintain vehicle performance presence vehicle test evaluation complex software controllers challenging task goal effort apply machine learning techniques general problem evaluating intelligent controller autonomous vehicle approach involves controller adaptively chosen set fault scenarios within vehicle simulator searching combinations produce performance vehicle controller search employs genetic algorithm evidence suggests approach effective forms automated testing sophisticated software controllers several intelligent controllers tested project using several different genetic learning programs course research representation evaluation function genetic operators basic algorithm evolved paper presents work point view describing process authors followed applying learning realworld problem
theory unsupervised speedup learning speedup learning seeks improve efficiency problem paper propose new theoretical model speedup learning captures systems improve problem solving performance solving set problems also use model notion batch problem solving argue learning sequential problem solving theoretical results applicable domains empirically results domain
fast nonparametric density estimation algorithm nonparametric density estimation problem approximating values probability density function given samples associated distribution nonparametric estimation finds applications discriminant analysis cluster analysis flow calculations based usual estimators make use kernel functions require order arithmetic operations evaluate density sample points describe sequence special weight functions requires almost linear number operations computation
evolution different levels organization imply
learning firstorder acyclic horn programs entailment paper consider learning firstorder horn programs entailment particular show subclass firstorder acyclic horn programs constant exactly learnable equivalence entailment membership queries provided allows polynomialtime subsumption procedure satisfies conditions one consequence firstorder acyclic horn programs constant exactly learnable entailment membership queries
using genetic algorithms solve npcomplete problems strategy using genetic algorithms gas solve npcomplete problems presented key aspect approach taken exploit observation although npcomplete problems equally difficult general computational sense much better representations others leading much successful use gas npcomplete problems others since npcomplete problem one polynomial time strategy described consists identifying canonical npcomplete problem gas work solving npcomplete problems mapping onto canonical problem initial empirical results presented support claim boolean satisfiability problem canonical problem npcomplete problems poor representations solved efficiently mapping first onto problems
learning multiagent stochastic domains using likelihood estimates fully cooperative multiagent agents share joint utility special interest key problem actions individual agents especially settings agents autonomous decision investigate approaches learning strategies stochastic domains agents actions directly observable others much recent work game theory bayesian learning perspective general problem equilibrium selection tends assume actions observed discuss special problems arise actions observable including effects rates convergence effect action failure probabilities also use likelihood estimates means generalizing play learning models setting finally propose use maximum likelihood means removing strategies aim convergence conventional equilibrium point learning
learning generic mechanisms experiences analogical reasoning humans appear often solve problems new domain expertise familiar domain however making hard often requires abstractions common source target domains recent work casebased design suggests generic mechanisms one type abstractions used designers however one important yet issue generic mechanisms hypothesize acquired incrementally problemsolving experiences familiar domains generalization patterns three important issues generalization experiences generalize experience far generalize methods use paper show mental models familiar domain provide content together problemsolving context learning occurs also provide constraints learning generic mechanisms design experiences particular show modelbased learning method integrated learning addresses issues generalization experiences
optimal mutation rates genetic search optimization single bit string means iterated mutation selection best genetic algorithm discussed respect three simple fitness functions ones problem standard binary encoded integer coded integer optimization problem mutation rate schedule optimal respect success probability mutation presented objective functions turns standard binary code search process even case objective functions normally mutation rate bit string length results indicate variation mutation rate useful cases fitness function multimodal function may caused objective function encoding mechanism
learning robot behaviors using genetic algorithms genetic algorithms used learn navigation avoidance behaviors robots learning performed simulation resulting behaviors used control approach learning behaviors robots described particular methodology learning via simulation model motivation making mistakes real systems may costly addition time constraints might limit number experiences learning real world many cases simulation model made run faster real time since learning may require behaviors might produce results applied real world might require much time real environment assume behaviors will evaluated simulation model offline system illustrated figure current best behavior real online system learning offline system learning algorithm designed learn useful behaviors simulations limited expectation behaviors learned simulations will useful realworld environments previous studies illustrated knowledge learned simulation robust might applicable real world simulation general noise conditions etc real world environment possible important identify differences simulation world note effect upon learning process research reported examine hypothesis next section briefly explains learning algorithm gives extensive found actual robot described describe simulation robot task actual robot
bayesian graphical modeling intelligent systems conventional intelligent systems uncertainty students knowledge yet outcome teaching exact state students knowledge uncertain recent years researchers made progress management uncertainty knowledgebased systems building developments describe architecture explicitly models uncertainty will facilitate accurate student modeling provide learn
algorithm boolean satisfiability problems satisfiability refers task finding assignment makes arbitrary boolean expression true paper compares neural network algorithm greedy algorithm solving satisfiability problems solve problem instances difficult traditional satisfiability algorithms results suggest scales better number variables increase solving least many hard problems
neural networks artificial life perspective last years several researchers within artificial life mobile robotics community used artificial neural networks explicitly neural networks artificial life perspective number consequences make research will call artificial life neural networks rather different traditional connectionist research aim paper make differences classical neural networks explicit
architectures invariant object learning recognition multiple views recognition objects sequences views modeled family selforganizing neural architectures called use view information encoded networks incorporates generates invariant representation image supervised incremental learning system fuzzy representations view categories whose outputs combined invariant object categories working memory makes object prediction evidence time object category nodes multiple views mit laboratory database views aircraft including small views without additive noise recognition rate achieved one view correct three views properties view object category nodes compared cells cortex
unifying view training algorithms multilayer perceptrons filter synapses recent interest deriving various neural network architectures modelling signals number algorithms published multilayer perceptrons synapses described finite response infinite response iir filters latter case also known locally recurrent globally feedforward networks derivations algorithms used different approaches calculating note present short unifying account different algorithms compare case derivation performance new algorithms subsequently presented simulation results performed benchmark algorithms note results compared chaotic time series number methods including standard multilayer perceptron local approximation method
optimal number learning samples hidden units function approximation feedforward network paper presents methodology estimate optimal number learning samples number hidden units needed obtain desired accuracy function approximation feedforward network representation error generalization error components total approximation error analyzed approximation accuracy feedforward network investigated function number hidden units number learning samples based behavior approximation error model error function introduced parameters determined experimentally alternative model error function include theoretical results general bounds approximation also analyzed combination knowledge computational complexity learning rule optimal learning set size number hidden units found resulting minimum computation time given desired precision approximation approach applied optimize learning mapping guided robot arm complex function approximation
graphical gaussian model determination propose methodology bayesian model determination graphical gaussian models achieve aim consider inverse prior distribution matrix given graph ensure across models prior distributions obtained prior conditional complete graph explore alternative structures hyperparameters latter consequences model model determination carried implementing reversible mcmc sampler particular move propose involves adding edge graph set moves preserve graph giving fast algorithm maintaining tree representation graph state variable propose use incomplete matrix containing elements corresponding element inverse allows computations performed locally clique level clear advantage analysis large complex datasets finally statistical computational performance procedure illustrated means artificial real multidimensional datasets
reasoning design perspective essential component behavior opportunity recognition recognition conditions facilitate pursuit goal opportunity recognition special case situation assessment process novel situation ability recognize opportunities problem contexts one way goals design crucial creative design order deal real world opportunity recognition attribute limited inferential power relevant goals propose goals working memory monitor internal hidden representations currently objects goal satisfied current internal representation goal match propose computational model working memory compare relevant theories planning working memory model implemented part system
size neural network gives optimal generalization convergence properties backpropagation technical report institute advanced computer studies university college abstract one important aspects machine learning paradigm scales according problem size complexity using task known optimal training error maximum number training updates investigate convergence backpropagation algorithm respect complexity required function approximation size network relation size required optimal solution degree noise training data general solution found worse function approximated complex networks result lower training generalization error certain cases use committee ensemble techniques level noise training data increased experiments performed obtain optimal solution case support observation larger networks produce better training generalization error using face recognition example network many parameters training points generalizes better smaller networks
lessons neural network training overfitting lessons neural network training overfitting may many reasons neural networks become popular machine learning models two important aspects machine learning models model generalizes unseen data model scales problem complexity using controlled task known optimal training error investigate convergence backpropagation algorithm find optimal solution typically found furthermore observe networks larger might expected result lower training generalization error result supported another real world example investigate training behavior analyzing weights trained networks degrees freedom seen little aid convergence characteristics multilayer perceptron neural networks polynomial models overfitting behavior different mlp often biased towards solutions finally analyze relevant theory reasons significant practical differences results question common beliefs neural network training regarding convergence optimal network size suggest alternate guidelines practical use lower degrees freedom help direct future work methods creation solutions importance bias possibly worse performance improved training algorithms
learning classification rules using extended abstract paper presents novel induction algorithm induces classification rules using lattice explicit map search space rules system shown compare favorably commonly used symbolic learning methods use heuristics rather explicit map guide search rule space furthermore learning system shown robust presence noisy data system also capable learning decision lists rule sets allowing comparisons different learning paradigms within algorithmic framework
next step towards case retrieval large case bases keywords casebased reasoning case retrieval case representation paper deals retrieval useful cases casebased reasoning focuses questions useful mean search useful cases organized present new search algorithm able search quickly case base even aspects usefulness combined query time compare algorithms show make implicit closed world assumption refer realization presented idea context prototype follows previously collected cases stored large case base expert describes problem gives aspects case similar similarity measure thus given now used explore case base within short time present required number cases make cases similar question now previously collected cases retrieval algorithm able deal similarity measures
evolution time space parallel genetic algorithm parallel genetic algorithm uses two major modifications compared genetic algorithm firstly selection distributed individuals world selection done individual independently neighborhood secondly individual may improve fitness lifetime local hillclimbing asynchronous running maximal efficiency mimd parallel computers search strategy based small number active intelligent individuals whereas uses large population individuals will investigate problems problem outline parallel search information exchange individuals represent optimization problem fitness landscape certain configuration space see tries two local minima third still better local minima using crossover operator successful fitness landscape certain correlation show correlation problem configuration space analysis explores implicitly correlation
casebased learning beyond classification feature vectors casebased research recent classifying cases represented feature vectors however useful tasks representations often review recent literature casebased learning focusing alternative performance tasks expressive case representations also topics need additional research
memorybased acquisition processing current approaches computational language technology knowledgebased try abstract away specific domains applications results complexity acquisition alternative propose particular approach natural language processing based automatic memorybased learning linguistic tasks consequences approach computational discussed application approach number acquisition tasks described
new sequential simulated annealing method let function explicitly defined sequence functional estimators context propose new sequential algorithm asymptotically using stepwise estimators prove conditions almost convergence law algorithm
competitive learning methods
stochastic complexity based estimation missing elements data paper study new approach missing data estimation multivariate categorical data approach discussed modelbased procedure relative model class functional form probability distribution complete data matrix case set models independence assumptions based given model class assumption informationtheoretic criterion derived select different complete data matrices intuitively general criterion called stochastic complexity represents shortest code length needed coding complete data matrix relative model class chosen using informationtheoretic criteria missing data problem reduced search problem finding data minimal stochastic complexity experimental part paper present empirical results approach using two real data sets compare results commonly used techniques case sample averages
evolutionary search algorithm scheduling problem present paper new evolutionary procedure solving general optimization problems combines efficiently mechanisms genetic algorithms search order explore solution space properly interaction phases periods optimization algorithm adaptation search principle national problem discussed hybrid method developed paper suited open shop scheduling problems results obtained appear quite satisfactory
modelling behavioural sequences evolutionary programming modelling behavioural behavioural observations often described sequence symbols drawn finite however inductive inference strings automated technique produce models data nontrivial task paper considers modelling behavioural data using probabilistic finite state automata number informationtheoretic techniques evaluating possible hypotheses measure used paper minimum message length mml although attempts made construct models incremental addition using heuristic rules mml give information cost models shown globally optimal evolutionary programming produce globally optimal models evolving data structures arbitrary complexity without requirement encode binary strings genetic algorithms however evaluation evolution process mml alone possible since will symbols partially correct solution suggested addition symbol symbol difficulty addition symbol also permits evolution models need explain data useful property avoid overfitting noisy data results given test set optimal model known set eye data derived simulator
inductive learning selection minimal complexity representations
signal processing reversible sampler autoregressive time series employing full technical report use reversible markov chain monte carlo mcmc methods address problem model order uncertainty autoregressive time series within bayesian framework efficient model achieved model space moves full conditional density parameters obtained analytically compared alternative method moves compute made new parameters move results presented synthetic time series
casebased planning learn learning viewed problem planning series modifications memory view learning propose applicability casebased planning methodology task planning learn argue relatively simple primitive inferential operators needed support flexible planning show possible obtain benefits casebased reasoning within planning learn framework
pac analyses similarity learning algorithm simple instancebased learning algorithm weighted similarity measure cases paper presents pac analysis motivated pac learning framework demonstrates two main ideas relevant study instancebased learners firstly hypothesis spaces learner different target concepts compared predict difficulty target concepts learner secondly helpful consider constituent parts instancebased learner explore separately many examples needed infer good similarity measure many examples needed case base applying approaches show learns quickly variables representation irrelevant target concept relevant variables paper overall behaviour behaviour constituent parts
discovering partial determinations mixed numerical symbolic domains partial determinations interesting form dependency attributes relation generalize functional dependencies allowing exceptions modify known mdl formula evaluating partial determinations allow use heuristic exhaustive search furthermore describe efficient approach handling numerical attributes empirical investigation tries evaluate presented ideas
assessment candidate models induced symbol datasets induction optimal finite state machine explanation symbol strings known least npcomplete however satisfactory approximately optimal explanations may found use evolutionary programming shown information theoretic measure finite state machine explanations used fitness function required evaluation candidate explanations search nearoptimal explanation obvious measure class explanation will others search empirical studies possible gain insight dimensions measure general probabilistic finite state machines explanations minimum message length estimator minimum number transitions will explanations information measure will also explanations distributions frequencies transitions node suggesting repeated sequences symbol strings will explanation approximate bounds acceptance explanations length string required induction successful also derived simplest possible random explanations information measure
evolving globally cellular automata evolutionary process interact distributed system order produce globally behavior using genetic algorithm evolve cellular automata show evolution synchronization one type emergent coordination takes advantage underlying potential form embedded typically phase regions designed evolutionary process global phase describe detail one typical solution discovered discovered synchronization algorithm terms embedded interactions also use description analyze evolutionary sequence solution discovered results implications understanding emergent collective behavior natural systems automatic programming spatially extended multiprocessor systems
estimators prove general bootstrap theorem possibly builds recent due van result extends results type bootstrap due raftery three examples models parameter spaces general theorem
individual collective prediction prediction survival time recurrence time important learning problem medical domains recurrence surface approximation method natural effective method predicting recurrence times using input data paper introduces survival curve extension approach produces accurate predicted rates recurrence maintaining accuracy individual predicted recurrence times method applied problem breast cancer recurrence using two different datasets
selective sampling using query committee algorithm running title selective sampling using query committee analyze query committee algorithm method filtering queries random stream inputs show committee algorithm achieves information gain positive lower bound prediction error decreases exponentially number queries show particular exponential decrease holds query learning perceptrons keywords selective sampling query learning bayesian learning experimental design freund bell laboratories hill
nonlinear component analysis kernel problem new method performing nonlinear form principal component analysis proposed use operator kernel functions one efficiently compute principal components highdimensional feature spaces related input space nonlinear map instance space possible products images give derivation method present first experimental results polynomial feature extraction pattern recognition
introduction special section knowledgebased construction probabilistic decision models ieee transactions modeling techniques developed recently uncertain reasoning permit significantly flexible specifications probabilistic knowledge specifically graphical networks influence diagrams compact representation probabilistic relationships support inference algorithms automatically exploit dependence structure models advances interest computational decision systems based theories belief preference however graphical languages still quite limited purposes knowledge representation describe relationships among particular event instances capture general knowledge probabilistic relationships across classes events capture general knowledge serious tasks relevant factors decision problem advance graphical decision model particular set probabilistic dependencies predefined set decision alternatives specific mathematical form utility function given properly specified model exist relatively efficient algorithms calculating posterior probabilities optimal decision policies range similar cases may handled parametric variations original model however structure dependencies set available alternatives form utility function changes situation situation fixed network representation longer adequate computational decision system possess general broad knowledge domain ability reason particular circumstances given decision problem within domain one obvious call call knowledgebased model construction generate decision model dynamically runtime based problem description information received thus far model construction consists selection assembly causal relationships broad knowledge base general relationships among domain concepts example develop system appropriate actions maintaining computer network natural graphical decision model include
dynamical selection learning algorithms determining conditions given learning algorithm appropriate open problem machine learning methods selecting learning algorithm given domain limited success paper proposes new approach predicting given examples class example space choosing best learners region example space make predictions regions example space defined prediction patterns learners used learners chosen prediction selected according past performance region dynamic approach learning algorithm selection compared methods selecting multiple learning algorithms approach extended weight rather select algorithms according past performance given region approaches evaluated set determining conditions given learning algorithm appropriate open problem machine learning methods selecting learning algorithm given domain domain limited success paper proposes new approach dynamically selects learning algorithm example example space choosing best learners prediction part example space regions example space formed observed prediction patterns learners used learners chosen prediction selected according past performance region defined crossvalidation history paper introduces method dynamic selection learning algorithms call dynamic learning algorithms used classify novel example depends example preliminary experimentation motivated extension dynamically weights learners predictions according accuracy experimentation compares collection strategies crossvalidation various forms phase six constituent learners heterogeneous search representation methods rule learner cn2 decision tree learner c45 decision tree learner instancebased learner knearest neighbor learner ten domains compared several strategies
learning concepts questions important issues machine learning explored role memory plays acquiring new concepts extent learner take active part acquiring concepts chapter describes program called uses concepts learned previously learn new concepts program forms hypotheses concept learned tests hypotheses trainer questions learning begins trainer shows example concept learned program determines objects example belong concepts stored memory description new concept formed using information obtained memory generalize description training example generalized description tested program constructs new examples shows trainer belong target concept
complex environments complex behaviors complex environments complex behaviors adaptation systems environments commonly viewed explicit fitness function defined priori measured based population size andor rates methods capture role environmental complexity selective control adaptive process simulations computational tools latent energy environments model allow characterize closely effects environmental complexity evolution adaptive behaviors described paper motivation arises need vary complexity controlled ways without assuming relationship changes adaptive behaviors goal achieved characterization environments different forms energy genetic algorithm using fitness local selection used model evolutionary process individuals population modeled neural networks simple systems variations behaviors related interactions varying environments outline results three experiments analyze different sources environmental complexity effects collective behaviors evolving populations
distributed representations nested compositional structure
efficient subsumption algorithm inductive logic programming paper investigate efficiency subsumption basic relation ilp npcomplete even restrict horn clauses contain small constant number literals investigate several restrictions first adapt notion clauses used ilp show subsumption polynomial time respect secondly adapt notion horn clauses show subsumption efficiently computable reasonably small show results combined give efficient reasoning procedure horn clauses recently suggested polynomial simple argument finally outline reduction algorithm essential part every algorithm proved ideas
strongly genetic programming technical report abstract genetic programming powerful method automatically generating computer programs via process natural selection koza however limitation known variables constants arguments functions values functions must data type correct introduce variation genetic programming called strongly genetic programming variables constants arguments values data type data type value specified allows process genetic operators generate correct parse trees key concepts generic functions true strongly functions rather classes functions generic data types analogous illustrate present four examples involving manipulation list manipulation multidimensional regression problem multidimensional kalman filter list manipulation function list manipulation function
even arbitrary transfer functions compute certain category algorithms architectures recurrent networks part paper submitted preference abstract existing demonstrating computational limitations recurrent correlation network explicitly limit results units sigmoidal transfer functions proof given shows given finite discrete deterministic transfer function used units network finitestate automata network model many units used proof applies equally continuous transfer functions finite number sigmoid function
efficient algorithms subsumption subsumption incomplete approximation logic implication important inductive logic programming theorem proving show context based elimination possible matches certain clauses tested subsumption polynomial time discuss relation subsumption clique problem showing particular using additional prior knowledge space small fraction search space identified possibly containing globally consistent solutions leads effective pruning rule present empirical results demonstrating combination approaches provides reduction computational effort
learning sparse perceptrons introduce new algorithm designed learn sparse perceptrons input representations include features algorithm based method able relatively natural class target concepts moreover algorithm appears work practice set three problem domains algorithm produces classifiers utilize small numbers features yet exhibit good generalization performance perhaps algorithm generates concept descriptions easy humans understand
inductive learning algorithms relieff current inductive machine learning algorithms typically use greedy search limited lookahead detect significant conditional dependencies attributes describe training objects instead functions lookahead propose use relieff extension relief developed heuristic guidance inductive learning algorithms assistant system induction decision trees using relieff estimator attributes selection step algorithm tested several artificial several real world problems results compared known machine learning algorithms excellent results artificial data sets two real world problems show advantage presented approach inductive learning
method hierarchical reinforcement learning paper presents new approach hierarchical reinforcement learning based decomposition value function decomposition procedural declarative representation value function hierarchical policy extends previous work hierarchical reinforcement learning hinton conditions decomposition represent optimal value function derived paper defines hierarchical learning algorithm convergence shows experimentally learn much faster ordinary flat learning finally paper discusses interesting issues arise hierarchical reinforcement learning including hierarchical credit assignment problem execution hierarchy
causality genetic programming causality changes structure object effects changes changes properties behavior object paper analyzes concept causality genetic programming suggests used adapting control parameters search first analyze effects crossover show weak causality representation operators hierarchical approaches based discovery evolution functions phenomenon however selection gradually strongly causal changes causality correlated search space exploitation discussed context tradeoff results described argue bottomup evolutionary thesis finally new developments based idea architecture evolution koza discussed causality perspective
empirical comparison voting classification algorithms bagging boosting variants methods voting classification algorithms bagging adaboost shown successful improving accuracy certain classifiers artificial realworld datasets review algorithms describe large empirical study comparing several variants conjunction decision tree three variants naivebayes purpose study improve understanding algorithms use combination techniques affect classification error provide bias variance decomposition error show different methods variants influence two terms allowed determine bagging reduced variance methods boosting methods adaboost reduced bias variance methods increased variance naivebayes stable observed adaboost used instead indicating fundamental difference voting variants introduced paper include pruning versus pruning use probabilistic estimates weight perturbations data found bagging improves probabilistic estimates conjunction used data measure tree sizes show interesting positive correlation increase average tree size adaboost trials success reducing error compare error voting methods methods show voting methods lead large significant errors practical problems arise implementing boosting algorithms explored including numerical use show adaboost instances hard areas also noise
intelligence longterm goal field creation understanding intelligence research practical theoretical benefits notion intelligence precise enough allow development robust systems general results concept rational agency long considered leading candidate role paper outlines evolution formal intelligence simultaneously reduces gap theory practice directions future research
design evaluation rise learning system technical report august
reduced representations compositional distributed representations solution problem representing compositional structure using distributed representations described method uses associate items represented vectors arbitrary variable short sequences various reduced representations fixed width vector representations items right used constructing compositional structures noisy given memories using separate associative memory good properties
figure average model size accepted random samples various size based algorithm algorithm maintains model consistent past examples new tries extend model minimal fashion conducted set experiments random automata represent different strategies generated algorithm learn based samples behavior algorithm learn compact models agree samples size sample small effect size model experimental results suggest random samples algorithm however following result difficulty learning almost uniform complete samples obvious algorithm solve complexity issue dfa general sample currently classes samples algorithm incorporating opponent models adversary search technical report report march unsupervised learning finite automata practical approach technical report report march evolution social activity technical report department computer science
analysis convergence generalization aa1 aa1 incremental learning algorithm adaptive selforganizing concurrent systems asocs asocs selforganizing dynamically growing networks computing nodes aa1 learns discrimination implements knowledge distributed fashion nodes paper reviews aa1 perspective convergence generalization formal proof aa1 converges arbitrary boolean instance set given discussion generalization aspects aa1 including problem handling follows results simulations realworld data presented show aa1 gives promising generalization
machine learning bias statistical bias statistical variance decision tree algorithms term bias widely different fields machine learning statistics paper uses term shows measure statistical bias variance learning algorithms statistical bias variance applied problems machine learning bias paper shows four examples finally paper discusses methods reducing bias variance methods based voting reduce variance paper compares bagging method tree method voting decision trees methods uniformly improve performance data sets repository tree yields perfect performance recognition task weighted nearest neighbor algorithm based infinite bootstrap also introduced general decision tree algorithms variance important implication work appropriate inappropriate machine learning important cause poor performance decision tree algorithms
roles reinforcement learning analyze use policies form domain knowledge improve speed scaling reinforcement learning algorithms often used robotics also wellknown aid statespace search systems consider policies conditions chosen level primitive actions learning agent act particular way period time overall may either accelerate learning depending particular task analyze effect simple example effect two parts effect changing exploratory behavior independent learning effect learning independent effect behavior example effects significant latter appears larger finally provide complex appropriately chosen accelerate overall learning
reinforcement learning hierarchies machines present new approach reinforcement learning policies considered learning process constrained hierarchies partially specified machines allows use prior knowledge reduce search space provides framework knowledge across problems component solutions solve larger complicated problems approach seen providing link reinforcement learning approaches control present provably convergent algorithms problemsolving learning hierarchical machines demonstrate effectiveness problem several states
explanationbased approach improve retrieval casebased planning casebased planner retrieving previous case solving new similar problem often implicit features new problem situation determine particular case may successfully applied means cases may retrieved error case may fail improve planners performance retrieval may incrementally improved detecting explaining failures occur paper provide definition case failure planner derivation replay solves new problems previous plan derivations provide ebl explanationbased learning techniques detecting constructing reasons failure also describe case library incorporate failure information produced finally present empirical study demonstrates effectiveness approach improving performance
statistical evaluation neural network experiments minimum requirements current practice
free speech probability estimation dynamic connected artificial neural networks paper presents new methods training large neural networks probability estimation architecture combining recurrent connections used capture important dynamic information speech signal number connections fully connected recurrent network grows number hidden units schemes sparse connection connection pruning explored found connected networks outperform fully connected equal number connections implementation combined architecture training scheme described detail networks evaluated hybrid system recognition database word recognition database achieved standard set database range reported training simulation software used made available author detailed information software training process given
bagging work bayesian account implications success error rate decisiontree classification learners often much reduced bagging learning multiple models bootstrap samples database combining uniform voting paper empirically test two alternative explanations based bayesian learning theory bagging works approximation optimal procedure bayesian model averaging appropriate implicit prior bagging works effectively shifts prior appropriate region model space experimental evidence first hypothesis second bagging simple effective way reduce error rate many classification learning algorithms example empirical study described reduces error decisiontree learner databases average bagging procedure given training set size bootstrap constructed taking samples training set thus new training set size produced original examples may appear average original examples will appear bootstrap sample learning algorithm applied training set procedure repeated times resulting models uniform voting bagging one several multiple model approaches recently received much attention see example procedures type include boosting freund schapire
query committee propose algorithm called query committee committee students trained data set next query chosen according principle maximal algorithm studied two models game perceptron learning another perceptron number queries committee algorithm yields asymptotically finite information gain leads generalization error decreases exponentially number examples contrast learning randomly chosen inputs information gain approaches zero generalization error decreases relatively slow inverse power law suggest asymptotically finite information gain may important characteristic good query algorithms
report department statistics open university report department computer science university abstract paper examines minimum encoding approaches inference minimum message length mml minimum description length mdl paper written objective providing introduction area describe coding techniques data examine techniques applied perform inference model selection
independent components natural scenes field suggested neurons line edge found primary visual cortex form sparse distributed representation natural scenes responses emerge unsupervised learning algorithm attempts find factorial code independent visual features show nonlinear applied ensemble natural scenes produces sets visual filters oriented filters produced network field addition outputs filters independent possible since network able perform independent components analysis ica compare resulting ica filters associated basis functions filters produced principal components analysis pca filters ica filters distributed outputs natural scenes also receptive fields simple cells visual cortex suggests neurons form informationtheoretic coordinate system images
model selection application data applications often long requiring provide large amounts data necessary using subset relevant variables answer question model current process particular used several model selection techniques logistic regression including stepwise regression occams window markov chain monte carlo model raftery bayesian random searching resulting models largely agree upon subset original variables paper partial data analysis requirement
mdps learning planning representing knowledge multiple temporal scales learning planning representing knowledge multiple levels temporal abstraction key challenges paper develop approach problems based mathematical framework reinforcement learning markov decision processes mdps extend usual notion action include behavior may temporally extended stochastic events examples options include object lunch primitive actions joint options may given priori learned experience may used actions variety planning learning methods theory decision processes applied model consequences options basis planning learning methods using paper develop connections building prior work others main novel results interface mdp levels analysis show set options changing conditions improve methods additional cost also introduce temporaldifference methods able learn options execution finally propose notion used improve options overall argue options models provide missing aspects powerful clear expressive framework representing knowledge
quantitative study experimental evaluations neural network learning algorithms current research practice neural network learning algorithms published examined amount experimental evaluation contain employ even single realistic real learning problem present results one problem using real world data furthermore one third present quantitative comparison previously known algorithm results suggest better assessment neural network learning algorithm research longterm benefit field raised respect easily benchmark problems built
role development genetic algorithms technical report number computer science engineering abstract developmental mechanisms forms typically genetic algorithms gas two representational spaces identical argue analysis developmental mechanisms useful understanding success several standard techniques relationships recently proposed provide framework two developmental mechanisms learning also showing several common effects search framework used analyze local search change dynamics observe contexts local search incorporated fitness evaluation illustrate reasons considering identify contexts local search distinguished fitness evaluation
genetic algorithm tutorial technical report revised november
learning monitoring strategies difficult genetic programming application finding optimal least good monitoring strategies important designing agent applied genetic programming task mixed results since agent control language general set monitoring strategies small part overall space possible behaviors often difficult genetic algorithm evolve even though performance superior results questions easy will genetic programming scale areas applied become complex
data analyses using simulated inductive learning methods decision making tasks require acquisition efficient decision rules noisy data unlike popular methods tasks must characteristics data without clear features data evaluation criteria problem domain experts get simple accurate knowledge noisy data paper describes novel method acquire efficient decision rules data using simulated inductive learning techniques basic ideas method simulated used get effective features data inductive learning used acquire simple decision rules data simulated one genetic algorithm based techniques evaluate generated genetic operations proposed method case study product data acquired rules simpler results direct application inductive learning domain expert easy understand level accuracy compared methods
experimental comparison genetic programming inductive logic programming learning recursive list functions paper experimentally compares three approaches program induction inductive logic programming ilp genetic programming genetic logic programming variant inducing prolog programs methods used induce four simple recursive functions results indicate ilp likely induce correct program small sets random examples generally less accurate performs worst able induce correct program interpretations results terms differences search methods inductive biases presented keywords genetic programming inductive logic programming comparison paper will also submitted workshop inductive logic programming
relevant two major problems casebased reasoning efficient retrieval source cases adaptation retrieved solutions conditions target analogical theorem proving induction describe abstraction restrict retrieval source cases mapping source problem target problem determine adapt source solution
structural similarity adaptation commonly casebased reasoning applied domains attribute value representations cases sufficient represent features relevant support classification diagnosis design tasks distance functions like transformation similarity functions applied retrieve past cases used generate solution actual problem often domain knowledge available adapt past solutions new problems evaluate solutions however domains like design law structural case representations corresponding structural similarity functions needed often acquisition adaptation knowledge seems impossible rather requires effort applications despite humans use cases main source generate adapted solutions achieve computationally paper presents general approach structural similarity assessment adaptation approach allows explore structural case representations limited domain knowledge support design tasks three modules design assistant generates adapted design solutions basis prior
natural gradient descent training multilayer perceptrons main difficulty implementing natural gradient learning rule compute inverse fisher information matrix input dimension large found new scheme represent fisher information matrix based scheme designed algorithm compute inverse fisher information matrix input dimension much larger number hidden neurons complexity algorithm order complexity conventional algorithms purpose order simulation robustness natural gradient learning rule
acquiring case adaptation knowledge hybrid approach ability casebased reasoning cbr systems apply cases novel situations depends case adaptation knowledge however cbr systems adequate adaptation knowledge proven difficult task paper describes hybrid method performing case adaptation using combination rulebased casebased reasoning shows approach provides framework acquiring flexible adaptation knowledge experiences autonomous adaptation suggests potential basis acquisition adaptation knowledge interactive user guidance also presents initial experimental results examining benefits approach comparing relative contributions case learning adaptation learning reasoning performance
learning systems paper discusses traditional reinforcement learning methods algorithms applied models result poor performance dynamic multiagent domains characterized multiple goals noisy perception action reinforcement propose methodology designing representation functions take advantage implicit domain knowledge order accelerate learning domains demonstrate experimentally two different mobile robot domains
learning problemsolving concepts problem solving learning problem solving related problem solving determines knowledge requirements reasoner learning must learning enables improved problemsolving performance different models problem solving however recognize different knowledge needs result set different learning tasks recent models analyze problem solving terms generic tasks methods models require learning problemsolving concepts new tasks new task view process learning problemsolving concepts paper identify learning issues raised framework problem solving view problem solver abstract device represent works terms model specifies knowledge reasoning problem solver results tasks describe model enables modelbased enables reasoner adapt task structure produce solutions better quality system illustrates process
supporting combined human machine planning interface planning analogical reasoning realistic complex planning situations require mixedinitiative planning framework human automated planners interact construct desired plan joint cooperation potential achieving better plans either human machine create alone human planners often take casebased approach planning past experience planning retrieving adapting past planning cases planning analogical reasoning generative casebased planning combined provides suitable framework study mixedinitiative integration however human user planning loop creates variety new research questions challenges found creating mixedinitiative planning system fall three categories planning paradigms differ human machine planning visualization plan planning process complex necessary task human users range across spectrum experience respect planning domain underlying planning technology paper presents approach three problems designing interface incorporate human process planning analogical reasoning interface allows user follow generative casebased planning supports visualization plan planning rationale addresses variance experience user allowing user control presentation information
evolutionary programming evolution strategies similarities differences evolutionary programming evolution strategies rather similar class probabilistic optimization algorithms model evolution discussed compared respect similarities differences basic components performance experimental runs theoretical results global convergence step size control convex quadratic function extension convergence rate theory evolution strategies presented discussed respect implications evolutionary programming
game theoretic approach moving horizon control control law constructed linear time varying system solving two player zero sum differential game moving horizon game used construct controller finite horizon conditions given controller results stable system satisfies infinite horizon norm bound risk sensitive formulation used provide state estimator observation feedback case
genetic algorithms multiparent recombination paper investigate genetic algorithms two parents involved recombination operation particular introduce gene reproduction mechanism generalizes classical crossover uniform crossover applicable arbitrary number two parents performed extensive tests optimizing numerical functions graph coloring observe effect different numbers parents experiments show recombination outperformed using parents classical functions problems results cases parents optimal others parents better
genetics back genetic algorithms
method combining multiple probabilistic classifiers soft competition different feature sets novel method proposed combining multiple probabilistic classifiers different feature sets order achieve improved classification performance generalized finite mixture model proposed linear combination scheme implemented based radial basis function networks linear combination scheme soft competition different feature sets automatic feature mechanism different feature sets always simultaneously used optimal way determine linear combination weights training linear combination scheme learning algorithm developed based expectationmaximization algorithm proposed method applied typical real world problem identification different feature sets often need simultaneously robustness simulation results show proposed method yields good performance identification
adapting evaluation space improve global learning
towards general distributed learning generalization word perfect introduction different learning models employ different styles generalization novel inputs paper proposes need multiple styles generalization support broad application base priority asocs model priority adaptive selforganizing concurrent system presented potential support multiple generalization styles adaptive network composed many simple computing elements operating parallel operate either data processing mode learning mode data processing mode system acts parallel hardware circuit learning mode incorporates rules represent application learned learning accomplished distributed fashion time logarithmic number rules new model significant learning time space complexity improvements previous models generalization learning system best always proper style generalization application dependent thus one style generalization may sufficient allow learning system support broad spectrum applications current connectionist models use one specific style generalization implicit learning algorithm suggest type generalization used selforganizing parameter learning system discovered learning takes place requires model allows flexible generalization styles mechanisms guide system best style generalization problem learned paper learning model seeks efficiently support requirement model called priority asocs class models called asocs adaptive selforganizing concurrent systems section paper gives example different generalization techniques approach problem section presents overview section illustrates flexible generalization supported section concludes paper
new approach model selection introduce new approach model selection performs better standard error estimation techniques many cases basic idea exploit intrinsic metric structure hypothesis space determined natural distribution training patterns use metric reference detect whether empirical error estimates derived small labeled training sample region around empirically optimal hypothesis using simple metric develop new geometric strategies detecting overfitting performing robust yet model selection spaces candidate functions new strategies dramatically outperform previous approaches experimental studies classical polynomial curve fitting moreover technique simple efficient applied function learning tasks requirement access auxiliary collection training data
using realvalued genetic algorithms evolve rule sets classification paper use genetic algorithm evolve set classification rules realvalued attributes show realvalued attribute encoded realvalued genes present new uniform method representing rules view supervised classification optimization problem evolve rule sets maximize number correct classifications input instances use variant approach machine learning system novel conflict resolution mechanism competing rules within rule set experimental results demonstrate effectiveness proposed approach benchmark classifier system
knowledgebased genetic learning genetic algorithms proven powerful tool within area machine learning however classes problems seem applicable solution given problem consists several parts influence case genetic operators crossover mutation work thus good performance paper describes approach overcome problem using highlevel genetic operators integrating task specific domain independent knowledge guide use operators advantages approach shown learning rule base adapt parameters image processing operator path within solution system
learning multiple descriptions concept sparse data environments greater classification accuracy achieved learning several concept descriptions data combining classifications stochastic search general tool used generate many good concept descriptions rule sets class data bayesian probability theory offers optimal strategy combining classifications individual concept descriptions use approximation theory strategy useful additional data difficult obtain every increase classification accuracy important primary result paper multiple concept descriptions particularly helpful flat hypothesis spaces many equally good ways grow rule similar gain another result experimental evidence learning multiple rule sets yields accurate classifications learning multiple rules domains demonstrate behaviors learn multiple concept descriptions adapting noisetolerant relational learning algorithm
decision tree computer science department university massachusetts technical report
reinforcement learning paper present novel multiagent learning paradigm called reinforcement learning introduces concept using features generalize state space work use learned feature space effective technique allow team agents learn towards specific goal adaptation traditional methods applicable complex multiagent domains large state spaces limited training opportunities multiagent scenarios team members always full communication one another may affect environment hence learner rely knowledge future state transitions world enables teams agents learn effective policies training examples even face large state space large amounts hidden state main responsible features learning task among team members using coarse feature space allowing agents reinforcement directly observation environment fully implemented tested robotic soccer domain complex multiagent framework paper presents algorithmic details empirical results demonstrating effectiveness developed multiagent learning approach learned features
using multiple node types improve performance dynamic multilayer perceptron paper discusses method training multilayer perceptron networks called dynamic multilayer perceptron method based upon approach builds networks form binary trees dynamically nodes layers needed focus paper effects using multiple node types within framework simulation results show performs favorably comparison learning algorithms using multiple node types network performance
entailment specification refinement specification refinement part formal program derivation method software directly constructed provably correct specification program derivation used critical software systems automated approach allow many types software systems goal research determine genetic programming used specification refinement process initial steps toward goal show wellknown proof logic program derivation encoded system infer sentences logic proof particular sentence results promising indicate useful derivation
type strongly genetic programming paper appears chapter peter advances genetic programming mit press abstract genetic programming automatic method generating computer programs stored data structures evolve better programs extension search space strongly genetic programming basic arguments return values functions also set restriction two levels extend allowing type hierarchy allows two levels
collective adaptation simple process agents integrated distributed search genetic programming based systems collective memory form collective adaptation search method system significantly improves search problem complexity increased however still considerable scope improvement collective adaptation search agents knowledge environment central information repository process agents able focused knowledge exploiting exploration search agents examine utility increasing capabilities agents
cbr integration casebased reasoning cbr integration constraint satisfaction problem formalism several transformations initial research idea design research results interesting insights realworld applicability integrated methodology integration explored alternative paper alternative results described
concept learning problem small
unbiased assessment learning algorithms order performance machine learning algorithms many researchers conduct experiments benchmark data sets since learning algorithms parameters popular adapt parameters obtain minimal error rate test set rate used algorithm causes bias quantify bias showing particular algorithm parameters will probably higher equally good algorithm fewer parameters demonstrate result showing number parameters trials required order outperform c45 foil respectively various benchmark problems describe unbiased ranking experiments conducted
exploring decision empirical investigation occams razor decision tree induction report series experiments decision trees consistent training data constructed experiments run gain understanding properties set consistent decision trees factors affect accuracy individual trees particular investigated relationship size decision tree consistent training data accuracy tree test data experiments performed massively parallel computer results experiments several artificial two real world problems indicate many problems investigated smaller consistent decision trees average less accurate average accuracy slightly larger trees
empirical evaluation bagging boosting ensemble consists set independently trained classifiers neural networks decision trees whose predictions combined classifying novel instances previous research shown ensemble whole often accurate single classifiers ensemble bagging boosting freund schapire two relatively new popular methods producing ensembles paper evaluate methods using neural networks decision trees classification algorithms results clearly show two important facts first even though bagging almost always produces better classifier individual component classifiers relatively overfitting generalize better baseline neuralnetwork ensemble method second boosting powerful technique usually produce better ensembles bagging however noise quickly data set
pruning averaging decision trees pruning decision tree considered researchers important part tree building noisy domains many approaches pruning alternative approach averaging decision trees received much attention perform empirical comparison pruning approach averaging decision trees comparison use computationally efficient method averaging namely averaging extended set tree since wide range approaches pruning compare tree averaging traditional pruning approach along optimal pruning approach
hidden markov modeling simultaneously cells associative cortex widely held idea regarding information processing brain hypothesis suggested according hypothesis basic unit information processing brain assembly cells act briefly closed system response specific stimulus work presents novel method characterizing activity using hidden markov model model able underlying cortical network activity behavioral processes study process hand simultaneous activity several cells cortex using model able identify behavioral mode animal directly identify corresponding collective network activity furthermore segmentation data discrete states also provides direct evidence state dependency correlation functions pair cells thus depends network state activity local connectivity alone
model selection model uncertainty graphical models using occams window consider problem model selection model uncertainty highdimensional tables motivated expert system applications approach used currently stepwise strategy guided tests based approximate asymptotic values leading selection single model inference conditional selected model sampling properties strategy complex failure take account model uncertainty leads uncertainty quantities interest principle provided standard bayesian formalism averages posterior distributions interest models weighted posterior model probabilities furthermore approach optimal sense predictive ability however used practice computing posterior model probabilities hard number models large often greater argue standard bayesian formalism propose alternative bayesian approach takes full account true model uncertainty averaging much smaller set models efficient search algorithm developed finding models consider two classes graphical models arise expert systems recursive causal models david assistant professor statistics adrian raftery professor statistics department statistics university washington seattle research partially supported graduate school research university washington nsf research supported onr contract authors grateful david york two helpful comments discussions michael providing data example
bayesian graphical models discrete data research supported nsf graduate fellowship authors grateful david david adrian raftery helpful comments discussions
categorical perception facial classification present automated recognition system capable identifying six basic novel face images ensemble simple feedforward neural networks used rate images outputs networks combined generate score networks trained database face images human subjects consistently single system achieves generalization novel face images individuals networks trained drawn database neural network model exhibits categorical perception pairs linear sequence images created two expressions individuals face sequence analyzed model transitions output response vector occur single step sequence pairs others plan models response limit direct testing determining human subjects exhibit categorical perception image sequences
blind separation real world signals using mixtures discuss advantages using mixtures improve upon blind source separation algorithms designed extract sound sources mixtures study nature responses helps choose adaptive filter architecture use acquired responses compare effectiveness separating filter configurations various filter using blind algorithm show adding additional sensors improve upon separation signals mixed real world filters
producing comprehensible models performance minimum description length mdl principle adapted handle continuous attributes inductive logic programming setting application developed coding mdl pruning mechanism behavior mdl pruning tested synthetic domain added noise different levels two real life problems modelling surface modelling compounds results indicate mdl pruning successful noise tool domains since acts building complex models accuracy model
blind separation delayed sources address difficult problem separating multiple multiple real combine work give natural gradient information rules recurrent iir networks separating mixed signals work simulated data rules fail real usually involve phase transfer functions using stable iir filters approach problem perform feedforward architecture frequency domain demonstrate separation two natural signals using approach
multiplicative updating rule blind separation derived method scoring blind source separation fisher information matrix used metric parameter space descent algorithm maximize likelihood function parameter space becomes serial updating rule property algorithm simplified using asymptotic form fisher information matrix around equilibrium
efficiency robustness natural gradient descent learning rule discovered new scheme represent fisher information matrix stochastic multilayer perceptron based scheme designed algorithm compute inverse fisher information matrix input dimension much larger number hidden neurons complexity algorithm order complexity conventional algorithms purpose order inverse fisher information matrix used natural gradient descent algorithm train multilayer perceptrons simulation natural gradient
lazy acquisition place knowledge paper define task place learning describe one approach problem framework represents distinct evidence probabilistic description place recognition relies nearest neighbor classification augmented process correct differences two learning mechanism lazy involves simple storage inferred evidence experimental studies physical simulated robots suggest approach improves place recognition experience handle significant sensor noise benefits improved quality stored cases scales environments many distinct additional studies suggest using historical information robots path environment actually reduce recognition accuracy previous researchers studied evidence place learning combined two powerful concepts used systematic experimentation evaluate methods abilities
evolution nondeterministic incremental algorithms new approach search state spaces let call nondeterministic incremental algorithm one able construct solution combinatorial problem selecting incrementally ordered sequence choices defines solution choice made case state space represented tree solution path root tree paper describes simulated evolution population nondeterministic incremental algorithms offers new approach exploration state space compared techniques like genetic algorithms evolutionary strategies hill climbing particular efficiency method implemented evolving end model presented sorting network problem reference problem computer science show end model drawbacks optimization techniques even outperforms problem indeed input sorting networks good best known built scratch even result input problem improved one
perceptual reversal neural network model vision visor neural network system object recognition scene analysis learns visual schemas examples processing visor based cooperation competition parallel bottomup topdown activation schema representations similar principles appear much human visual processing visor therefore used model various perceptual phenomena paper focuses analyzing three phenomena simulation visor mental perceptual reversal results illustrate similarity differences mechanisms mental show two accounts perceptual reversal neural cognitive factors may contribute phenomenon demonstrate actions gradually learned actions successful simulation effects suggests similar mechanisms may human visual perception learning visual schemas
visor scene analysis structured neural networks novel approach object recognition scene analysis based neural network representation visual schemas described given input scene visor system focuses attention component schema representations match inputs schema hierarchy learned examples unsupervised adaptation reinforcement learning visor learns objects important others identifying scene importance spatial relations varies depending scene inputs differ schemas recognition process robust automatically generates measure confidence analysis
constructive training methods feedforward neural networks binary weights dimacs technical report august
using genetic algorithm learn behaviors autonomous vehicles autonomous vehicles will require planning reactive components order perform components needed longterm planning explicit reasoning future states required reactive components allow system always action available realtime exhibit robust behavior lack ability reason future states long time period work addresses problem creating reactive components autonomous vehicles creating reactive behaviors rules generally difficult requiring acquisition much knowledge domain experts problem knowledge acquisition system learns reactive behaviors autonomous agents learns behaviors simulation process creating rules therefore reducing learning algorithm designed learn useful behaviors simulations limited current work investigating behaviors learned simulation environments work real world environments paper describe describe behaviors learned simulated autonomous aircraft autonomous vehicles robots behaviors include track ing navigation avoidance
backpropagation perceptrons feedforward nets sigmoidal activation functions often designed minimizing cost criterion technique may outperformed classical perceptron learning rule least problems paper show arise error criterion threshold type zero values beyond desired target values precisely show data linearly separable one considers nets hidden neurons error function local minima global simulations networks hidden units consistent results often data classified minimizing threshold criterion may fail classified using instead simple cost addition proof gives following stronger result hypotheses continuous gradient procedure initial weight configuration separating set weights obtained finite time precise perceptron learning theorem results compared classical pattern recognition problem threshold linear activations local minima exist even data shown even using threshold criterion local minima may occur data separable used
modelling risk disease time space paper combines existing models spatial data hierarchical bayesian framework particular emphasis role time effects data analysis implemented via markov chain monte carlo methods methodology illustrated cancer data two approaches spatial particularly described first includes random effects model account second adds simple measure behaviour dataset particular interest state may caused increased levels cancer however data inadequate proper investigation issue email
benchmark classifier learning although many algorithms learning examples developed many comparisons reported generally accepted benchmark classifier learning existence standard benchmark greatly assist comparisons dimensions proposed describe classification tasks based realworld synthetic datasets chosen set covering method repository machine learning databases form benchmark
schema theorem prices theorem schema theorem widely taken foundation explanations power genetic algorithms gas yet expressed implications arguments reviewed upon explaining schema theorem implications performing interpretations schema theorem implicitly assumed correlation exists assumption made explicit results based prices covariance selection theorem schemata play part performance theorems derived representations operators general however schemata recombination operators used using recombination distribution representation recombination operators missing schema theorem derived makes explicit perform finally method adaptive landscape analysis examined commonly used correlation instead alternative transmission function fitness domain proposed optimal estimating performance limited samples
independent component analysis simulated eeg using model report supported part medical research development office naval research department work unit views expressed article authors reflect policy position department department defense government distribution
finding accurate approach relational learning approach analytic learning described searches accurate horn clause domain theory hillclimbing search guided information based evaluation function performed applying set operators derive domain theories analytic learning system one component multistrategy relational learning system compare accuracy concepts learned analytic strategy concepts learned analytic strategy domain theory
learned information filters system learns filter documents will suffer poor performance initial training phase one way addressing problem exploit filters learned users fashion investigate direct transfer learned filters limiting case learning system evaluate stability several different learning methods direct transfer conclude symbolic learning methods use correlated features data perform poorly transfer even perform conventional evaluation settings effect robust holds several learning methods diverse set users used training classifier even learned classifiers adapted new users distribution experiments give rise several concrete improving generalization performance setting including variation feature selection method widely used text categorization
evolving neural networks species present coevolutionary architecture solving problems apply evolution artificial neural networks although work preliminary nature number advantages approaches coevolutionary approach utilizes divideandconquer technique species representing simpler evolved separate instances genetic algorithm executing parallel among species formed representing complete solutions species created dynamically needed results presented coevolutionary architecture produces higher quality solutions fewer evolutionary trials compared alternative non coevolutionary approach problem evolving networks parity computation
automated classification dna structure sequence information keywords dna entropy estimation dna introduce algorithm combines simple pattern general method estimating entropy sequence pattern exploits partial match build model sequence since primary features interest biological sequence domains small variations exact particularly suited domains describe two methods use entropy estimate perform maximum classification apply methods several problems structure classification short dna sequences results include email email email
using partitioning speed rule induction rise press rule induction algorithm gradually generalizing rules starting one rule per example several advantages compared common strategy gradually specializing initially rules shown lead significant accuracy gains algorithms like cn2 large number application domains however running time like rule induction algorithms quadratic number examples making processing large databases paper studies use partitioning speed rise compares wellknown method use partitioning induction setting creates possible system partitioning often reduces running time improves accuracy time noisy conditions performance rapidly partitioning remains stable
artificial life model investigating evolution investigate issue nature present artificial life model allow computer organisms robots genotype system sensory motor environment organisms behave simulations neural networks trained control mobile robot designed keep clear objects evolutionary process modular neural networks control robots behavior emerge result genetic preliminary simulation results show modular architecture outperforms architecture represents starting architecture simulations moreover interaction mutation rate results future goal use model order explore relationship evolutionary emergence phenomenon gene
differential theory learning efficient neural network pattern recognition describe new theory differential learning broad family pattern classifiers including many wellknown neural network paradigms learn stochastic concepts efficiently describe relationship classifiers ability generalize unseen test examples efficiency strategy learns list series differential learning efficient information computational resource requirements whereas traditional probabilistic learning strategies illustrated simple example analysis conclude optical character recognition task three different types generated classifiers generalize significantly better generated
constructive induction approach neural networks machine learning methods given knowledge representation space inadequate learning process will fail also true methods using neural networks form representation space overcome limitation automatic construction method neural network proposed paper describes method constructive induction neural network trained backpropagation algorithm method searches better representation space analyzing hypotheses generated step iterative learning process method applied ten problems include particular inverse problems problems successfully solved initial set parameters extension representation space necessary extension problem
estimating accuracy learned concepts paper investigates alternative estimators accuracy concepts learned examples particular crossvalidation bootstrap estimators studied using synthetic training data foil learning algorithm experimental results previous papers statistics bootstrap method superior crossvalidation nevertheless results also suggest conclusions based crossvalidation previous machine learning papers specifically observations true error concept learned foil independently drawn sets examples concept varies widely estimate true error provided crossvalidation high variability approximately unbiased iii bootstrap estimator lower variability crossvalidation systematically biased
towards bayesian automated problem driving autonomous vehicle normal traffic many areas research substantial significance describe work progress new approach problem uses decisiontheoretic architecture using dynamic probabilistic networks architecture provides sound solution problems sensor noise sensor failure uncertainty behavior vehicles effects ones actions report advances theory inference decision making dynamic partially observable domains approach implemented simulation system autonomous vehicle successfully variety difficult situations
contextsensitive learning methods text categorization two recently implemented machine learning algorithms experts evaluated number large text categorization problems algorithms construct classifiers allow context word affect even whether presence absence will contribute classification however experts differ many differences include different notions context different ways combining contexts construct classifier different methods search combination contexts different criteria contexts included combination differences experts perform extremely across wide variety categorization problems generally previously applied learning methods view result usefulness classifiers represent contextual information
automatic parameter selection minimizing estimated error address problem finding parameter settings will result optimal performance given learning algorithm using particular dataset training data describe method considering determination best parameters discrete function optimization problem method uses search crossvalidation around basic induction algorithm search explores space parameter values running basic algorithm many times training sets produced crossvalidation get estimate expected error parameter setting thus final selected parameter settings specific induction algorithm dataset studied report experiments method datasets selected using c45 basic induction algorithm confidence level method improves performance c45 domains performance one statistically c45 sample datasets used comparison method yields average relative decrease error rate expect see similar performance improvements using method machine learning
beyond correlation artificial intelligence events data feature vector offers environment exploratory data analysis several empirical studies applied environment international conflict management dataset current analysis techniques include boolean analysis temporal analysis automatic rule learning implemented common common interface system features advanced interface makes intuitive people data discover significant relationships system data within objects defines generic interactions data users analysis algorithms generic data make possible rapid integration new datasets new analytical algorithms heterogeneous data sophisticated research conflict suitable processing semantic representations natural language system experiments cases demonstrated feasibility building knowledge bases synthetic pages
inputoutput analysis feedback loops saturation
sources increased accuracy two proposed boosting algorithms introduce two boosting algorithms aim increase generalization accuracy given classifier incorporating component algorithms construct complementary classifier generate coarse hypotheses training data show two algorithms generalization accuracy representative collection data sets two algorithms distinguished one modifies class targets selected training instances order train complementary classifier show two algorithms achieve approximately equal generalization accuracy create complementary classifiers display different degrees accuracy diversity study provides evidence may useful investigate families boosting algorithms incorporate varying levels accuracy diversity achieve appropriate given task domain
object localization applications many areas engineering science goal spatially object many applications desirable minimize number measurements collected purpose sufficient localization accuracy example large number localization measurements may either extend time required perform procedure increase patient localization accuracy function spatial distribution discrete measurements object measurement noise present metrics presented evaluate information available set discrete object measurements study new approaches discrete point data selection problem described include hillclimbing genetic algorithms gas incremental learning extensions standard methods employ multiple parallel populations explored results extensive empirical testing provided results suggest combination hillclimbing result best overall performance system incorporates methods presented paper currently evaluated trials methods selecting point data supported national science foundation graduate student fellowship graduate student fellowship national space space center david partially supported national science foundation national challenge grant award object localization applications
robust relational learner research reported paper describes ilp system uses search heuristic based statistical correlation several interesting properties heuristic discussed shown naturally extended simple powerful stopping criterion independent number training examples instead stopping criterion depends search heuristic estimates utility literals uniform scale comparison foil domain data outline ideas topdown pruning present preliminary results
heuristics models uncertainty psychological evidence shows probability theory proper model intuitive human instead heuristics proposed model paper argues probability theory even model new model uncertainty designed assumption systems knowledge resources insufficient respect questions system needs answer proposed heuristics human reasoning also observed new model according assumption
evolution algorithms image enhancement interactive genetic programming technical report school computer science university abstract paper present approach interactive development programs image enhancement genetic programming based transformations approach user individual selection presence user allow running without fitness function also efficient search procedure capable producing effective solutions problems evaluations paper also propose strategy reduce user interaction choices made user interactive runs later use build model replace longer runs experimental results interactive strategy also reported
functional theory creative reading reading area human cognition studied researchers artificial intelligence researchers yet still exist theory accurately describes complete process believe past attempts short due incomplete understanding overall task reading namely complete set mental tasks reasoner must perform mechanisms tasks present functional theory reading process argue represents coverage task theory combines experimental results psychology artificial intelligence along insights gained research greater understanding mental tasks necessary reading will enable new natural language understanding systems flexible capable earlier ones furthermore argue creativity necessary component reading process must considered theory system attempting describe present functional theory creative reading novel knowledge organization scheme supports creativity mechanisms reading theory currently implemented integrated story analysis creativity system computer system science stories paper part institute technology college computing technical report series
genetic programming parallel implementation parallel
theory practice vector trained small training sets examine performance vector changes function training set size specifically study training set distortion predicts test distortion training set randomly drawn subset blocks test training images using vapnikchervonenkis dimension derive formal bounds difference test training distortion vector describe extensive empirical simulations test bounds variety bit rates vector dimensions give practical suggestions determining training set size necessary achieve good generalization conclude using training sets small fraction available data one produce results close results available data used
finite gain linear systems subject input saturation paper deals global inputoutput stabilization linear systems controls stable systems shown linear feedback law suggested approach indeed provides stability respect every norm explicit bounds gains obtained related systems without saturation results extend class systems state matrix size jordan blocks may expected fact systems globally asymptotically statespace sense shown particular
global stabilization linear discretetime systems bounded feedback paper deals problem global stabilization linear discrete time systems means bounded feedback laws main result proved analog one proved continuous time case authors shows stabilization possible system arbitrary controls transition matrix spectral less equal one proof provides principle algorithm construction feedback laws implemented either parallel connections single hidden layer neural networks simple saturation functions
separation two sets npcomplete problem determining whether two point sets real space separated two program minimizing product two linear functions set program solution iterative linear programming algorithm finite number steps point necessary optimality condition global minimum encouraging computational experience number test problems reported
feature selection via mathematical programming problem two finite point sets feature space separating plane utilizes features possible formulated mathematical program parametric objective function linear constraints step function appears objective function approximated sigmoid exponential real line treated exactly considering equivalent linear program equilibrium constraints lpec computational tests three approaches available realworld databases carried compared adaptation optimal brain damage method reducing neural network complexity one feature selection algorithm via minimization reduced crossvalidation error cancer database reducing problem features feature selection important problem machine learning basic form problem consists eliminating many features given problem possible still task acceptable accuracy minimal number features often leads better generalization simpler models easily interpreted present work task discriminate two given sets feature space using given features possible formulate problem mathematical program parametric objective function will attempt achieve task generating separating plane feature space small dimension possible minimizing average distance points plane one computational experiments carried feature selection procedure showed effectiveness minimizing number features selected also quickly recognizing removing random features introduced thus breast cancer database feature space dimensions random features added one algorithms removed random features original features resulting separating plane dimensional reduced feature space using crossvalidation separation error dimensional space reduced corresponding error original problem space see section details note mathematical programming approaches feature selection problem recently proposed even though approach based lpec formulation lpec method solution different ones used minimization approach involved theoretical one specific algorithm results given effective computational applications mathematical programming neural networks given
learning contextfree grammars capabilities limitations recurrent neural network external stack memory work describes approach deterministic contextfree grammars connectionist paradigm using recurrent neural network automaton consists recurrent neural network connected external stack memory common error function show able learn dynamics underlying automaton examples grammatical strings network learn state transitions automaton also learns actions required control stack order use continuous optimization methods develop analog stack discrete stack quantization activations network learned transition rules stack actions show enhancement networks learning capabilities providing addition initial comparative study simulations first second third order recurrent networks shown increased degree freedom higher order networks improve generalization necessarily learning speed
genetic algorithm genetic algorithm robust problemsolving method based natural selection speed advantage ability offer great rewards genetic speedups orders magnitude observed frequently used software routines hardware way generalpurpose engine certain modules require function thus desirable fully functional genetic algorithm presented system designed using allow easy designed act user programs implement function optimized parameters may also specified user simulation results performance analyses presented prototype compared similar implemented soft simple tests prototype many cycles run suggested improvements make orders magnitude faster
factorial hidden markov models one basic probabilistic tools used time series modeling hidden markov model hmm hmm information past time series single discrete hidden state present generalization hmms state multiple state variables therefore represented distributed manner inference learning model depend computing posterior probabilities hidden state variables given observations present exact algorithm inference model relate algorithm hmms algorithms general belief networks due combinatorial nature hidden state representation exact algorithm intractable intractable systems approximate inference carried using gibbs sampling mean field theory also present structured approximation state variables based derive tractable learning algorithm empirical comparisons suggest approximations efficient accurate alternatives exact methods finally use structured approximation model show outperforms hmms complex temporal patterns dataset
exploiting tractable intractable networks develop refined mean field approximation inference learning probabilistic neural networks mean field theory unlike assume units behave independent degrees freedom instead exploits principled way existence large computationally tractable illustrate advantages framework show incorporate weak higher order interactions firstorder hidden markov model first order structure within mean field theory
bayesian estimation introduction chapter takes different address problem learning will reason terms probability make extensive use chain rule known bayes rule fast definition probability provided reference chapter review methods bayesian learning applied modelling purposes original analyses comments also provided section latent bayesian statistics means kind methods long sound provide good results applied learning tasks applies two presented object past years neural networks community will take side present strong points weaknesses context work bayesian especially interesting provide continuous update rules used cost yield automatic selection regularisation level unlike methods presented chapter necessary try several regularisation levels perform many bayesian framework one training achieved procedure
theory learning classification rules
bitsback coding software guide abstract document first review theory behind bitsback coding free energy coding hinton describe interface software used bitsback coding method new approach problem optimal compression source code produces multiple given symbol may seem codeword use case shortest one however proposed bitsback approach random codeword selection yields effective codeword length less shortest codeword length random choices boltzmann distributed effective length optimal given source code software describe guide easy use source code pages long illustrate bitsback coding software simple gaussian mixture problem
constructive induction data experiments
using recurrent neural networks learn structure interconnection networks modified recurrent neural network used learn interconnection network set routing examples modified several distinct initial states equivalent single learning multiple different sequential machines define sequential machine structure augmented show essentially augmented sequential machine example learn small training extract networks internal representation corresponding paper adapted chapter version paper published
representation requirements supporting decision model formulation paper outlines methodology analyzing representational support knowledgebased broad domain relevant set inference patterns knowledge types identified comparing analysis results existing representations insights gained design approach integrating categorical uncertain knowledge context sensitive manner
computational utility propose computational framework understanding modeling human framework integrates many existing theoretical yet sufficiently concrete allow simulation experiments attempt explain experience instead differences exist within cognitive information processing system information versus information central idea explore correspond temporally states network computational modules three simulations described behavior states models corresponds roughly behavior states people experience performing similar tasks simulations show states improves performance noise decisions keep system track toward solution
examples random source discuss two types algorithms selecting relevant examples developed context computation learning theory examples selected stream examples generated independently random first two algorithms socalled boosting algorithms schapire schapire freund freund algorithm describe algorithms proven properties point suggest possible future implications
inductive logic programming paper traces development main ideas led present state knowledge inductive logic programming story begins research psychology subject human concept learning results research influenced early artificial intelligence combined formal methods inductive inference evolve present inductive logic programming inductive logic programming often considered young however research back nearly years paper traces development ideas beginning psychology effect concept learning research artificial intelligence independent requirement psychological basis formal methods inductive inference developed separate rise inductive logic programming account entirely unbiased attention given work researchers influenced interest machine learning paper attempt describe recent developments ilp account includes research prior term inductive logic programming first used reason major paper taken periods evolution life
rule revision recurrent neural networks recurrent neural networks process recognize generate temporal sequences encoding grammatical strings temporal sequences recurrent neural networks trained behave like deterministic sequential finitestate automata algorithms developed extracting grammatical rules trained networks using simple method prior knowledge rules recurrent neural networks show recurrent neural networks able perform rule revision rule revision performed comparing rules rules finitestate automata extracted trained networks results training recurrent neural network recognize known nontrivial randomly generated regular grammar show networks preserve correct rules able correct training rules initially incorrect incorrect mean rules ones randomly generated grammar
multiparent recombination section survey recombination operators apply two parents create multiparent recombination operators defined fixed number parents three operators number parents random number might greater two yet operators parameter set arbitrary integer number special attention latter type operators summarize results effect operator performance
backpropagation networks weights created equal approach yields performance backpropagation learning algorithms typically networks structure single vector weight parameters optimized suggest performance may improved utilizing structural information instead introduce framework weight model activation error signals treated approximately independent random variables characteristic scale weight changes matched allowing structural properties nodes affect local learning rate error model also permits calculation upper bound global learning rate batch updates turn leads different update rules bias weights
discovering representation space transformations learning concept descriptions combining dnf mofn rules paper addresses class learning problems require construction descriptions combine mofn rules traditional disjunctive normal form dnf rules presented method learns descriptions call conditional mofn rules using constructive induction approach approach representation space modified according patterns discovered iteratively generated hypotheses need mofn rules detected observing equivalence patterns hypotheses patterns indicate symmetry relations among pairs attributes attributes combined maximal symmetry classes symmetry class method constructs attribute adds new dimension representation space search hypothesis iteratively modified representation spaces done standard inductive rule learning algorithm shown proposed method capable solving problems difficult traditional symbolic learning methods
length conversational case scoring inferences conversational casebased reasoning ccbr approach embedded cbr content line products bias case scoring algorithm particular cases tend given higher assuming factors held constant report summarizes investigation bias introduce approach eliminating bias evaluate retrieval performance six case libraries also suggest explanations results note limitations study
stochastic hillclimbing baseline method evaluating genetic algorithms investigate effectiveness stochastic hillclimbing baseline evaluating performance genetic algorithms gas combinatorial function particular address four problems gas applied literature maximum cut problem problem multiprocessor document allocation problem jobshop problem demonstrate simple stochastic hillclimbing methods able achieve results comparable superior obtained gas designed address four problems illustrate case jobshop problem insights obtained formulation stochastic hillclimbing algorithm lead improvements encoding used department computer science university california berkeley supported graduate fellowship paper written author email department university california berkeley supported graduate fellowship email
concept sharing means improve learning paper describes several means sharing related concepts improve learning domain sharing comes form possibly entire structures previous concepts may aid learning concepts useful information domain using two domains evaluate effectiveness concept sharing respect accuracy concept size search complexity noise
abstract parallel genetic algorithm graph partitioning problem parallel algorithm shows genetic algorithms stochastic search optimization techniques used wide range applications paper addresses application genetic algorithms graph partitioning problem standard genetic algorithms large populations suffer lack efficiency quite high execution time massively parallel genetic algorithm proposed implementation results various given comparative analysis approach hillclimbing algorithms simulated annealing also presented experimental measures show algorithm gives better results concerning quality solution time needed reach
improving accuracy speed support vector machines support vector learning machines finding application pattern recognition regression estimation operator problems general methods improving generalization performance improving speed test phase increasing interest paper combine two techniques pattern recognition problem method improving generalization performance virtual support vector method incorporating known invariances problem method achieves error rate test digit images method improving speed reduced set method approximating support vector decision surface apply method achieve factor speedup test phase virtual support vector machine combined approach yields machine times faster original machine better generalization performance achieving error virtual support vector method applicable problem known invariances reduced set method applicable support vector machine
extracting treestructured representations trained networks significant limitation neural networks representations learn usually humans present novel algorithm extracting comprehensible symbolic representations trained neural networks algorithm uses queries induce decision tree approximates concept represented given network experiments demonstrate able produce decision trees maintain high level networks comprehensible accurate unlike previous work area algorithm general applicability scales large net works problems highdimensional input spaces
theory analyzing relationships among weight evidence degree belief shown special cases belief functions rule used combine belief functions based distinct evidence together lead theory solve problem fundamental postulates theory must new approach uncertainty management introduced many intuitive ideas theory avoiding problem
explanationbased learning explanationbased learning ebl theoretical basis using generalization probably approximately correct pac learning problem solving domains paper two forms explanationbased learning sufficient conditions success two forms ebl called macro serial respectively exhibit two distinct sources power bias solution space analysis shows exponential speedup achieved either biases suitable domain somewhat surprisingly also shows computing necessary obtain speedups theoretical results experiments domain work suggests best way address utility problem ebl implement bias exploits structure set domains one interested learning
incorporating invariances support vector learning machines developed recently support vector learning machines achieve high generalization ability minimizing bound expected test error however far way adding knowledge invariances classification problem hand present method incorporating prior knowledge transformation invariances applying transformations support vectors training critical determining classification boundary
learning complex robotic behaviors paper reports recent results using genetic algorithms learn decision rules complex robot behaviors method involves evaluating rule sets simulator applying simulated evolution evolve effective rules main contributions paper task learned complex behavior involving multiple mobile robots learned rules verified experiments mobile robots case study involves task one mobile robot attempts guide another robot specified area
learning trees rules features learning systems examples represented feature vectors components either real numbers nominal values propose extension representation allows value feature set strings instance represent small nominal features size species feature one might use feature vector since make assumptions number possible set elements extension traditional representation closely connected infinite attribute representation argue many decision tree rule learning algorithms easily extended features also show example many realworld learning problems efficiently naturally represented features particular text categorization problems problems arise firstorder representations features
system training feedforward simple recurrent networks efficiently correctly
generalization mobile robot learning content areas robotics reinforcement learning machine learning mobile robot domain challenges reinforcement learning algorithms difficult problems structural credit assignment uncertainty structural credit assignment particularly domains realtime trial length limiting factor number learning steps physical hardware perform noisy sensors complex dynamic environments learning problem leading situations speed learning policy flexibility may important policy optimality input generalization addresses problems typically time robot domains present two algorithms perform simple fast generalization input space based algorithms trade longterm optimality performance flexibility algorithms tested simulation learning across different numbers steps shown perform better earlier stages learning particularly presence noise trials performed mobile robot subject uncertainty real world simulation results wide margin strongly supporting role generalization strategies noisy realtime mobile robot domains
modeling using state space models time series problems noise two categories dynamic noise process noise added measurement process influence future values system framework empirical squared relative returns prices exhibit significant amount noise model predict time evolution estimate state space models explicitly include noise obtain relaxation times ranging three exchange three five stock indices cases twodimensional hidden state required yield consistent noise compare results ordinary autoregressive models without hidden state find autoregressive models relaxation times two orders magnitude due distinction dynamic noise new interpretation dynamics terms state space model stochastic models models useful several problems including risk management
chess program learns combining minimax search paper present variation algorithm enables used conjunction minimax search present experiments chess program used learn evaluation function playing free chess server improved just games play discuss reasons success also relationship results results
use analogy automated theorem proving technical report artificial intelligence laboratory university
misclassification minimization problem minimizing number points plane attempting separate two point sets convex real space formulated linear program equilibrium constraints lpec general lpec exact penalty problem quadratic objective linear constraints algorithm proposed penalty problem stationary point global solution novel aspects approach include linear formulation step function exact penalty formulation without constraint assumptions iii exact solution extraction sequence penalty function finite value penalty parameter general lpec explicitly exact solution lpec constraints parametric quadratic programming formulation lpec associated misclassification minimization problem
generic model evolving agents
neural network weight updates technical report abstract long known neural networks learn faster input hidden unit activities zero recently extended approach also error signals sejnowski generalize notion factors involved weight update leading propose hidden unit activation functions linear component error improves credit assignment networks connections benchmark results show speed learning significantly without trained networks generalization ability
priority asocs asocs models two significant advantages learning models paper presents asocs adaptive selforganizing concurrent system model massively parallel processing incrementally defined rule systems areas adaptive logic robotics logical inference dynamic control asocs adaptive network composed many simple computing elements operating parallel asocs operate either data processing mode learning mode data processing mode asocs acts parallel hardware circuit learning mode asocs incorporates rule expressed boolean conjunction distributed fashion time logarithmic number rules paper proposes learning algorithm architecture priority asocs new asocs model uses rules new model significant learning time space complexity improvements previous models architectures neural networks traditional computing systems neural networks learn inputoutput mappings using highly distributed processing memory numerous simple processing elements weighted links permit high degree parallelism typical neural network fixed topology learns modifying weighted links nodes new class connectionist architectures proposed called asocs adaptive selforganizing concurrent systems asocs models support efficient computation learning parallel execution learning done incremental presentation rules andor examples asocs models learn modifying topology data types include boolean variables recent models support analog variables model incorporates rules adaptive logic network parallel self fashion processing mode asocs supports fully parallel execution actual inputs according learned rules adaptive logic network acts parallel hardware circuit execution mapping input boolean vectors output boolean vectors fashion overall asocs follows high level goals current neural network models however mechanisms learning execution vary significantly asocs logic network dynamic network growing efficiently fit specific application current asocs models based digital nodes asocs also supports use symbolic heuristic learning mechanisms thus combining parallelism distributed nature connectionist computing potential power symbolic learning proof concept asocs developed
theory applications agnostic paclearning small decision trees exhibit theoretically algorithm agnostic paclearning decision trees levels whose computation time almost linear size training set evaluate performance learning algorithm common realworld datasets show datasets provides simple decision trees little loss predictive power compared c45 fact datasets continuous attributes error rate tends lower c45 best knowledge first time paclearning algorithm shown applicable realworld classification problems since one prove agnostic paclearning algorithm guaranteed produce close optimal level decision trees sufficiently large training sets distribution data differs strongly learning algorithms considered applied machine learning guarantee given performance new datasets also demonstrate algorithm used diagnostic tool investigation expressive limits level decision trees finally combination new bounds decision trees bounded depth derive provides now first time tools necessary comparing learning curves decision trees realworld datasets theoretical estimates pac learning theory
distribution performance multiple neural network trials distribution performance back department electrical computer engineering university now brain information processing group research program institute physical chemical research abstract performance neural network simulations often reported terms mean standard number simulations performed different starting conditions however many cases distribution individual results approximate gaussian distribution may symmetric may multimodal present distribution results practical problems show assuming gaussian distributions significantly affect interpretation results especially comparison studies controlled task consider find distribution performance towards better performance target functions towards worse performance
qualitative framework probabilistic inference introduction algorithms
environmental effects minimal behaviors world structure environment behaviors organisms evolved structure described behavioral consequences explained predicted aim establish initial answers questions simulating evolution simple organisms simple environments different structures artificial called neither sensors memory behave solely actions moving according probability distribution simulated environments contain multiple structured terms spatial temporal density appears changes environmental parameters affect evolved behaviors different ways three parameters importance describing world one useful behavioral strategies evolves movement allows lack internal match behavior temporal spatial structure environment find construct environments individual behaviors making study impact global environment structure individual behavior much complex
causal diagrams empirical research structural equations primary aim paper show graphical models used mathematical language integrating statistical information particular paper develops principled nonparametric framework causal inference diagrams determine assumptions available sufficient identifying causal effects data diagrams produce mathematical expressions causal effects terms observed distributions otherwise diagrams suggest additional observations auxiliary experiments desired inferences obtained key words causal inference graph models structural equations treatment effect
biases estimating attributes analyse biases measures estimating quality attributes values information gain relevance tend linearly increase number values attribute values distance measure relief weight evidence decrease attributes increase irrelevant attributes bias tests based distribution similar functions able discriminate among attributes different quality also introduce new function based mdl principle whose value slightly decreases increasing number attributes values
weighted nearest neighbor algorithm learning symbolic features past nearest neighbor algorithms learning examples best domains features numeric values domains examples treated points distance metrics use standard definitions symbolic domains sophisticated treatment feature space required introduce nearest neighbor algorithm learning domains symbolic features algorithm distance tables allow produce realvalued instances weights instances modify structure feature space show technique produces excellent classification accuracy three problems studied machine learning researchers predicting protein secondary structure identifying dna sequences english text direct experimental comparisons learning algorithms show nearest neighbor algorithm comparable superior three domains addition algorithm advantages training speed simplicity conclude experimental evidence use development nearest neighbor algorithms domains ones studied
supervised unsupervised discretization continuous features many supervised machine learning algorithms require discrete feature space paper review previous work continuous feature discretization identify defining characteristics methods conduct empirical evaluation several methods compare unsupervised discretization method methods supervised algorithms found performance naivebayes algorithm significantly improved features using method fact tested datasets version naivebayes slightly outperformed c45 average also show cases performance c45 induction algorithm significantly improved features advance experiments performance never significantly interesting phenomenon considering fact c45 capable locally ing features
evolving cellular automata genetic algorithms review recent work review recent work done group applying genetic algorithms gas design cellular automata perform computations requiring global coordination used evolve two computational tasks density classification synchronization cases discovered rules rise sophisticated emergent computational strategies strategies analyzed using computational mechanics framework information interactions effects information processing framework also used explain process strategies designed work described first step employing gas useful emergent computation multiprocessor systems also first step understanding evolutionary process produce complex systems sophisticated collective computational abilities
computation evolved cellular automata
statistical dynamics road genetic algorithm common phenomenon many evolutionary processes natural artificial alternate periods brief periods rapid change behavior paper analytical model dynamics genetic algorithm introduced identifies new general mechanism evolutionary dynamics gas population dynamics described terms space fitness distributions trajectories fitness distribution space derived closed form limit infinite populations show finite populations induce even regions fitness exhibit local optimum particular model predicts fitness periods population fitness finite population size identifies locations fitness fixed points enables exact predictions fitness distributions fitness giving insight nature periods results obtained expressions terms gas parameters analysis matrices neighborhood fitness distribution allows calculation stable dimensions reveals state spaces structure general quantitative features stability speed also determined analysis shows quantitative predictions range dynamical behaviors specific finite population dynamics derived solution infinite population dynamics theoretical predictions shown agree statistics simulations also discuss connections results population genetics molecular evolution theory
learning concept classification rules using genetic algorithms paper explore use adaptive search technique genetic algorithms construct system continually learns refines concept classification rules interaction environment performance system measured set concept learning problems compared performance two existing systems c45 preliminary results support despite minimal system bias effective concept learner quite competitive c45 target concept increases complexity
options design exploration system reference user guide version
study crossvalidation bootstrap accuracy estimation model selection review accuracy estimation methods compare two common methods crossvalidation bootstrap recent experimental results artificial data theoretical results restricted settings shown selecting good classifier set classifiers model selection crossvalidation may better expensive leaveoneout crossvalidation report largescale runs c45 naivebayes estimate effects different parameters algorithms realworld datasets crossvalidation vary number whether bootstrap vary number bootstrap samples results indicate datasets similar best method use model selection cross validation even computation power allows using
scaling accuracy naivebayes classifiers decisiontree hybrid naivebayes induction algorithms previously shown surprisingly accurate many classification tasks even conditional independence assumption based however studies done small databases show larger databases accuracy naivebayes scale decision trees propose new algorithm induces hybrid decisiontree classifiers naivebayes classifiers decisiontree nodes contain univariate splits regular leaves contain classifiers approach naivebayes decision trees resulting classifiers frequently outperform especially larger databases tested
mlc machine learning library present mlc library classes tools supervised machine learning mlc provides general learning algorithms used end users main objective provide researchers experts wide variety tools accelerate algorithm development increase software reliability provide comparison tools display information just collection existing algorithms mlc attempt extract algorithms unified view simple coherent paper discuss problems mlc solve design mlc current
computing nonparametric hierarchical models bayesian models involving dirichlet process mixtures modern nonparametric bayesian movement much rapid development models last direct result advances computational methods early work area focused use nonparametric ideas models applications otherwise standard hierarchical models chapter provides historical review perspective developments focus use integration nonparametric ideas hierarchical models illustrate parametric assumptions common standard bayesian hierarchical models incorporate uncertainties functional forms using dirichlet process components approach computation using mcmc methods resulting illustrated two examples taken report topic
analysis bayesian classifiers involves formulation models specific algorithms paper present analysis bayesian classifier simple induction algorithm many learning tasks analysis assumes monotone conjunctive target concept independent noisefree boolean attributes calculate probability algorithm will induce arbitrary pair concept descriptions use compute probability correct classification instance space analysis takes account number training instances number attributes distribution attributes level class noise also explore behavioral implications analysis presenting predicted learning curves artificial domains give experimental results domains check reasoning one goal research machine learning discover principles relate algorithms domain characteristics behavior end many researchers carried systematic experimentation natural artificial domains search empirical regularities others focused theoretical analyses often within paradigm probably approximately correct learning however experimental studies based analyses learning task whereas formal analyses address worst case thus bear little relation empirical results attributes class attribute frequencies obtain predictions behavior induction algorithms used experiments check analyses however research focus algorithms typically used experimental practical machine learning important analyses extended methods recently growing interest probabilistic approaches inductive learning example fisher described incremental algorithm conceptual clustering heavily bayesian ideas literature reports number systems build work system uses bayesian methods cluster instances groups researchers focused induction bayesian inference networks recent bayesian learning algorithms complex easily analysis share common simpler tractable supervised algorithm refer simply bayesian classifier comes originally work pattern recognition method probabilistic summary class summary contains conditional probability attribute value given class probability base rate class data structure approximates representational power perceptron describes single decision boundary instance space algorithm new instance updates probabilities stored specified class neither order training instances classification errors effect process given test instance classifier uses evaluation function describe detail later
adaptive regularization regularization form weight decay important training optimization neural network architectures work provide tool based asymptotic sampling theory iterative estimation weight decay parameters basic idea gradient descent estimated generalization error respect regularization parameters scheme implemented net framework network training pruning based approximation scheme require essential computational addition needed training pruning approach demonstrated experiment concerning prediction chaotic series find optimized weight relatively large connected networks initial pruning phase decrease pruning
growing layers perceptrons introducing algorithm perceptrons perceptron learning algorithm cycles among hyperplanes hyperplanes may compared select one gives best split examples always possible perceptron build plane least one example describe grows multilayer networks capable distinguishing non data using simple perceptron rule linear threshold units resulting algorithm simple fast scales large convergence properties perceptron completely specified using two parameters results presented comparing neural network paradigms symbolic learning systems
neural network gradient factors technical report abstract long known neural networks learn faster input hidden unit activities zero recently extended approach also error signals generalize notion factors involved networks gradient leading propose hidden unit activation functions linear component error improves credit assignment networks connections benchmark results show speed learning significantly without trained networks generalization ability
randomly learning monotone dnf incomplete membership oracle introduce new model algorithmic learning using equivalence oracle incomplete membership oracle answers random subset learners membership queries may missing demonstrate high probability still possible learn monotone dnf formulas polynomial time provided fraction missing answers bounded constant less one even membership queries expected yield information algorithm will exactly identify monotone dnf formulas expected queries task shown require exponential time using equivalence queries alone extend algorithm handle errors discuss several possible error models work may lead better understanding power membership queries effects query models concept learning
discovery physical principles design experiences one method making access abstract domain principles one method acquiring knowledge abstract principles discover experience view generalization experiences absence prior knowledge target principle task hypothesis formation discovery also view use principles analogical design task hypothesis testing another discovery paper focus discovery physical principles generalization design experiences domain physical devices important issues generalization experiences generalize experience far generalize methods use represent specific designs form sbf models sbf model provides functional causal explanation working device represent domain principles models show function device determines generalize sbf model sbf model suggests far generalize iii functions indicates method use
use mental models index learning design power casebased method comes ability retrieve right case new problem specified implies learning right indices case potential reuse crucial success method hierarchical organization case memory two distinct related issues index learning learning indexing learning right level generalization paper show use sbf models index learning context design physical devices sbf model design provides functional causal explanation structure design function describe sbf model design together specification task design case might provides indexing design case memory also discuss prior design experiences stored help determine level index generalization system implements evaluates modelbased method learning indices design cases
references linear controller design limits performance parallel projection operators nonlinear feedback stabilization linear systems bounded controls proc nonlinear control systems design journal version appear ieee control
markov chain monte carlo model determination hierarchical graphical models bayesian approach comparing models involves calculating posterior probability plausible model highdimensional tables set plausible models large focus attention reversible markov chain monte carlo develop strategies calculating posterior probabilities hierarchical graphical models even tables size sets models may large choice suitable prior distributions model parameters also discussed detail two examples presented first example table model probabilities calculated using reversible approach compared model probabilities calculated exactly using alternative approximation second example table exact methods infeasible due large number possible models
learning indices schema selection addition learning new knowledge system must able learn knowledge likely applicable index information identified given situation relevant knowledge schema systems memory discuss issue indices may learned automatically context story understanding task present program learn new indices existing schemas discuss two methods using system identify relevant schema even input directly match existing index learn new index allow retrieve schema efficiently future
robust performance adaptation using horizon control time varying systems paper construct suboptimal controllers satisfy new robust performance condition using horizon technique method described synthesis controllers online making use exact model finite interval extending future based two differential equation solution finite horizon problem derived resulting freedom exploited construct controllers closed loop induced norm less value within set described terms future variation dual results possible adaptive interpretation also constructed
instancebased learning system lattice theory concept learning one studied areas machine learning work domain deals decision trees paper concerned different kind technique based concept present new based system uses entropy function topdown approach select concepts lattice construction generates new relevant numerical features initial boolean features concepts uses new features examples finally applies distance similarity measure examples keywords multistrategy learning instancebased learning lattice feature transformation
hybrid learning using genetic algorithms decision trees pattern classification paper introduces hybrid learning methodology integrates genetic algorithms gas decision tree learning id3 order evolve optimal subsets features robust pattern classification used search space possible subsets large set candidate discrimination features given feature subset id3 produce decision tree classification performance decision tree unseen data used measure fitness given feature set turn used evolve better feature sets process feature subset found satisfactory classification performance experimental results presented illustrate feasibility approach difficult problems involving recognizing visual concepts facial image data results also show improved classification performance reduced description complexity compared standard methods feature selection
combining first order logic connectionist learning paper presents neural network architecture structured data refine knowledge bases expressed first order logic language presented framework suited classification problems concept depend upon numerical features data fact main goal neural architecture refining numerical part knowledge base without changing structure particular discuss method set classification rules neural computation units focus attention method algorithms refine network weights data classification theory refined automatically acquired symbolic relational learning system able deal numerical features fact primary goal neural network architecture capability dealing structured data size allowing dynamically classification rules different items occur input data extensive experimentation challenging artificial case study shows network converges quite generalizes much better propositional learners equivalent task definition
teaching teaching neuroevolution evolving population neural nets contains information terms genes also collection behaviors population members information thought kind population two ways exploiting explored paper generate large number different quickly evaluate comparing performance population away appear poor teaching use backpropagation train toward performance population techniques result faster effective neuroevolution effectively combined demonstrated problem additional methods exploitation possible will studied future work results suggest exploitation powerful idea allows several aspects genetic algorithm
engine algorithm examples paper describes engine sme program studying analogical processing sme built explore theory analogy provides tool constructing matching algorithms consistent theory flexibility cognitive simulation studies simplifying experimentation furthermore sme efficient making useful component machine learning systems review theory describe design engine analyze complexity algorithm demonstrate steps polynomial typically bounded next demonstrate examples operation taken cognitive simulation studies work machine learning finally compare sme analogy programs discuss several areas future work paper appeared artificial intelligence information please
modeling invention analogy investigate aspects cognition involved invention precisely invention bell propose use sbf language representation invention knowledge claim sbf shown support wide range reasoning physical devices plausible account might represent knowledge invention propose use architecture implementation model shown precisely model wide range human cognition upon architecture execution matching declarative knowledge activation thus present model combines cognitive validity powerful specialized modelbased reasoning methods sbf
constraint distance online character recognition online character recognition observe two kinds variations small geometric completely different styles propose new approach deal problems defining extension distance known offline character recognition system implemented knearest neighbor classifier called classifier respectively classifiers invariant transformations like scale deal variations order direction results presented digit database
boltzmann machines gibbs sampling exact boltzmann learning done certain restricted networks technique set boltzmann machines introducing new general rule compared solutions probability density estimation problem boltzmann machines results obtained gibbs sampling
complexity function learning majority results computational learning theory concerned concept learning special case function learning classes functions range much less known theory learning functions larger range particular relatively results exist general structure common models function learning nontrivial function classes positive learning results exhibited models introduce paper notion binary adversary tree function learning allows give somewhat surprising equivalent characterization optimal learning cost learning class realvalued functions terms definition involve learning model another general structural result paper cost learning function classes learning costs individual function classes furthermore exhibit efficient learning algorithm learning convex linear functions previously class linear functions class functions multidimensional domain known learnable within rigorous framework formal model online learning finally give sufficient condition arbitrary class functions allows learn class functions written maximum functions allows exhibit number nontrivial classes functions exist efficient learning algorithms
extracting comprehensible concept representations trained neural networks although applicable wide array problems demonstrated good performance number difficult realworld tasks neural networks usually applied problems comprehensibility acquired concepts important concept representations formed neural networks hard understand typically involve distributed nonlinear relationships encoded large number realvalued parameters address limitation developing algorithms extracting symbolic concept representations trained neural networks first discuss important able understand concept representations formed neural networks briefly describe approach discuss number issues comprehensibility work finally discuss choices made research date open research issues yet addressed
learning consistently teacher one view computational learning theory learner acquiring knowledge teacher introduce formal model learning idea may gaps knowledge particular consider learning teacher labels examples positive instance concept learned negative instance concept learned instance unknown classification way knowledge concept class positive negative examples sufficient determine examples goal learner teacher attempting infer labels examples rather learn approximation presented teacher thus goal learner still acquire knowledge teacher now learner must also identify gaps notion learning consistently teacher present general results describing known learning algorithms used obtain algorithms learn consistently teacher investigate learnability variety concept classes model including monotone dnf formulas horn sentences decision trees axisparallel boxes euclidean space among others learnability results presented
efficient method estimate generalization error bagging one uses bootstrap training set try improve learning algorithms performance computational requirements estimating generalization error test set means crossvalidation often leaveoneout crossvalidation one needs train underlying algorithm order times size training set number paper presents several techniques exploiting decomposition estimate generalization error learning algorithm without yet training underlying learning algorithm best estimators exploits set experiments reported found accurate alternative estimator algorithms error estimator underlying algorithms error improvement particularly small test sets suggests novel using bagging proved estimation generalization error
towards automatic discovery building blocks genetic programming paper presents algorithm discovery building blocks genetic programming called adaptive representation learning central idea adaptation problem representation extending set functions set evolvable set common knowledge evolutionary process necessary structure solving problem supports creation creation discovery performed automatically based differential fitness block activation relies utility measure similar schema fitness window past generations technique described tested problem controlling agent dynamic nondeterministic environment automatic discovery help scale technique complex problems
exact identification readonce formulas using fixed points functions paper describe new technique exactly identifying certain classes readonce boolean formulas method based sampling inputoutput behavior target formula probability distribution determined fixed point formulas function defined probability output formula input bit independently probability performing various statistical tests easily sampled variants distribution able efficiently infer structural information formula high probability apply results prove existence short universal identification sequences large classes formulas also describe extensions algorithms handle high rates noise learn formulas depth model respect specific distributions research carried three authors mit laboratory computer science support provided nsf grant grant contract grant siemens corporation extended abstract paper appeared proceedings annual symposium foundations computer science supported part foundation grant supported grant
learning dnf formulas incomplete membership oracle consider problem learning dnf formulas using equivalence queries incomplete membership queries defined demonstrate model applied classes namely describe polynomialtime algorithm exactly identifies dnf formula dnf hypothesis using incomplete membership queries equivalence queries class dnf formulas
word perfect transformation implementing neural networks artificial neural networks anns fixed topology learning typically suffer number shortcomings result variations anns use dynamic topologies shown ability overcome many problems paper introduces transformations general strategy parallel implementation feedforward networks use dynamic topologies creates set nodes node computes part network output independent nodes using local information type transformation allows efficient support adding nodes dynamically learning paper deals specifically sense one node responsible output particular paper presents two anns competitive learning network network combines elements supervised learning competitive learning complexity learning execution algorithms anns linear number inputs logarithmic number nodes original network
learning local error nonlinear regression learning local error nonlinear regression present new method obtaining local error nonlinear regression estimates confidence predicted values depend input approach problem applying framework assumed distribution errors demonstrate method first data locally varying normally distributed target noise apply data santa time series competition underlying system noise known quantization error error give local estimates model cases method also provides effect improves generalization performance
learning refine indexing introspective reasoning significant problem casebased reasoning cbr systems determining features use case similarity retrieval describe research addresses feature selection problem using introspective reasoning learn new features indexing method cbr system introspective reasoning component monitors system performance detect poor identifies features lead retrieval cases refines indexing criteria include needed features avoid future failures explore benefit introspective reasoning performing empirical tests implemented system tests examine effect introspective index refinement effects problem order case index learning show introspective learning new index features improves performance across different problem orders
structure oriented case retrieval
structural learning fuzzy rules examples inductive learning algorithms try obtain knowledge system set examples one difficult problems machine learning consists structure knowledge propose algorithm able fuzzy information able learn structure rules represent system algorithm gives reasonable small set fuzzy rules represent original set examples
theory refinement maintenance position since consider theory refinement possible key concept clear view maintenance try give structured overview actual overview along description search problem explain basic approach show variety existing systems try give direction future research
discrimination computer problem computer systems viewed generally problem learning distinguish self describe method change detection based generation cells system mathematical analysis reveals computational costs system preliminary experiments illustrate method might applied problem computer
mcmc convergence diagnostic via central limit theorem markov chain monte carlo mcmc methods introduced provide simulation based strategy statistical inference application fields related methods theoretical convergence properties studied recent literature however many improvements still expected provide theoretically solutions problem monitoring convergence actual outputs mcmc algorithms convergence assessment problem paper introduce discuss methodology based central limit theorem markov chains assess convergence mcmc algorithms instead searching approximate primarily control precision estimates invariant probability measure functions respect measure confidence regions based normal approximation first proposed control method tests hypothesis averages functions markov chain independent parallel chains control provides good guarantees whole state space explored even multimodal situations lead automated stopping rules second tool connected control based graphical monitoring stabilization variance iterations near limiting variance methods require knowledge sampler driving chain paper mainly focus finite state markov chains since setting allows derive consistent estimates limiting variance variance iterations heuristic procedures based bounds investigated extension continuous case also proposed numerical simulations performance methods given several examples finite chain multimodal invariant probability finite state random walk theoretical rate convergence known continuous state chain multimodal invariant probability gibbs sampler
direct time series prediction using paper explores application temporal difference learning sutton forecasting behavior dynamical systems realvalued outputs opposed situations performance learning comparison standard supervised learning depends amount noise present data paper use deterministic chaotic time series task direct predictions experiments show standard supervised learning better learning algorithm viewed predictions similar effect obtained sharing internal representation network thus compare two architectures paradigms first architecture separate hidden units consists individual networks five direct prediction tasks second shared hidden units single larger hidden layer finds representation five predictions next five steps generated data set find significant difference two architectures paper available
simple algorithm discovers efficient perceptual codes describe algorithm allows multilayer unsupervised neural network build hierarchy representations sensory input network bottomup recognition connections used sensory input underlying representations unlike artificial neural networks also topdown generative connections used sensory input representations phase learning algorithm network driven bottomup recognition connections topdown generative connections trained better sensory input representation chosen recognition process phase network driven topdown generative connections produce representation sensory input recognition connections trained better recovering representation sensory input phases synaptic learning rule simple local combined effect two phases create representations sensory input efficient following sense average takes bits describe sensory input vector directly first describe representation sensory input chosen recognition process describe difference sensory input reconstruction chosen representation
priors infinite networks technical report department computer science university toronto college road toronto abstract bayesian inference begins prior distribution model parameters capture prior beliefs relationship modeled multilayer perceptron networks parameters connection weights prior direct meaning prior functions computed network implied prior weights paper show priors weights defined way corresponding priors functions reach reasonable limits number hidden units network using priors thus need limit size network order avoid overfitting infinite network limit also provides insight properties different priors gaussian prior weights results gaussian process prior functions smooth depending hidden unit activation function prior weights quite different effects obtained using priors based stable distributions networks one hidden layer combination gaussian priors appears interesting
nearoptimal performance reinforcement learning polynomial time present new algorithms reinforcement learning prove polynomial bounds resources required achieve nearoptimal return general markov decision processes observing number actions required approach optimal return lower bounded mixing time optimal policy undiscounted case horizon time discounted case give algorithms requiring number actions total computation time polynomial number states undiscounted discounted cases interesting aspect algorithms explicit handling tradeoff first results reinforcement learning literature giving algorithms provably converge nearoptimal performance polynomial time general markov decision processes
case cases call casebased reasoning inherently difficult basic casebased reasoning cbr involves reasoning cases representations real episodes rather rules facts structures connection real episodes fact cbr systems reason directly cases rather reason abstractions cases paper argue pure casebased reasoning reasoning representations concrete reasonably complete claim working representations satisfy criteria illustrate argument examples three previous systems cbr system developed first author
generalization reinforcement learning approximating value function appear eds advances neural information processing systems mit press cambridge straightforward approach dimensionality reinforcement learning dynamic programming replace table generalizing function approximator neural net although successful domain guarantee convergence paper show combination dynamic programming function approximation robust even cases may produce entirely policy introduce new algorithm yet still benefits successful generalization
modeling simple genetic algorithms permutation problems exact model simple genetic algorithm developed permutation based representations permutation based representations used scheduling problems combinatorial problems problem function developed model search space mixing matrices various permutation based operators also developed
evaluating evolutionary algorithms test functions commonly used evaluate effectiveness different search algorithms however results evaluation dependent test problems algorithms subject comparison unfortunately developing test suite evaluating competing search algorithms difficult without clearly defined evaluation goals paper discuss basic principles used develop test examine role test used evaluate evolutionary search algorithms current test include functions easily solved simple search methods greedy test functions also characteristics dimensionality search space increased new methods examined constructing functions different degrees interactions cost evaluation scale respect dimensionality search space
contextsensitive generalization ica source separation arises surprising number signal processing applications speech recognition eeg analysis square linear blind source separation problem without time one must find matrix result mixing unknown independent sources unknown mixing matrix recently introduced ica blind source separation algorithm bell sejnowski powerful surprisingly simple technique solving problem ica performing despite making use temporal structure input paper presents new algorithm contextual ica derives maximum likelihood density estimation formulation problem incorporate arbitrarily complex adaptive source models thereby make use temporal structure input allows separate number situations standard ica including sources low gaussian sources sources gaussian since ica special case derivation provides rigorous derivation ica
adaptive neural network applicability adaptive neural network problems input demonstrating deterministic natural language inputs significant syntactic complexity developed using recurrent connectionist architectures traditional mechanism known necessary proper treatment contextfree languages symbolic systems design network
engineering systems paper address problem constructing reliable implementations given assumption particular implementation will correct approach taken paper errors minimize impact context system system multiple versions together will system unique characteristics neural computing exploited order reliable systems form diverse systems used together decision strategy majority theoretical notions methodological diversity improvement system performance implemented tested important aspect engineering optimal system components choose optimal subset three general techniques choosing final system components implemented evaluated several different approaches effective engineering complex systems designs realized evaluated determine overall reliability reliability overall system comparison reliability component
use methodological diversity improve neural network generalisation present statistical framework dealing failures software systems develop theoretical model holds promise high system reliability use multiple diverse sets alternative versions paper adapt framework investigate feasibility exploiting diversity observable multiple populations neural networks developed using diverse evaluate generalisation improvements achieved range diverse network generation processes attempt order constituent methodological features respect potential use engineering useful diversity also define explore use relative measures diversity version sets guide potential exploiting diversity
learning control knowledge models expertise workshop modelling machine learning development knowledgebased systems requirements system knowledge system will change one types knowledge affected changing requirements ordering problemsolving steps aid knowledgebased systems adapting systems changing requirements number techniques learning applied knowledge engineering focus construction models problemsolving instead directly constructing knowledgebased system paper describe work progress apply machine learning techniques model expertise
state merging algorithm results one dfa learning abstract paper first describes structure results one dfa learning competition competition designed work algorithms scale larger training data describe discuss algorithm price orders state according amount evidence second algorithm will described separate paper
cortical activity among stationary states school center neural computation university school mathematical sciences exact sciences school university institute computer science center neural computation university
fast bottomup decision tree pruning algorithm nearoptimal generalization work present new bottomup algorithm decision tree pruning efficient requiring single given tree prove strong performance guarantee generalization error resulting tree work typical setting given tree may derived given training sample thus may setting give bounds amount additional generalization error pruning compared optimal pruning generally results show pruning small error whose size small compared algorithm will find pruning whose error much larger style result called index result cover context density estimation novel feature algorithm decision based entirely properties sample analyze algorithm develop tools local uniform convergence generalization standard notion may prove useful settings
training support vector machines application face detection investigate application support vector machines computer vision learning technique developed team bell seen new method training polynomial neural network radial basis functions classifiers decision found solving linearly constrained quadratic programming problem optimization problem challenging quadratic form completely dense memory requirements grow square number data points present decomposition algorithm guarantees global optimality used train large data sets main idea behind decomposition iterative solution evaluation optimality conditions used generate improved iterative values also establish stopping criteria algorithm present experimental results implementation demonstrate feasibility approach face detection problem involves data set data points
learning finite automata using local distinguishing experiments one open problems schapire whether copies algorithm combined one better performance paper describes algorithm called combination idea represent states learned model using observable symbols hidden symbols constructed learning hidden symbols created reflect distinct behaviors model states distinct behaviors represented local distinguishing experiments global distinguishing sequences created learners prediction actual observation unknown machine model environment also form sequence shown learn probability model approximation unknown machine number actions polynomial size environment
models evolutionary validation problem guidance support evolutionary models natural systems represent abstraction system thus models suffer validation problem believe results model particularly problem alife models evolution time scale evolution complexity make controlled experiments difficult alife contribute significantly biology must find methods build confidence models one alternative experimental tests model previously verified theory applied series evolutionary validation tests model species dynamics competitive adaptation curve model shown course spatial structure inadequate capture realistic dynamics spatial structure extended local dynamics model begin behave wide range parameters validation dynamics model provides indirect support evolutionary behavior species within every model abstraction goal model capture system real world behavior model matches behavior real system thus model may valid representation real system question problem validation traditionally try validity model data real system comparing predictions model artificial life artificial life models tend highly abstract general field discover general properties life makes experimental validation extremely difficult time scale evolution tends restrict experiments observation example manipulation organisms extremely short simplified environments example similarly complexity size makes experiments difficult control alternative form validation reference evolutionary theory instead model matches experimental data model matches understanding dynamics evolution extent theories evolution experimental observations validity model fails match theories follows example technique applied model designed examine factors impact maintenance species diversity purpose model explore new theoretical biology evolutionary dynamics model theories competition adaptation island genotype diversity presence absence selection varying mutation rates model results found diversity
adapting crossover evolutionary algorithms one issues evolutionary algorithms relative importance two search operators mutation crossover genetic algorithms gas genetic programming role crossover evolutionary programming evolution strategies role mutation existence many different forms crossover issue despite theoretical analysis appears difficult decide priori form crossover use even crossover used one possible solution difficulty dynamically modify forms crossover use often use solves problem paper describes adaptive mechanism controlling use crossover explores behavior mechanism number different situations improvement adaptive mechanism presented surprisingly improvement also used enhance performance
probabilistic independence networks hidden markov probability models graphical techniques modeling dependencies random variables explored variety different areas including statistics statistical physics artificial intelligence speech recognition image processing genetics models developed relatively independently research paper explore hidden markov models hmms related structures within general framework probabilistic independence networks paper contains review basic principles shown wellknown algorithms hmms special cases general inference algorithms arbitrary furthermore existence inference estimation algorithms general graphical models provides set analysis tools hmm explore class hmm structures examples relatively complex models handle sensor speech recognition introduced treated within graphical model framework illustrate advantages general approach report describes research done department information computer science university california laboratory california institute technology research center biological computational learning artificial intelligence laboratory massachusetts institute technology authors support provided part grant nsf asc9217041 support laboratorys artificial intelligence research provided part advanced research projects agency defense discussions application algorithm
vapnikchervonenkis entropy perceptron perceptron learning randomly labeled patterns analyzed using gibbs distribution set patterns entropy distribution extension vapnikchervonenkis entropy reducing exactly limit infinite temperature close relationship seen within formalism recent progress towards understanding relationship statistical physics vapnikchervonenkis approaches learning two approaches unified statistical mechanics based entropy paper case learning randomly labeled patterns capacity problem extends results previous finite temperature will explained paper extension important generalization problem occurs context learning patterns labeled target rule general framework illustrated simple perceptron maps dimensional realvalued input valued output given sample inputs weight vector determines sample via weight vector defines normal hyperplane positive negative examples training error respect reference defined just fraction different labels two consider case reference chosen random address issue
learning without acquisition task one agent traces path second agents sensory field second agent path exactly move sequence locations first agent nontrivial behaviour whose acquisition might expected involve strongly biased learning however present paper shows case behaviour acquired using fairly primitive learning regime provided agents environment made specific sequence dynamic states
evolving behaviour artificial problem research demonstrate solution artificial problem likely specific characteristics santa presents consistent method producing general solutions using concepts training testing machine learning research method useful producing general simulation environments
speech recognition dynamic bayesian networks dynamic bayesian networks useful tool representing complex stochastic processes recent developments inference learning allow use realworld applications paper apply problem speech recognition state representation allows explicitly represent longterm context addition information hidden markov models hmms furthermore enables model shortterm correlations among multiple observation within single given structure capable representing long shortterm correlations applied algorithm learn models parameters use structured models error rate recognition task compared discrete hmm also improved significantly published results task first successful application largescale speech recognition problem investigation learned models indicates hidden state variables strongly correlated properties speech signal
selforganizing sets experts describe evaluate connectionist systems composed expert networks preprocessing training data competitive learning network system automatically process decomposition expert using several different types challenging problem assess approach degree automatically generated experts subset overall task comparison equivalent addition assess utility approach competition systems previously developed measures diversity systems also applied provide quantitative assessment degree specialization obtained ensemble show types problem abstract automatic decomposition produce effective set networks together support high level performance study provide support differential effectiveness within two classes problem continuous homogeneous functions discrete functions
parametric regression learning problem model turn estimator let present briefly learning problem will address chapter following goal modelling mapping multidimensional input output output multidimensional will mostly address situations one dimensional real value furthermore take account fact observe actual true mapping due perturbations noise will rather joint probability expect probability values corresponding mapping focus automatic learning example set data sampled joint distribution collected help set try identify model data set learning fit model system given point measured using criterion representing distance model prediction system local risk performance model measured expected represents ability yield good performance possible situations pairs thus called generalisation error optimal set parameters
towards robust model selection using estimation approximation error bounds
using casebased reasoning reinforcement learning framework optimization changing criteria practical optimization problems jobshop scheduling often involve optimization criteria change time identified flexible computational paradigms difficult combinatorial optimization problems since control problem optimization reinforcement learning techniques potentially helpful however fundamental assumptions made traditional algorithms valid optimization casebased reasoning cbr limitations traditional approaches paper present casebased reasoning approach implemented system optimization jobshop scheduling approach experimental results show able effectively solve problems changing optimization criteria known system exist implicitly manner case base
model depending set parameters used estimate introduced earlier use regularisation learning procedure now understood regularisation often increase quality results even solution acceptable likely regularisation will produce improvement performance exist method giving directly best value regularisation parameter even linear case topic chapter thus propose methods estimate best value best one leads smallest generalisation error methods presented compared propose estimators generalisation error estimation used approximate best regularisation level present techniques estimate generalisation error basis extra data deal algebraic estimates error use extra data rely number assumptions contribution chapter present techniques analyse also present short derivations links different estimators generalisation error comparison course chapter error will quadratic difference methods possible consider kind error without modification method hand algebraic estimates specific quadratic cost adapting another cost function require derive new expressions estimators
users guide recursive covering approach local learning
hybrid approach controllers robot achieve tasks evolutionary approaches robot design research work shown success evolving controllers robots genetic approaches observe however controller also robot body affect behavior robot robot system paper develop hybrid approach evolve controllers robot achieve tasks order assess performance developed approach used evolve simulated agent controller body avoidance simulated environment experimental results show promise work addition importance controllers robot analyzed discussed paper
statistical learning regularisation regression application system identification time
estimation modelling keep track cases estimation set ffi depends estimated probabilities become quite noisy number elements set small reason estimate standard estimate empirical average variable either given satisfied conditions ffi standard estimated easily generally increases output test ffi approaches input condition let now define maximum ffi max index defined represents much data test input information available index measures much remaining information associated involving input index respect probability clear therefore average positive quantities furthermore system deterministic zero certain number inputs sum averages system also noisefree sum greater dimension refers results obtained using method statistical variable selection statistical variable selection feature selection number techniques aimed choosing relevant subset input variables regression classification problem document will limit related regression problem even though methods discussed apply classification variable selection seen part data analysis problem selection variable relevance associated measurement system general setting purely combinatorial problem given possible variables possible subsets including set full set variables given performance measure prediction error optimal scheme test subset choose one gives best performance easy see extensive scheme number variables rather low identifying models variables requires much computation number techniques overcome combinatorial limit use iterative locally optimal technique construct estimate relevant subset number steps will refer stepwise selection methods stepwise regression subset methods will address forward selection start set variables step select candidate variable using selection criteria check whether variable added set given condition contrary elimination methods start full set input variables step least significant variable selected according selection criteria variable irrelevant removed process iterated condition easy examples variable causes previously included variable become irrelevant thus seems appropriate consider running elimination time new variable added forward selection combination known stepwise regression linear regression
abstraction considered lazy learning language processing
use operations dynamically adapt analog source identification circuit accommodate used genetic programming evolve topology numerical values component analog electrical circuit correctly classify analog electrical signal three categories dynamically changed adding new source run
evolution learning planning memory using genetic programming essential component intelligent agent ability observe encode use information environment traditional approaches genetic programming focused evolving functional reactive programs minimal use state paper presents approach investigating evolution learning planning memory using genetic programming approach uses fitness environment use memory allows fairly straightforward evolved representations problem gold collection used demonstrate usefulness approach results indicate approach evolve programs store simple representations environments use representations produce simple plans
genetic algorithm programming environments interest genetic algorithms rapidly paper reviews software environments programming genetic algorithms background initially genetic algorithms models programming next classify software environments three main categories category programming environment review common features present case study leading environment
connection pruning static adaptive pruning schedules neural network pruning methods level individual network parameters connection weights improve generalization shown empirical study however open problem pruning methods known today selection number parameters removed pruning step pruning strength work presents pruning method automatically adapts pruning strength evolution weights loss generalization training method requires algorithm parameter user results statistical significance tests comparing static networks early stopping given based extensive experimentation different problems results indicate training pruning often significantly better significantly worse training early stopping without pruning furthermore often superior superior diagnosis tasks pruning early training process required
exploring framework instance based learning naive bayesian classifiers relative performance different methods classifier learning varies across domains recent instance based learning methods use similarity measures based conditional class probabilities probabilities key component naive bayes methods given approach interest consider differences two methods relative performance different domains naive bayes like framework identifying differences naive bayes framework experiments variants lie naive bayes framework conducted domains results strongly suggest relative performance naive bayes extent class represented single instance framework however factor appears significant
references automatic student modeling library construction using theory refinement symbolic revision demonstrates theory refinement techniques developed machine learning used build student models intelligent systems application unique since normal goal theory refinement errors knowledge base introducing experiment involving number students interacting automated teaching concepts programming used evaluate approach experiment demonstrated ability theory refinement generate accurate student models raw induction ability resulting models support feedback actually improves students subsequent performance theory modeling computer instruction report cambridge mit technology know artificial intelligence
tractable inference complex stochastic processes monitoring control dynamic system depends ability reason current future trajectory case stochastic system tasks typically involve use belief probability distribution state process given point time unfortunately state spaces complex processes large making explicit representation belief state intractable even dynamic bayesian networks process represented representation belief state intractable investigate idea utilizing compact approximation true belief state analyze conditions errors due approximations taken lifetime process make answers completely irrelevant show error belief state exponentially process evolves thus even multiple approximations error process remains bounded show additional structure used design approximation scheme improving performance significantly demonstrate applicability ideas context monitoring task showing orders magnitude faster inference achieved small accuracy
new approach induction logical point view reasoning system designed generalpurpose intelligent reasoning system adaptive works insufficient knowledge resources paper focuses components contribute systems induction capacity shows traditional problems induction addressed system approach induction uses formal language semantics consistently various types uncertainty induction rule generates conclusions common instance terms revision rule combines evidence different sources induction types inference abduction based semantic foundation inference activities system systems control mechanism makes inference possible
casebased reasoning although casebased reasoning cbr natural formulation many problems previous work cbr applied design made apparent elements cbr paradigm widely applied time evaluating constraint satisfaction techniques design found motivation constraint satisfaction problems case adaptation led combine two order gain advantages casebased reasoning allowing cbr widely applied combining two found approaches paper describes combined casebased reasoning gives brief overview future work exploiting emergent combining reasoning modes
ridge regression learning algorithm dual variables paper study dual version ridge regression procedure allows perform nonlinear regression constructing linear regression function high dimensional feature space feature space representation result large increase number parameters used algorithm order combat dimensionality algorithm allows use kernel functions used support vector methods also discuss powerful family kernel functions constructed using anova decomposition method kernel corresponding splines infinite number nodes paper introduces regression estimation algorithm combination two elements dual version ridge regression applied anova enhancement splines experimental results presented based data set indicate performance algorithm relative algorithms
bcm network develops orientation ocular dominance natural scene environment visual environment used training network bcm neurons study effect synaptic density functions two eyes formation orientation ocular dominance lateral network visual environment use composed natural images show bcm rule natural image environment cortical sufficient producing networks orientation selective cells ocular dominance work extension previous single cell model
bayesian estimation von parameter von distribution maximum entropy distribution corresponds distribution uniform field direction parameter parameter ratio field strength temperature previously obtained bayesian estimator von distribution parameters using informationtheoretic minimum message length mml principle examine variety bayesian estimation techniques examining posterior distribution compare mml estimator bayesian techniques range classical estimators find bayesian estimators outperform classical estimators
design analogy creativity
support vector machines kernel spaces randomized workshop support vector machines research sponsored part nsf grant part grant second revised version report number title november
generating accurate diverse members neuralnetwork ensemble neuralnetwork ensembles shown accurate classification techniques previous work shown effective ensemble consist networks highly correct ones make errors different parts input space existing techniques however address problem creating set networks paper present technique called uses genetic algorithms directly search accurate diverse set trained networks works first creating initial population uses genetic operators continually create new networks keeping set networks accurate possible much possible experiments three dna problems show able generate set trained networks accurate several existing approaches experiments also show able effectively incorporate prior knowledge available improve quality ensemble
comparison regression methods symbolic induction methods neural networks diagnosis classifier induction algorithms differ inductive hypotheses represent search space hypotheses classifier better another problems selective superiority paper empirically compares six classifier induction algorithms diagnosis prediction classification based simultaneously analyzing features measured patient relative algorithms linear regression decision trees nearest neighbor classifiers model class selection system logistic regression without feature selection neural nets discussed generalization analyzed
using multiparent crossover gas reproduction operators obtain becomes feature instead boolean one main objective relate performance gas extent used reproduction less arbitrary functions reported current literature investigate behaviour allow systematic characterization user control fitness landscape test gas varying extent ranging tests performed two types landscapes random landscapes nearest landscape types selected landscapes range results confirm superiority recombination problems
unsupervised learning using mml paper discusses unsupervised learning problem important part unsupervised learning problem determining number constituent groups components classes best describes data apply minimum message length mml criterion unsupervised learning problem modifying earlier mml application give empirical comparison criteria literature estimating number components data set conclude minimum message length criterion performs better alternatives data considered unsupervised learning tasks
representing physical design knowledge design
single factor analysis mml estimation minimum message length mml technique applied problem estimating parameters multivariate gaussian model correlation structure single common factor implicit estimator equations derived compared obtained maximum likelihood analysis unlike mml estimators consistent used estimate factor factor tests simulated data show mml estimates accurate estimates former exist data show little evidence factor mml estimate shown condition existence mml estimate essentially log likelihood ratio factor model value expected hypotheses
inverse entailment paper firstly provides development techniques inverting secondly introduces inverse entailment generalisation enhancement previous approaches describes implementation system implemented available previous techniques terms inverse entailment leads new results learning positive data inverting implication pairs clauses
learning firstorder definitions functions firstorder learning involves finding definition relation examples relation relevant background information paper particular firstorder learning system modified finding definitions functional relations restriction leads faster learning times cases definitions higher predictive accuracy firstorder learning systems might benefit similar specialization
adaptive boosting neural networks character recognition technical report abstract boosting general method improving performance learning algorithm consistently generates classifiers need perform slightly better random recently proposed promising boosting algorithm adaboost applied great success several benchmark machine learning problems using rather simple learning algorithms particular decision trees paper use adaboost improve performances neural networks applied character recognition tasks compare training methods based sampling training set weighting cost function system achieves error data base online handwritten digits adaptive boosting multilayer network achieved error offline data set
learning represent challenge problem constructive induction ability inductive learning system find good solution given problem dependent upon representation used features problem systems perform constructive induction able change representation constructing new features describe important realworld problem finding genes dna believe offers interesting challenge researchers report experiments demonstrate two different input representations task result significantly different generalization performance neural networks decision trees neural symbolic methods constructive induction fail bridge gap two representations believe realworld domain provides interesting challenge problem constructive induction relationship two representations known representational shift involved construct ing better representation
evolutionary algorithms robotics
boxes high dimension dimacs technical report preliminary version paper appeared proceedings conference published notes artificial intelligence pages journal version will appear email part research done author student email research supported research
advantages decision lists implicit inductive logic programming paper demonstrates capabilities inductive logic programming ilp system whose distinguishing characteristics ability produce firstorder decision lists use output assumption negative examples use background knowledge development originally motivated problem learning generate past english however paper demonstrates superior performance two different sets benchmark ilp problems tests finite element design problem show decision lists enable produce generally accurate results range methods previously applied problem tests selection problems prolog text demonstrate combination implicit allow learn correct programs far fewer examples foil
results controllability abstract paper studies controllability properties recurrent neural networks new contributions extension previous result slightly different model formulation proof necessary sufficient condition analysis case recurrent neural networks
figure results various figure results without markov boundary scoring
coupled hidden markov models complex action recognition perceptual computing learning common sense technical report abstract present algorithms coupling training hidden markov models hmms model interacting processes demonstrate superiority conventional hmms vision task classifying actions hmms perhaps successful framework perceptual computing modeling classifying dynamic behaviors popular offer dynamic time training algorithm clear bayesian semantics however markovian framework makes strong assumptions system generating single process small number states extremely limited state memory model often inappropriate vision speech applications resulting low model performance coupled hmms provide efficient way many problems offer superior training model robustness initial conditions
learning undiscounted delayed rewards general framework reinforcement learning proposed several researchers solution optimization problems realization adaptive control schemes allow efficient application reinforcement learning either areas necessary solve structural temporal credit assignment problem paper latter usually use learning algorithms employ discounted rewards argue realistic problems kind solution satisfactory since address effect noise different experiences allow easy explanation parameters involved learning process possible solution propose keep delayed reward undiscounted actual adaptation rate empirical results show dependent kind used stable convergence even increase performance obtained
domain assignment certain classes problems defined twodimensional domains grid structure optimization problems involving assignment grid cells processors present nonlinear network model problem partitioning tasks among processors minimize communication minimizing communication context shown equivalent domain minimize total corresponds collection tasks assigned processor tight lower bound function area developed show generate using assignments corresponding closed form solutions developed certain classes domains conclude computational results parallel highlevel genetic algorithms produced good sometimes provably optimal solutions large minimization problems
value function approximations jobshop scheduling report successful application value function approximation task jobshop scheduling scheduling problems based problem scheduling processing steps space program value function approximated layer feedforward network sigmoid units lookahead greedy algorithm using learned evaluation function outperforms best existing algorithm task iterative method incorporating simulated annealing understand reasons performance improvement paper introduces several measurements learning process discusses several hypotheses suggested measurements conclude use value function approximation source difficulty method fact may explain success method independent use value iteration additional experiments required discriminate among hypotheses
hyperplane ranking simple genetic algorithm several metrics used empirical studies explore mechanisms convergence genetic algorithms metric designed measure consistency arbitrary ranking hyperplanes partition respect target string coefficients calculated small functions order characterize sources linear nonlinear interactions simple measure also developed look closely effects increasing functions correlations metric measure discussed relationships convergence behavior simple genetic algorithm studied large sets functions varying degrees
learning rules using problems ordered increasing order difficulty teaching problemsolving widely used technique computational reason knowledge gained solving simple problems useful efficiently solving difficult problems approach learning acquire knowledge form rules first order learned using new algorithm based inductive logic programming techniques demonstrate feasibility approach applying two planning
residual qlearning applied visual attention vision features coupled context sensitive sensor control analogous vision vision operates efficiently uniform vision resolution treated dynamically resource requires refined visual attention mechanism demonstrate reinforcement learning significantly improves performance visual attention overall vision system task model based target recognition simulated vision system shown classify targets fewer learning strategies acquisition visual information relevant task learning generalize strategies conditions
learning horn definitions equivalence membership queries horn definition set horn clauses paper consider learning firstorder horn definitions show class exactly learnable equivalence membership queries follows class pac learnable using examples membership queries results shown applicable learning efficient rules planning domains
empirical speedup learning goal decomposition rules speedup learning study improving problemsolving performance experience guidance describe system successfully combines best features explanationbased learning empirical learning learn goal decomposition rules examples successful problem solving membership queries demonstrate system efficiently learn effective decomposition rules three different domains results suggest empirical learning overcome problems purely explanationbased learning purely empirical learning effective speedup learning method
stabilization control authors recently proved general theorems global stabilization linear systems subject control saturation paper develops detail explicit design equations control aircraft tests obtained controller original nonlinear model paper represents first detailed derivation controller using techniques question results encouraging
model environment avoid local learning technical report december
automatic storage indexing plan derivations based replay failures casebased planner retrieving previous case solving new similar problem often implicit features new problem situation determine particular case may successfully applied means cases may fail improve planners performance detecting explaining case failures occur retrieval may improved incrementally paper provide definition case failure casebased planner derivation replay solves new problems previous plan derivations provide explanationbased learning ebl techniques detecting constructing reasons case failure also describe case library organized incorporate failure information produced finally present empirical study demonstrates effectiveness approach improving performance
adaptive mixtures probabilistic describe analyze mixture model supervised learning probabilistic online learning algorithm efficiently structure estimates parameters probabilistic mixture theoretical analysis comparative simulations indicate learning algorithm best arbitrarily large possibly infinite models also present application model inducing
competitive learning nodes competitive learning network achieved competition basis neural activity simple inhibitory mechanisms limited sparse representations schemes support distributed representations computationally neural plasticity competitive interaction instead obtain alternatives fully distributed representations use technique improve binary information gain optimization algorithm feature extraction sejnowski approach used improve learning algorithms
computation induced norm single input linear systems saturation
bayesian training backpropagation networks hybrid monte carlo method shown bayesian training backpropagation neural networks performed hybrid monte carlo method approach allows true predictive distribution test case given set training cases approximated arbitrarily closely contrast previous approaches approximate posterior weight distribution gaussian work hybrid monte carlo method implemented conjunction simulated annealing order speed relaxation good region parameter space method applied test problem demonstrating produce good predictions uncertainty predictions appropriate weight scaling factors found automatically applying known techniques calculation free energy differences also possible compare different network architectures work described also applicable wide variety statistical models neural networks
automatic indexing retrieval reuse topologies former contain much generic automatic way order use computer effort however seems way access developing generic software tool reuse former consider every part domain things like personal style tools used today consider small parts domain personal style possible build basic tool content former may extended modeling much domain desirable paper will describe reuse tool perform task focusing binary relations
neural network model prediction important difficult prediction task many domains particularly medical decision making presents unique set problems learning system outputs unknown paper presents new approach prediction using ideas nonparametric statistics fully utilize available information neural architecture technique applied breast cancer resulting flexible accurate models may play role ing
genetic algorithms external parameters seen first paper new approach presented basic idea evolution strategies gas mutation rates changed items adapting search process first experimental results presented indicate appropriate settings mutation rate possible even gas
interactive model teaching previous teaching models learning theory community batch models models teacher generated single set helpful examples present learner paper present interactive model learner ability queries query learning model show model least powerful previous teaching models also show learnable queries even randomized learner model previous teaching models classes shown known efficiently learnable important concept class known learnable dnf formulas demonstrate power approach providing deterministic teacher learner class dnf formulas learner makes equivalence queries hypotheses also dnf formulas
searching effective neuralnetwork ensemble neuralnetwork ensemble successful technique outputs set separately trained neural network combined form one unified prediction effective ensemble consist set networks highly correct ones make errors different parts input space however existing techniques address problem creating set present algorithm called uses genetic algorithms explicitly search highly diverse set accurate trained networks works first creating initial population uses genetic operators continually create new networks keeping set networks highly accurate much possible experiments four realworld domains show able generate set trained networks accurate several existing ensemble approaches experiments also show able effectively incorporate prior knowledge available improve quality ensemble
sequential context sensitive default extensions default logic conceptual difficulties representing common sense reasoning tasks argue try formulate modular default rules work circumstances need take account importance context continuously evolving reasoning process sequential thresholding quantitative default logic makes explicit role context plays construction extension present semantic characterization generic reasoning default logic sequential thresholding provides link two mechanisms way integrate two
generalized markov decision processes reinforcementlearning algorithms problem maximizing expected total discounted reward completely observable markovian environment markov decision process mdp models particular class sequential decision problems algorithms developed making optimal decisions mdps given either mdp specification opportunity interact mdp time recently sequential decisionmaking problems studied development new algorithms analyses describe new generalized model mdps many recent variations prove basic results concerning model develop generalizations value iteration policy iteration modelbased reinforcementlearning qlearning used make optimal decisions generalized model various assumptions applications theory particular models described including mdps mdps qlearning games approximate max via sampling central results property value operator theorem reduces asynchronous convergence convergence
learning queries examples treestructured bias incorporating declarative bias prior knowledge learning active research topic machine learning treestructured bias specifies prior knowledge tree relevance relationships attributes paper presents learning algorithm implements treestructured bias learns target function probably approximately correctly random examples membership queries given treestructured bias theoretical predictions paper
learning boltzmann trees introduce large family boltzmann machines trained using standard gradient descent networks one layers hidden units connectivity show implement supervised learning algorithm boltzmann machines exactly without simulated annealing stochastic averages yield weight space computed technique present results problems bit parity detection hidden
learning data data describing resolutions network local loop learn rules reasons resolution valid ranging management pressure paper describe four different approaches dealing problem data order first determine whether machine learning promise domain determine machine learning might perform offer evidence machine learning help build method will perform better system currently place
bias variance prediction error classification rules study notions bias variance classification rules following develop decomposition prediction error natural components derive bootstrap estimates components illustrate used describe error behaviour classifier practice process also obtain bootstrap estimate error classifier
linear systems paper deals systems obtained linear discretetime devices followed function just provides output systems appear naturally study observations signal processing neural network theory results given minimal concepts certain major differences exist linear case results generalize surprisingly straightforward manner
supporting design retrieval casebased reasoning cbr paradigm close behavior conceptual design seems computer approach library design cases available goal paper presents general framework casebased retrieval system supports chemical process design crucial problems like case representation structural similarity measure widely described presented experimental results expert evaluation shows usefulness described system real world problems papers discussion concerning research problems future work
study crossover operators genetic programming analysis sources power genetic algorithms guidance applications genetic algorithms years technique applying recombination operator crossover population individuals key power number results concerning crossover operators respect overall performance recently example genetic algorithms used design neural network modules control circuits studies genetic algorithm without crossover outperformed genetic algorithm crossover report studies concludes results caused small population size new results presented illustrate effectiveness crossover population size larger performance view results indicate better neural networks evolved time genetic algorithm uses crossover
adaptive strategy selection concept learning paper explore use genetic algorithms gas construct system called continually learns refines concept classification rules environment performance system compared two concept learners c45 suite target concepts comparison identify strategies responsible success concept learners implement subset strategies within produce multistrategy concept learner finally multistrategy concept learner enhanced allowing gas adaptively select appropriate strategies
overfitting crossvalidation data learning task select one hypothesis set hypotheses may example generated multiple applications randomized learning algorithm common approach evaluate hypothesis set previously unseen crossvalidation data select hypothesis crossvalidation error crossvalidation data partially corrupted noise set hypotheses selecting large also overfitting crossvalidation data paper explain overfitting occurs show surprising result overcome selecting hypothesis higher crossvalidation error others lower crossvalidation errors give reasons selecting hypothesis crossvalidation error propose new algorithm uses computationally efficient form leaveoneout crossvalidation select hypothesis finally present experimental results one domain show consistently hypothesis crossvalidation error even using reasonably large crossvalidation sets
missing six proceedings learning queries incomplete information extended abstract investigate learning membership equivalence queries assuming information provided learner incomplete incomplete mean membership queries may know model worstcase version incomplete membership query model attempts model practical learning situations including experiment describe teacher may answer reliably queries critical learning algorithm present algorithms learn monotone dnf membership queries learn monotone dnf membership equivalence queries compared complete information case query complexity increases additive term linear number know answers received also observe number queries general exponential new model incomplete membership model
automata linear systems theoretical framework discretetime hybrid systems iii paper summarizes definitions several main results approach hybrid systems combines finite automata linear systems developed author early related recent results briefly
stability property show wellknown lyapunov sufficient condition stability also necessary open question raised several authors past years additional property including one terms nonlinear stability also provided
successive linear programming approach differential algebraic equations determination consistent initial conditions important aspect solution differential algebraic equations specification initial conditions even slightly often leads failure problem paper present successive linear programming approach solution array equations problem formulation errors user specifications among others allows reliable convergence strategies incorporate variable bounds region concepts new consistent set initial conditions obtained minimizing variable values specified ones problems caused step change input functions new criterion presented identifying subset variables continuous across formulation applied determine consistent set initial conditions solution problem domain numerous example problems solved illustrate concepts
evolving efficient tool optimization discovery strategies field optimization machine learning techniques efficient promising tools like genetic algorithms gas hillclimbing designed field evolving end model proposes way explore space states combined use simulated coevolution drawbacks previous techniques even allow model outperform difficult problems new model applied sorting network problem reference problem many computer original game named first problem end model able build scratch sorting networks good best known input problem even improved one years old result input problem game end evolved strategy comparable human designed strategy
incremental coevolution organisms new approach optimization discovery strategies field optimization machine learning techniques efficient promising tools like genetic algorithms gas hillclimbing designed field evolving end model presented paper proposes way explore space states using simulated incremental coevolution organisms drawbacks previous techniques even allow model outperform difficult problems new model applied sorting network problem reference problem many computer original game named first problem end model able build scratch sorting networks good best known input problem even improved one years old result input problem game end evolved strategy comparable human designed strategy
within law planning casebased reasoning systems used single best similar case basis solution many problems however single exact solution rather range acceptable answers use cases basis solution also indicate boundaries within solution found solve problems choosing point within boundaries paper discuss use cases system implemented domain personal planning
program optimization faster genetic programming used genetic programming develop efficient image processing software goal work detect certain breast cancer detected current segmentation classification methods traditional techniques relatively good job classifying features clusters genetic programs work multiresolution representation aimed handling features large scales main problem efficiency employ program speed evolution process factor ten paper present genetic programming system describe optimization techniques
two constructive methods designing compact feedforward networks threshold units propose two algorithms constructing training compact feedforward networks linear threshold units shift procedure constructs networks single hidden layer constructs networks resulting networks guaranteed perform given task binary realvalued inputs various experimental results reported tasks binary real inputs indicate methods compare favorably alternative procedures deriving similar strategies terms size resulting networks generalization properties
lazy bayesian trees naive bayesian classifier simple effective attribute independence assumption often real world number approaches developed problem bayesian tree learning algorithm builds decision tree generates local bayesian classifier instead predicting single class however bayesian tree learning still problems tree learning inferred bayesian trees demonstrate low average prediction error rates reason believe error rates will higher leaves training examples paper proposes novel lazy bayesian tree learning algorithm test example builds appropriate bayesian tree practice one path local bayesian classifier created experiments wide variety realworld artificial domains show new algorithm significantly lower overall prediction error rates naive bayesian classifier c45 bayesian tree learning algorithm
revising bayesian network parameters using backpropagation problem learning bayesian networks hidden variables known hard problem even simpler task learning just conditional probabilities bayesian network hidden variables hard paper present approach learns conditional probabilities bayesian network hidden variables multilayer feedforward neural network ann conditional probabilities onto weights ann learned using standard backpropagation techniques avoid problem exponentially large anns focus bayesian networks nodes experiments real world classification problems demonstrate effectiveness technique
learning presence prior knowledge case study using model computational models natural systems often contain free parameters must set optimize predictive accuracy models viewed form supervised learning presence prior knowledge view fixed aspects model prior knowledge goal learn correct values free parameters report series attempts learn parameter values global model called system developed unfortunately attempts apply standard machine learning global error functions gradient descent work constraints introduced structure model prior knowledge create difficult nonlinear optimization problem successful required taking divideandconquer approach subsets parameters others held constant approach made possible carefully selecting training sets portions model designing error functions part desirable properties automated tool developed currently applied global data set
evolutionary approach learning robots evolutionary learning methods found useful several areas development intelligent robots approach described evolutionary algorithms used explore alternative robot behaviors within simulation model way reducing overall knowledge engineering effort paper presents initial results applying genetic learning system avoidance navigation task mobile robots
generalizations decomposition prediction error bias variance real valued random variable using squared error loss understood however recent developments classification techniques become desirable extend concepts general random variables loss functions misclassification loss function categorical random variables particular interest explore concepts variance bias develop decomposition prediction error functions systematic variable parts predictor providing examples conclude discussion various definitions proposed
similarity applied retrieval relevant cases retrieving relevant cases crucial component casebased reasoning systems task use query retrieve useful information exact matches partial matches close according certain measures difficulty fact may easy may even impossible specify query precisely completely resulting situation known usually problem small domains large store various information information bases databases specification becomes thus flexible retrieval algorithm required allowing query specification changing viewpoint efficient database techniques exists exact matches finding relevant partial matches might problem document proposes similarity basis flexible retrieval historical research similarity assessment presented used motivation formal definition similarity also describe retrieval system information bases
experiments new boosting algorithm earlier paper introduced new boosting algorithm called adaboost theoretically used significantly reduce error learning algorithm consistently generates classifiers whose performance little better random also introduced related notion method learning algorithm concepts labels discriminate paper describe experiments carried assess adaboost without performs real learning problems performed two sets experiments first set compared boosting bagging method used various classifiers including decision trees single attributevalue tests compared performance two methods collection second set experiments studied detail performance boosting using classifier problem
maximizing robustness linear threshold classifier discrete weights quantization parameters perceptron central problem hardware implementation neural networks using numerical technology interesting property neural networks used classifiers ability provide robustness input noise paper presents efficient learning algorithms maximization robustness perceptron especially designed combinatorial problem arising discrete weights
induction decision trees bayesian classification applied diagnosis machine learning techniques used extract knowledge data stored medical databases application various machine learning algorithms used extract diagnostic knowledge support diagnosis applied methods include variants assistant algorithm topdown induction decision trees variants bayesian classifier available dataset reliable diagnosis considered system consequently diagnostic rules added used additional training instances training examples experimental results show classification accuracy explanation capability naive bayesian classifier fuzzy discretization numerical attributes superior methods estimated practical use
divideandconquer approach learning prior knowledge paper introduces new machine learning presents method solving particularly difficult model task part global change research project model task problem training free parameters scientific model order optimize accuracy model making future predictions form supervised learning examples presence prior knowledge obvious approach solving problems formulate global optimization problems goal find values free parameters minimize error model training data unfortunately global optimization approach becomes computationally infeasible model highly nonlinear paper presents new divideandconquer method analyzes model identify series smaller optimization problems whose sequential solution solves global problem paper argues methods global optimization required order agents large amounts prior knowledge learn efficiently
identification control nonlinear systems using neural network models design stability analysis report revised may
declarative language bias formalism describe principles declarative language bias used inductive learning systems define efficiently finite subspaces first order clausal logic set propositional formulae association rules horn clauses full clauses prolog implementation available access keywords declarative language bias concept learning knowledge
feedback stabilization using nets paper compares representational capabilities one hidden layer two hidden layer nets consisting feedforward linear threshold units certain problems two hidden layers required contrary might principle expected known approximation theorems differences based numerical accuracy number units needed capabilities feature extraction rather much basic classification direct inverse problems former correspond approximation continuous functions latter concerned approximating continuous functions often encountered context inverse determination control questions general result given showing nonlinear control systems using two hidden layers general using just one
discovery autonomous learning environment discovery involves among many intelligent activities however little known form occurs paper framework proposed autonomous systems learn discover environment within framework many intelligent activities perception action exploration experimentation learning problem solving new term construction integrated coherent way framework presented detail implemented system called evaluated performance several discovery tasks autonomous learning environment feasible approach integrating activities involved discovery process
predicting time series support vector machines support vector machines used time series prediction compared radial basis function networks make use two different cost functions support vectors training loss robust loss function discuss choose regularization parameters models two applications considered data noisy normal uniform noise equation santa competition set cases support vector machines show excellent performance case support vector approach improves best known result benchmark factor
evaluation pattern classifiers applications paper evaluate classification accuracy four statistical three neural network classifiers two image based pattern classification problems classification optical character recognition isolated digits evaluation results reported useful designers practical systems two important commercial applications problem transform images used generate input feature set similarly problem transform ridge directions used generate input feature set statistical classifiers used euclidean minimum distance quadratic minimum distance normal knearest neighbor neural network classifiers used multilayer perceptron radial basis function probabilistic data digit images training digit images testing data training testing images addition evaluation accuracy multilayer perceptron radial basis function networks evaluated size generalization capability evaluated datasets best accuracy obtained either problem provided probabilistic neural network minimum classification error
avoiding saturation trajectory problem trajectory tracking presence input constraints considered desired trajectory slower time scale order avoid input saturation necessary conditions function must satisfy derived nominal trajectory problem optimal control problem
clique detection via genetic programming genetic programming applied task finding graph nodes graph represented tree structures form candidate intrinsic properties clique detection design good fitness evaluation analyze properties show clique found better finding maximum clique graph set
adaptive similarity assessment casebased explanation
combining rules cases learn case adaptation computer models casebased reasoning cbr generally guide case adaptation using fixed set adaptation rules difficult practical problem identify knowledge required guide adaptation particular tasks open issue cbr cognitive model case adaptation knowledge learned describe new approach acquiring case adaptation knowledge approach adaptation problems initially solved reasoning scratch using abstract rules structural transformations general memory search heuristics traces processing used successful rulebased adaptation stored cases enable future adaptation done casebased reasoning similar adaptation problems encountered future adaptation cases provide task guidance case adaptation process present approach concerning relationship memory search case adaptation memory search process storage reuse cases representing adaptation episodes points discussed context ongoing research computer model learns case adaptation knowledge casebased response planning
inferential theory learning developing foundations multistrategy learning development multistrategy learning systems based clear understanding roles applicability conditions different learning strategies end chapter introduces inferential theory learning provides conceptual framework explaining logical capabilities learning strategies learning process modifying learners knowledge exploring learners experience theory postulates process described search knowledge space learners experience guided learning goals search operators knowledge generic patterns knowledge change may employ basic type induction analogy several fundamental knowledge described novel general way generalization abstraction explanation specialization prediction respectively generalization reference set description set described abstraction reduces amount detail reference set explanation generates explain imply given properties reference set knowledge one reference set similar reference set using concepts theory multistrategy learning methodology illustrated example dynamically adapts strategies learning task defined input information learners background knowledge learning goal integrating whole range inferential learning strategies empirical generalization constructive induction deductive generalization explanation prediction abstraction
comparing support vector machines gaussian radial basis function classifiers support vector machine novel type learning machine based statistical learning theory contains polynomial classifiers neural networks radial basis function rbf networks special cases rbf case algorithm automatically determines centers weights threshold minimize upper bound expected test error present study devoted experimental comparison machines classical approach centers determined clustering weights found using error backpropagation consider three machines namely classical rbf machine machine gaussian kernel hybrid system centers determined method weights trained error backpropagation results show service database handwritten digits machine achieves highest test accuracy followed hybrid approach approach thus theoretically also superior practical application report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology bell laboratories now research bell laboratories support center provided part grant national science foundation contract asc9217041 mit march work time study bell laboratories massachusetts institute technology now department information systems computer science national university lower ridge road bell laboratories research supported supported onr contract number thank useful discussions please direct correspondence
induction ensembles ensembles classifiers decision trees often exhibit greater predictive accuracy single classifiers alone bagging boosting two standard ways generating combining multiple classifiers unfortunately increase predictive performance usually decrease ensembles less boxes comparable neural networks far attempts pruning ensembles successful approximately reducing ensembles paper describes different approach tries keep small induction already also limits complexity single classifiers single classifiers maximal depth combined majority voting ensembles induced simple hillclimbing procedure ensembles reasonably transformed equivalent decision trees conduct empirical evaluation investigate predictive classifier
characterization input state stability just input state stability generalizes idea finite gains respect new notion input state stability generalizes concept finite gain using norm inputs paper obtain necessary sufficient characterization property expressed terms
belief networks hidden markov models markov random fields unifying view use graphs represent independence structure multivariate probability models relatively independent fashion across wide variety research since beginning paper provides brief overview current research particular attention recent developments topics probabilistic expert systems statistical physics image analysis genetics errorcorrecting codes kalman filters speech recognition markov models
belief revision probability theory reasoning system bayes theorem variations often used revise systems beliefs however explicit conditions implicit conditions probability assignments properly distinguished follows bayes theorem generally applicable revision rule upon properly distinguishing belief revision belief updating see rule variations revision rules either without limitation bayesian approach often revision general form done bayesian approach probability distribution function alone contain information needed operation
relation logic beginning paper three binary term defined first based relation second third suggest novel way process extension also interesting relations logic based three simple systems logic defined language semantics uniformly represents processes also uniformly abduction induction revision
probably approximately optimal derivation strategies inference graph many derivation strategies particular ordering steps involved reducing given query sequence database optimal strategy given distribution queries complete strategy whose expected cost minimal expected cost depends conditional probabilities retrieval given class queries paper describes algorithm first uses set training examples approximate probability values uses estimates produce probably approximately optimal strategy given ffi produces strategy whose cost within cost optimal strategy probability greater ffi paper also shows obtain strategies time polynomial ffi size inference graph many important classes graphs including andor trees
reasoning system version used show system works limitations uses new form term logic extended several types uncertainties represented induction abduction revision carried unified system works parallel way memory system dynamically organized also interpreted network
unified treatment uncertainties uncertainty artificial intelligence active research field several approaches suggested studied dealing various types uncertainty however hard approaches general usually aimed special application environment paper begins defining environment show existing approaches used situation new approach reasoning system introduced work environment system designed assumption systems knowledge resources usually insufficient handle tasks environment system consistently represent several types uncertainty multiple operations uncertainties finally new approach compared previous approaches terms uncertainty representation interpretation
time series using gated experts simulated annealing many realworld time series underlying data generating process different stationary modes operation important problem modeling systems discover underlying switching process identifying number dynamics many time series problem since often obvious means distinguish different discuss use nonlinear gated experts perform segmentation system identification time series unlike standard gated experts methods however use concepts statistical physics enhance segmentation problems experts required
synthetic processing multiple scale neural system boundary surface representation
problem noise small disjuncts systems learn examples often create disjunctive concept definition disjuncts concept definition cover training examples small disjuncts problem small disjuncts error large disjuncts may necessary achieve high level predictive accuracy paper extends previous work done problem small disjuncts taking noise account investigates hard learn noisy data difficult distinguish noise true exceptions process evaluating insights gained mechanisms noise learning two domains investigated experimental results paper suggest chess domain breast cancer domain true least low levels class noise
stable dynamic parameter adaptation
crossvalidation bootstrap estimating error rate prediction rule training set data used construct rule predicting future responses error rate rule traditional answer question given crossvalidation crossvalidation estimate prediction error nearly unbiased highly variable article discusses bootstrap estimates prediction error thought versions crossvalidation particular bootstrap method rule shown substantially outperform crossvalidation simulation experiments providing point estimates also consider estimating variability error rate estimate results nonparametric apply possible prediction rule however study classification problems loss detail simulations include smooth prediction rules like linear discriminant function ones like nearest neighbors
fast using memorybased learning techniques paper discuss application memorybased learning fast first discuss application fast decision tree variant dataset described consists roughly test train items second series experiments used architecture two second level classifier added context predictions extra features incorrect predictions first level generalisation accuracy training testing times order
consistency examine issue consistency new perspective avoid overfitting training data considerable number current systems goal learning hypotheses consistent training instances setting new goal hypothesis simplicity occams razor instead using simplicity goal developed novel approach addresses consistency directly words concept learner explicit goal selecting appropriate degree consistency training data begin paper exploring concept learning less perfect consistency next describe system adapt degree consistency response feedback predictive accuracy test data finally present results initial experiments begin address question hypotheses fit training data different problems
evolving optimal populations xcs classifier systems
solving gas adapting constraint weights handling complete problems gas great challenge particular presence constraints makes finding solutions hard paper present problem independent constraint handling mechanism stepwise adaptation weights apply solving problem experiments prove mechanism substantially increases performance furthermore compare best heuristic technique trace conclude superior heuristic method
robust interpretation neuralnetwork models artificial neural network seem promising regression classification especially large spaces methods represent nonlinear function low dimensional ridge functions therefore appear less sensitive dimensionality space however due non global minimum existence possibly many local minima model network non stable introduce method neural network results uses novel techniques results robust interpretation model employed network simulated data known models used demonstrate results demonstrate effects different regularization methods robustness model graphical methods introduced present interpretation results demonstrate interaction study conclude interpretation method works models may sometimes especially approximations true model less robust
feature selection functional links evolutionary computation neural networks paper describe different ways select transform features using evolutionary computation features intended serve inputs feedforward network first way selection features using standard genetic algorithm solution found specifies whether certain feature present show prediction rates various approach fact kind selection features special case socalled functional links functional links transform input pattern space new pattern space functional links one use general functions found using evolutionary computation polynomial functional links found evolving coding polynomial symbolic functions use genetic programming genetic programming finds symbolic functions applied inputs compare latter two methods two artificial datasets realworld medical image dataset
online machine learning knowledge acquisition case study paper reports development realistic knowledgebased application using system problems requirements resulting tasks formulated account construction knowledge base task demonstrates use several learning algorithms inference engine graphical interface requirements design analysis revision refinement extension working model combined one incremental process illustrates cooperative modeling approach case study taken domain precisely deals management networks used part management tool acquiring refining policy modeling approach compared approaches machine learning
adaptive source separation source separation consists recovering set independent signals mixtures unknown coefficients observed paper introduces class adaptive algorithms source separation implements adaptive version estimation called adaptive separation via independence algorithms based idea serial updating specific form matrix updates systematically yields algorithms simple structure real complex mixtures performance algorithm depend mixing matrix particular convergence rates stability conditions interference levels depend distributions source signals close form expressions quantities given via asymptotic performance analysis numerical experiments effectiveness proposed approach
improving bagging performance increasing decision tree diversity ensembles decision trees often exhibit greater predictive accuracy single trees alone bagging boosting two standard ways generating combining multiple trees boosting empirically determined effective two recently proposed may produces diverse trees bagging paper reports empirical findings strongly support hypothesis greater decision tree diversity bagging simple modification underlying decision tree learner utilizes decision predefined depth starting point tree induction modified procedure yields competitive results still one attractive properties bagging iterations independent additionally also investigate possible integration bagging boosting procedures compared empirically various domains
arcing edge technical report statistics department university california berkeley abstract recent work shown adaptively training set growing classifier using new weights combining classifiers constructed date significantly decrease generalization error procedures type called arcing first successful arcing procedure introduced freund called adaboost effort explain adaboost works schapire derived bound generalization error convex combination classifiers terms margin introduce function called edge differs margin two classes framework understanding arcing algorithms defined framework see arcing algorithms currently literature optimization algorithms minimize function edge relation derived optimal reduction maximum value edge pac concept weak learner two algorithms described achieve optimal reduction tests synthetic real data schapire recent empirical evidence significant generalization error growing number different classifiers training set best class freund schapire proposed algorithm called adaboost adaptively training set way based past history constructs new classifier using current weights uses misclassification rate classifier determine size number empirical studies many data sets using trees cart c45 base classifier adaboost produced decreases generalization error compared using single tree error rates reduced point tests wellknown data sets result cart plus adaboost significantly better commonly used classification methods empirical results showed methods adaptive combining called arcing also led low test set error rates algorithm called error rates almost identical adaboost classifiers consisting randomly selected hyperplanes using different method adaptive voting also low error rates thus least three arcing algorithms give excellent classification accuracy explanation
generalized permutation approach job shop scheduling genetic algorithms order sequence tasks job shop problem number machines related machine order new representation technique mathematically known permutation presented main advantage single representation analogy permutation scheme problem produce sets operation sequences infeasible symbolic solutions consequence representation scheme new crossover operator preserving initial scheme structure will behavior similar known simple permutation schemes actually operator arises generalisation computational experiments show information solutions efficiently solutions together new representation support cooperative aspect genetic search scheduling problems strongly
blind separation delayed sources based information maximization recently bell sejnowski presented approach blind source separation based information maximization principle extend approach general cases sources may delayed respect present network architecture capable sources derive adaptation equations weights network maximizing information network examples using sources speech presented illustrate algorithm
reference classes multiple reference class problem probability theory multiple extensions problem special cases beliefs current solution accepted two domains specificity priority principle analyzing example several factors principle found relevant priority reference class new approach reasoning system discussed factors taken account argued solution provided better solutions provided probability theory
working paper information systems first application independent component analysis extracting structure paper discusses application modern signal processing technique known independent component analysis ica blind source separation multivariate financial time series key idea ica linearly map observed multivariate time series new space statistically independent components viewed since joint probabilities become simple products coordinate system apply ica three years daily returns largest compare results obtained using principal component analysis results indicate estimated fall two categories large responsible major changes stock prices smaller little overall level show overall stock price surprisingly using small number weighted contrast using derived principal components instead independent components price less similar original one independent component analysis potentially powerful method analyzing understanding driving mechanisms financial markets promising applications risk management since ica focuses higher order statistics
theory inferred causation causal relationships observations task paper concerns empirical basis causation addresses following issues propose semantics causation show contrary common causal influences distinguished following standard inductive reasoning also establish sound characterization conditions distinction possible provide effective algorithm inferred causation show large class data algorithm direction causal influences defined finally issue causation
using qualitative models guide inductive learning paper presents method using qualitative models guide inductive learning objectives induce rules accurate also respect qualitative model reduce learning time exploiting domain knowledge learning process essential practical application inductive technology integrating results learning back existing apply method two process control problems network process used mining surprisingly addition achieving accuracy induced rules also increased show value qualitative models terms equivalence additional training examples finally discuss possible extensions
explanation based learning comparison symbolic neural network approaches explanation based learning typically considered symbolic learning method explanation based learning method utilizes purely neural network representations called recently developed shown several desirable properties including robustness errors domain theory paper briefly summarizes algorithm explores correspondence neural network based ebl method ebl methods based symbolic representations
performance multiparent crossover operators numerical function optimization problems multiparent crossover generalizing traditional uniform crossover crossover generalizing point introduced subsequent see several aspects multiparent recombination discussed due space limitations however full overview experimental results showing performance multiparent gas numerical optimization problems never published technical report gap make results available
conversational decision environment report documents conversational decision environment developed center applied research artificial intelligence branch naval research laboratory software prototype developed practical advances casebased reasoning project office naval research purpose decision tasks system maintenance training crisis response planning fault diagnosis target classification implemented used machine containing virtual machine document describes capabilities goal transition tool enhancement user feedback testing recent research advances casebased reasoning related areas
automated decomposition modelbased learning problems new generation sensor rich massively distributed autonomous systems developed potential performance adaptive traffic systems monitoring achieve high performance systems will need accurately model environment sensor information scale requires largescale modeling paper presents modelbased learning method developed observing expertise large scale model estimation tasks method exploits analogy learning diagnosis implementation applied modeling building demonstrating significant improvement learning rate
evolving visual routines traditional machine vision assumes vision system complete labeled description world recently several researchers model proposed alternative model considers perception distributed collection visual routines researchers argued natural living systems visual routines product natural selection far researchers visual routines actual implementations paper propose alternative approach visual routines simple tasks evolved using artificial evolution approach present results series runs actual images simple routines evolved using genetic programming techniques koza results obtained promising evolved routines able correctly classify images better best algorithm able hand
use explicit goals knowledge guide inference learning combinatorial inferences always central problem artificial intelligence although inferences drawn knowledge available inputs large potentially infinite inferential resources available reasoning system limited limited inferential capacity many potential inferences must control process inference inferences equally useful given reasoning system reasoning system goals form utility function acts based beliefs assigns utility beliefs given limits process inference variation utility inferences clear reasoner inferences will valuable paper presents approach problem makes utility potential belief explicit part inference process method generate explicit knowledge question focus attention thereby transformed two related problems explicit knowledge used control inference facilitate goal pursuit general knowledge present theory knowledge goals knowledge use processes understanding learning theory illustrated using two case studies natural language understanding program learns reading novel stories differential diagnosis program improves accuracy experience
decision models theory explanation paper presents theory analysis construction explanations describe planning behavior agents discuss content explanations process builds explanations explanations constructed decision models describe planning process agent considering whether perform action decision models represented explanation patterns standard patterns causality based previous experiences discuss nature explanation patterns use representing decision models process retrieved used evaluated
representation evolution neural networks evolutionary approach developing improved neural network architectures presented shown possible use genetic algorithms construction backpropagation networks real world tasks therefore network representation developed certain properties results various application presented
incremental learning explanation patterns indices paper describes reasoner improve understanding understood domain application already novel problems domain recent work issue using past explanations stored memory understand novel situations however process assumes past explanations understood provide good lessons used future situations assumption usually false one learning novel domain since situations encountered previously domain might understood completely instead reasonable assume reasoner gaps knowledge base reasoning new situation reasoner able gaps new information explanations memory gradually evolve better understanding domain present story understanding program past explanations situations already memory uses build explanations understand novel stories system refines understanding domain gaps explanations explanations learning new indices explanations type incremental learning since system improves knowledge domain incremental fashion rather learning new whole
analysis dynamics neural network hidden markov models present method analysis nonstationary time series multiple operating modes particular possible detect model switching dynamics less time one mode another achieved two steps first unsupervised training method provides prediction experts inherent dynamical modes trained experts used hidden markov model allows model application data demonstrates analysis modeling realworld time series improved paradigm taken account
finding new rules incomplete theories explicit biases induction contextual information proceedings addressed translates theory refines using backpropagation result back rules adding extra hidden units connections initial network however require paper presented constructive induction techniques recently added either theory refinement system intermediate concept employs existing rules theory derive features use induction intermediate concept creation employs inverse resolution introduce new intermediate concepts order gaps theory multiple levels revisions allow either make use imperfect domain theories ways typical previous work constructive induction theory refinement result either able handle wider range theory existing theory refinement system
residual algorithms reinforcement learning function approximation number reinforcement learning algorithms developed guaranteed converge optimal solution used tables shown however algorithms easily become implemented directly general system sigmoidal multilayer perceptron system memorybased learning system even linear system new class algorithms residual gradient algorithms proposed perform gradient descent mean squared bellman residual convergence shown however may learn cases larger class algorithms residual algorithms proposed guaranteed convergence residual gradient algorithms yet fast learning speed direct algorithms fact direct residual gradient algorithms shown special cases residual algorithms shown residual algorithms combine advantages approach direct residual gradient residual forms value iteration qlearning advantage learning presented theoretical analysis given explaining properties algorithms simulation results given demonstrate properties
unsupervised learning algorithm note describes useful adaptation peak regime used unsupervised learning processes competitive learning adaptation enables learning capture probability effects thus fully capture probabilistic structure training data
protein experiment planning using analogy protein experiments planning experiment execution experiment design execution central activity natural sciences system provides general architecture integration automated planning techniques variety domain knowledge order plan scientific experiments planning techniques include rulebased methods especially use analogy analogy allows planning experience captured cases analogy also allows system function absence strong domain knowledge cases efficiently retrieved large using massively parallel methods
belief networks
monitoring embedded agents finding good monitoring strategies important process design embedded agent describe nature monitoring problem point makes difficult show monitoring strategies often derive always appropriate demonstrate mathematically empirically wide class problems socalled problems exists simple strategy interval reduction outperforms monitoring also show features environment may influence choice optimal strategy paper concludes monitoring strategy taxonomy defining features might
learning goal oriented bayesian networks risk management paper discusses issues related bayesian network model learning binary classification tasks general primary focus current research bayesian network learning systems variants creation bayesian network structure database best turns applied specific purpose classification performance network models may poor demonstrate bayesian network models created meet specific goal purpose intended model first present algorithm constructing bayesian networks predicting datasets second argue demonstrate current bayesian network learning methods may fail perform real life applications since learn models specific goal purpose third discuss performance goal oriented variant
analytical mean squared error curves temporal difference learning calculated analytical expressions bias variance estimators provided various temporal difference value estimation algorithms change updates trials markov chains using table representations illustrate classes learning curve behavior various chains show manner sensitive choice step size trace parameters
misclassification minimization problem minimizing number points plane attempting separate two point sets convex real space formulated linear program equilibrium constraints lpec general lpec exact penalty problem quadratic objective linear constraints algorithm proposed penalty problem stationary point global solution novel aspects approach include linear formulation step function exact penalty formulation without constraint assumptions iii exact solution extraction sequence penalty function finite value penalty parameter general lpec explicitly exact solution lpec constraints parametric quadratic programming formulation lpec associated misclassification minimization problem
free energy coding paper introduce new approach problem optimal compression source code produces multiple given symbol may seem codeword use case shortest one however proposed free energy approach random codeword selection yields effective codeword length less shortest codeword length random choices boltzmann distributed effective length optimal given source code expectationmaximization parameter estimation algorithms minimize effective codeword length illustrate performance free energy coding simple problem compression factor two gained using new method
explaining predictions bayesian networks influence diagrams bayesian networks influence diagrams used widely importance efficient explanation mechanism becomes apparent focus predictive explanations ones designed explain predictions probabilistic systems analyze issues involved defining computing evaluating explanations present algorithm compute
mdl mml similarities differences introduction minimum encoding inference part iii report department computer science university abstract paper introduction minimum encoding inductive inference given hand series papers written objective providing introduction area describe message length estimates used minimum message length mml inference minimum description length mdl inference differences message length estimates two approaches explained implications differences applications discussed
performance analysis large dense backpropagation networks connectionist network determine study performance training evaluation large feedforward neural networks using sophisticated coding node machine achieve connections per second connection updates per second recall machine peak performance training large nets less efficient recall factor benchmark machine code optimized analyzing performance starting optimal parallel algorithm specific still reduce run time factor recall factor training analysis also yields strategies code optimization still design therefore model run time behavior memory system interconnection network gives changing parameters system order analyze performance impact
casebased reasoning casebased interactive crisis response assistant paper present interactive casebased approach crisis response provides users ability rapidly develop good responses allowing control decisionmaking process implemented approach interactive crisis assistant planning scheduling crisis domains relies casebased methods response development process initial candidate solutions drawn previous cases human user adapt solutions current situation will discuss interactive approach crisis response using artificial domain developed purpose evaluating candidate assistant mechanisms crisis response
learning predict user operations adaptive scheduling mixedinitiative systems present challenge finding effective level interaction humans computers machine learning presents promising approach problem form systems automatically adapt behavior accommodate different users paper present empirical study learning user models adaptive assistant crisis scheduling describe problem domain scheduling assistant present initial formulation adaptive learning task results baseline study report results three subsequent experiments investigate effects problem reformulation representation results suggest problem reformulation leads significantly better accuracy without usefulness learned behavior studies also several interesting issues adaptive scheduling
framework knowledge acquisition iterative revision schedule improvement reactive
bayesian informationtheoretic priors bayesian network parameters consider bayesian informationtheoretic approaches determining prior distributions parametric model family informationtheoretic approaches based recently modified definition stochastic complexity minimum message length mml approach bayesian alternatives include uniform prior equivalent sample size priors order able empirically compare different approaches practice methods model family practical importance family bayesian networks
approach intelligent information retrieval intelligent information retrieval iir requires inference number inferences drawn even simple reasoner large inferential resources available practical computer system limited problem one long researchers paper present method used two recent machine learning programs control inference relevant design iir systems key feature approach use explicit representations desired knowledge call knowledge goals theory addresses representation knowledge goals methods generating goals heuristics selecting among potential inferences order satisfy goals view iir becomes kind planning decisions infer infer infer based representations desired knowledge internal representations systems inferential abilities current state theory illustrated using two case studies natural language understanding program learns reading novel stories differential diagnosis program improves accuracy experience conclude making several suggestions machine learning framework integrated existing information retrieval methods
using communication reduce distributed multiagent learning paper attempts bridge fields machine learning robotics distributed discusses use communication reducing effects fully distributed multiagent systems multiple learning parallel interacting two key problems hidden state credit assignment addressed applying local communication dual role sensing reinforcement methodology demonstrated two learning experiments first describes learning coordination task two robots second task four robots learning social rules communication used share sensory data overcome hidden state share reinforcement overcome credit assignment problem agents bridge gap payoff
good genetic algorithms finding large experimental study paper investigates power genetic algorithms solving problem measure performance standard genetic algorithm set problem instances consisting embedded random graphs indicate need improvement introduce new genetic algorithm annealed exhibits superior performance problem set scale problem size test hard benchmark instances performance algorithm caused convergence local minima problem sequence modifications implemented ranging changes input representation systematic local search recent version called incorporates features crossover greedy diversity enhancement shows speedup number iterations required find given solution improvement clique size found discuss issues related simd implementation genetic algorithms machines high time complexity serial algorithm computing one iteration preliminary conclusions genetic algorithm needs heavily work clique problem computationally expensive use known find larger algorithms although effort improvements clear evidence time will better success local minima
active learning statistical models many types learners one compute statistically optimal way select data review techniques used feedforward neural networks show principles may used select data two alternative learning architectures mixtures gaussians locally weighted regression techniques neural networks expensive approximate techniques mixtures gaussians locally weighted regression efficient accurate report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology support center provided part grant national science foundation contract asc9217041 authors also foundation atr human information processing laboratories siemens research nsf grant grant office naval research michael jordan nsf young version paper appears eds advances neural information processing systems morgan san
design analysis efficient reinforcement learning algorithms
characterizing rational versus exponential learning curves consider standard problem learning concept random examples learning curve defined expected error learners hypotheses function training sample size shown distribution free setting smallest expected error learner achieve worst case concept class converges zero error training sample size however recently demonstrated exponential convergence often observed experimental settings average error fit addressing simple original analysis paper shows rational exponential worst case learning curves distribution free theory results support experimental findings finite concept classes consistent learner achieves exponential convergence even worst case continuous concept classes learner exhibit convergence every target concept domain distribution precise boundary rational exponential convergence drawn simple concept chains show dense chains always force rational convergence worst case exponential convergence always achieved dense chains
using sampling queries extract rules trained neural networks concepts learned neural networks difficult understand represented using large realvalued parameters one approach understanding trained neural networks extract symbolic rules describe classification behavior several existing approaches operate searching rules present novel method rule extraction search problem instead learning problem addition learning training examples method exploits property networks efficiently describe algorithms extracting conjunctive rules present experiments show method efficient conventional approaches
fast domains using decomposition paper presents fast algorithm provides optimal near optimal solutions minimum problem grid minimum problem partition grid size equal area regions minimizing total regions approach taken grid completely integer number regions method gives rise integer program efficiently solved existing codes solution problem used generate grid region assignments implementation algorithm grid regions provably optimal solution less one second sufficient memory hold grid array extremely large minimum problems solved easily
growing radial basis function networks paper presents evaluates two algorithms incrementally constructing radial basis function networks class neural networks suitable control applications popular backpropagation networks first algorithm derived previous method developed second one inspired cart algorithm developed generation regression trees algorithms proved work number tests exhibit comparable performances evaluation standard case study temporal series reported
evolving fuzzy prototypes efficient data clustering number prototypes used represent class position prototype within class membership function associated prototype paper proposes novel evolutionary approach data clustering classification many limitations traditional systems approach number positions fuzzy prototypes using realvalued genetic algorithm acts classes system benefits naturally global information possible class interactions addition concept receptive field prototype used replace classical membership function infinite fuzzy support multidimensional gaussian function prototype unique variance dimension cluster hence notion nearest prototype proposed model completely fuzzy system called data clustering algorithms including popular algorithm require priori knowledge problem domain number starting positions prototypes although knowledge may assumed domains whose dimensionality fairly small whose underlying structure relatively intuitive clearly much less settings number input parameters may large classical systems also suffer fact define clusters one class time hence account made potential interactions among classes drawbacks fact classification typically based fixed membership function prototypes paper proposes novel approach data clustering classification limitations traditional systems model based genetic evolution fuzzy prototypes realvalued genetic algorithm used number positions prototypes acts classes measures fitness classification accuracy system naturally global information class interaction concept receptive field prototype also presented used replace classical fixed function infinite fuzzy support membership function new membership function inspired used hidden layer rbf networks consists multidimensional gaussian function prototype unique variance dimension cluster classification notion nearest prototype proposed model completely fuzzy system called
worstcase quadratic loss bounds prediction using linear functions gradient descent paper study performance gradient descent applied problem online linear prediction arbitrary product spaces prove worstcase bounds sum squared prediction errors various assumptions concerning amount priori information sequence predict algorithms use variants extensions online gradient descent whereas algorithms always predict using linear functions hypotheses results requires data linearly related fact bounds proved total prediction loss typically expressed function total loss best fixed linear predictor bounded norm upper bounds tight within constants matching lower bounds provided cases finally apply results problem online prediction classes smooth functions
online learning smooth functions single variable study online learning classes functions single real variable formed bounds various functions derivatives determine best bounds worstcase sum squared errors also absolute errors several classes prove upper bounds classes smooth functions loss functions prove upper lower bounds terms number trials
utility feature weighting algorithms algorithms known depend heavily distance metric paper investigate use weighted euclidean metric weight feature comes small set options describe algorithm search space discrete weights using crossvalidation error evaluation function although large set possible weights reduce learners bias also lead increased variance overfitting empirical study shows many data sets advantage weighting features increasing number possible weights beyond two zero one little benefit sometimes performance
estimating attributes analysis extensions relief context machine learning examples paper deals problem estimating quality attributes without dependencies among developed algorithm called relief shown efficient estimating attributes original relief deal discrete continuous attributes limited problems paper relief extended deal noisy incomplete multiclass data sets extensions verified various artificial one known realworld problem
analysis nearest neighbor algorithm paper present analysis nearest neighbor algorithm simple induction method studied many researchers analysis assumes conjunctive target concept noisefree boolean attributes uniform distribution instance space calculate probability algorithm will test instance distance prototype concept along probability nearest stored training case distance test instance compute probability correct classification function number observed training cases number relevant attributes number irrelevant attributes also explore behavioral implications analysis presenting predicted learning curves artificial domains give experimental results domains check reasoning
evolution combinatorial optimization john prior report may
coevolution mutation rates order better understand life helpful look beyond life know simple model coevolution implemented addition genes mutation rate individuals made possible evolve also allowed evolution mutation extremely high mutation rates model shows individuals interact sort game maintain relatively high mutation rates however individuals interactions greater consequences one individual interaction tend evolve relatively low mutation rates model suggests different genes may evolved different mutation rates varying interactions genes
machine learning behaviour based robotics new synthesis complexity grows learning task difficult face problem using architecture based learning classifier systems description learning technique used structure proposed present experiments show behaviour acquisition achieved simulated robot learns structural properties animal behavioural organization proposed
bayes optimal instancebased learning paper present probabilistic instancebased learning approach bayesian framework moving construction explicit hypothesis datadriven instancebased learning approach equivalent averaging possibly many individual models general bayesian instancebased learning framework described paper applied set assumptions defining parametric model family discrete prediction task number simultaneously predicted attributes small includes example classification tasks machine learning literature illustrate use suggested general framework practice show approach implemented special case strong independence assumptions underlying called naive bayes classifier resulting bayesian instancebased classifier empirically domain data sets results compared performance traditional naive bayes classifier results suggest bayesian instancebased learning approach yields better results traditional naive bayes classifier especially cases amount training data small
comparative study genetic search present comparative study genetic algorithms search properties treated combinatorial optimization technique done context problem comparison relative metropolis process extension simulated annealing contribution first show large difficult instances contribution crossover search process marginal little running mutation selection metropolis process second show problem instances genetic search consistently performs worse simulated annealing subject similar resource bounds correspondence two algorithms made precise via decomposition argument provides framework results
constructive learners learn constructive induction learners problem representation modified normal part learning process may necessary initial representation inadequate inappropriate however distinction constructive methods appears highly several conventional definitions process constructive induction appear include learning processes paper argue process constructive learning identified relational learning suggest
fast probabilistic modeling combinatorial optimization probabilistic models recently utilized optimization large combinatorial search problems however complex probabilistic models attempt capture dependencies computational costs algorithm presented paper provides method using probabilistic models conjunction fast search techniques show used two different fast search algorithms hillclimbing incremental learning resulting algorithms maintain many benefits probabilistic modeling far less computational extensive empirical results provided successfully applied jobshop scheduling problems paper also presents review probabilistic modeling optimization
stochastic approach inductive logic programming current systems field inductive logic programming ilp use primarily efficiency guided search techniques greedy algorithms suffer local optimization problem present paper describes system named tries problem using stochastic search method based generalization simulated annealing called markovian neural network various tests performed benchmark realworld domains results show advantages weaknesses stochastic approach
radial basis function approach financial time series analysis
finding optima estimating probability densities many optimization problems structure solutions complex relationships different input parameters example experience may certain parameters closely related explored independently similarly experience may establish subset parameters must take particular values search cost landscape take advantage relationships present framework analyze global structure optimization landscape novel efficient algorithm estimation structure derived use knowledge structure guide randomized search solution space turn refine estimate structure technique obtains significant speed gains randomized optimization procedures
study generalization capabilities xcs analyze generalization behavior xcs classifier system environments generalizations done experimental results presented paper evidence generalization mechanism xcs learning even simple tasks environments present new operator named specify solution problem xcs specify operator named compared xcs terms performance generalization capabilities different types environments experimental results show deal greater variety environments robust xcs respect population size
efficient learning selective bayesian network classifiers paper present computationally efficient method inducing selective bayesian network classifiers approach use informationtheoretic metrics efficiently select subset attributes learn classifier explore three conditional informationtheoretic metrics extensions metrics used extensively decision tree learning namely gain gain ratio metrics distance metric experimentally show algorithms based gain ratio distance metric learn selective bayesian networks predictive good better learned existing selective bayesian network induction approaches significantly lower computational cost prove phase algorithms polynomial complexity compared worstcase exponential time complexity corresponding phase
evolutionary design neural architectures preliminary taxonomy guide literature
towards theory optimal similarity measures way learning similarity measure effectiveness casebased reasoning system known depend similarity measure however clear whether similarity measures might improve performance casebased reasoner commonly used measures paper therefore deals problem choosing best similarity measure limited context instancebased learning classifications discrete example space consider fixed similarity measures ones former case give definition similarity measure believe optimal current prior distribution target concepts prove optimality within restricted class similarity measures show optimal similarity measure specific prior distributions conclude simple similarity measure good cases section show definition leads naturally
qlearning problems may viewed markov decision processes mdps potentially large state sets particularly methodology computing optimal policies developed approach reduces problem finding optimal policies original mdp sequence stopping problems whose solutions determine optimal policy socalled indices shown index task state may interpreted particular component function associated process simple mdp standard solution methods computing optimal policies successive approximation apply paper explores problem learning indices online without aid process model suggests utilizing qlearning agents solve includes example online reinforcement learning approach applied simple problem stochastic instance drawn wide class problems may formulated problems
boosting ability topdown decision tree learning algorithms provably optimal decision tree analyze performance topdown algorithms decision tree learning employed widely used c45 cart software main result proof algorithms boosting algorithms mean functions internal nodes decision tree weakly approximate unknown target function topdown algorithms study will weak advantage build tree achieving desired level accuracy bounds obtain show interesting dependence splitting criterion used topdown algorithm precisely functions used internal nodes error approximations target function splitting criteria used cart c45 trees size respectively error thus example small constant advantage random larger constant advantage trees constant size new splitting criterion suggested analysis much stronger preliminary version paper appears proceedings annual symposium theory computing pages press authors addresses research hill new department computer science university supported part science foundation science grant science technology
example stronger version binary tree hypothesis paper describes example hypothesis states greedy decision tree generation algorithm constructs binary decision trees branches single attributevalue pair rather values selected attribute will always lead tree fewer leaves given training set show also relieff less functions enables induction algorithm generates binary decision trees optimal smallest decision trees cases
automatic realworld problems often difficult solved single system many examples natural artificial systems show modular approach reduce total complexity system solving difficult problem success modular artificial neural networks speech image processing typical example however designing modular system difficult task relies heavily human experts prior knowledge problem systematic automatic way form modular system problem paper proposes novel evolutionary learning approach designing modular system automatically without human starting point using technique based fitness sharing genetic algorithms new effort made towards using population complete modular system specialized expertise species entire population rather single individual introducing gating algorithm demonstrate approach automatic improving coevolutionary game learning following earlier researchers learn play iterated review problems earlier coevolutionary learning explain poor generalization ability generalization ability approach significantly better past using specialized expertise entire population though gating algorithm instead best individual main improvement
learning sense physical domains paper describe approach representing using improving sensory skills physical domains present architecture represents control knowledge terms states sequences states system operates cycles state matches environmental situation state control behavior conditions fail finding another matching state higher priority information probability conditions will satisfied minimizes sensing knowledge states likely three statistical learning methods let system gradually reduce sensory load gains experience domain report experimental evaluations ability three simulated physical tasks aircraft balancing pole experiments include studies identify reduction sensing due learning mechanisms others examine effect domain characteristics
exploitation cooperation iterated follow using genetic algorithm play iterated population strategy evaluated performs members current population creates dynamic environment algorithm moving target instead usual evaluation fixed set strategies conduct two sets experiments first set investigates conditions evolve best strategies second set studies robustness strategies thus evolved strategies useful population effective wide variety results indicate population nearly always generations time bias population almost always results confirm cooperation almost always becomes strategy also confirm population expert strategies best done small amounts initial population genetic diversity lack robustness strategies produced evaluation demonstrated examples population exploited strategy causes populations average score less emerge exploiting strategies example evolution back suitable mutation find way reduce make population play extra
unsupervised learning convex coding unsupervised learning algorithms based convex proposed find convex combination basis vectors input learning algorithms produce basis vectors minimize reconstruction error convex algorithm develops locally linear models input algorithm discovers features algorithms used model handwritten digits compared vector quantization principal component analysis neural network implementations involve feedback connections project reconstruction back input layer
unified architecture finite state machine induction although recurrent neural nets successful learning finitestate machines continuous internal state dynamics neural net matched discrete behavior describe architecture called allows discrete states evolve net learning consists standard recurrent neural net trained gradient descent adaptive clustering technique state space based assumption finite set discrete internal states required task actual network state set corrupted noise due weights learns discrete state maximum probability noisy state simulations show leads significant improvement generalization performance earlier neural net approaches induction
boltzmann chains hidden markov models propose statistical framework modeling discrete time series maximum likelihood estimation done via boltzmann learning onedimensional networks weights call networks boltzmann chains show contain hidden markov models hmms special case framework also new architectures address particular shortcomings hmms look two architectures parallel chains model feature sets time scales networks model longterm dependencies hidden states networks show implement boltzmann learning rule exactly polynomial time without simulated annealing necessary computations done exact procedures statistical mechanics
evolutionary approach vector design vector quantization coding technique encoding set vectors different sources image speech design vector yields distortion one challenging problems field source coding however problem known difficult conventional solution technique works process iterative yield locally optimal results paper design evaluate three versions genetic algorithms computing vector preliminary study sources showed genetic approach outperforms conventional technique cases
constructive induction concepts decision trees discuss approach constructing composite features induction decision trees composite features correspond mofn concepts three goals research first explore family greedy methods building mofn concepts one described paper second show concepts formed internal nodes decision trees bias learner finally evaluate method several generated naturally occurring data sets determine effects bias
first order regression present new approach called first order regression handling numerical information inductive logic programming ilp combination ilp numerical regression firstorder logic descriptions induced subspaces numerical regression among realvalued variables program implementation idea numerical regression focused distinguished continuous argument target predicate show viewed generalisation usual ilp problem applications several realworld data sets described prediction modelling dynamics predicting finite element design operators reconstruction electric comparison performance previous results domains indicates effective tool ilp applications involve numerical data
report abstract paper identifies goal handling processes begin account kind processes involved invention identify new kinds goals special properties mechanisms processing goals means integrating social interaction processes focus invention goals address significant associated invention goals represent goals expert around whole knowledge expert grows less invention goals reflect goals among experts increase sensitivity individuals particular events might contribute satisfaction exploration based example invention bell propose mechanisms explain early goals rise new goals multiple new goals finally describe computational model accounts role goals invention
mutation rates order better understand life helpful look beyond life know simple model coevolution implemented addition gene mutation rate individual allowed mutation rate evolve model shows individuals interact sort game maintain relatively high mutation rates however individuals interactions greater consequences one individual interaction tend evolve relatively low mutation rates model suggests different genes may evolved different mutation rates varying interactions genes
finding promising exploration regions weighting expected navigation costs many learning tasks neither free constant cost often cost query depends distance current location state space desired query point much gained instances keeping track length shortest path state every first action take paths information learning agent efficiently explore environment calculating every step action will move towards region estimated exploration benefit balancing exploration potential states encountered far currently estimated
firstorder single layer recurrent neural networks examine representational capabilities firstorder single layer recurrent neural networks neurons show powerful firstorder however firstorder augmented output layers feedforward neurons implement finitestate employed state split two equivalent states use allows efficient implementation finitestate using augmented firstorder
learning past english symbolic pattern connectionist models learning past english aspect language acquisition generated since become task testing cognitive modeling several artificial neural networks anns implemented challenge better symbolic models paper present generalpurpose symbolic pattern based upon decisiontree learning algorithm id3 conduct extensive comparisons generalization ability ann models different representations conclude generalizes past unseen better ann models wide margin offer insights case also discuss new default strategy decisiontree learning algorithms
defining explanation probabilistic systems probabilistic systems gain wider use need mechanism explains systems findings becomes critical system will also need mechanism ordering competing explanations examine two representative approaches explanation literature one due one due show suffer significant problems propose approach defining notion better explanation combines features together recent work pearl others causality
evolving complex structures via coevolution cooperative coevolutionary approach learning complex structures presented although preliminary nature appears number advantages approaches cooperative coevolutionary approach parallel evolution interact useful ways form complex higher level structures architecture designed general enough permit appropriate priori knowledge form initial biases towards particular kinds brief summary initial results obtained testing architecture several problem domains presented shows significant speedup traditional approaches
analytic comparison nonlinear norm bounding techniques low order systems saturation
learning environment experimentation need examples intelligent system must able adapt learn correct update model environment incrementally complex environments many parameters interactions cost sampling possible range states test results action practical approach present practical approach based continuous selective interaction environment type fault domain knowledge causes behavior environment experimentation additional information needed correct systems knowledge
pruning recurrent neural networks improved generalization performance determining architecture neural network important issue learning task recurrent neural networks general methods exist permit estimation number layers hidden neurons size layers number weights present simple pruning heuristic significantly improves generalization performance trained recurrent networks illustrate heuristic training fully recurrent neural network positive negative strings regular grammar also show rules extracted networks trained recognize strings rules extracted pruning consistent rules learned performance improvement obtained pruning networks simulations shown training pruning recurrent neural net strings generated two regular grammars state grammar state parity grammar simulations indicate pruning method gives generalization performance superior obtained training weight decay
characterizing generalization performance model selection strategies investigate structure model selection problems via decomposition particular characterize essential structure model selection task bias variance generates sequence hypothesis classes leads new understanding methods first penalty terms effect particular profile function model complexity true match systematic overfitting results depending whether penalty terms large small second usually best according true task therefore fixed strategy optimal across problems use characterization identify notion easy hard model selection problems particular show variance profile grows rapidly relation biases standard model selection techniques become significant errors happen example regression independent variables drawn distributions finally discuss new model selection strategy dramatically outperforms standard methods hard tasks
combining estimates regression classification consider problem combine collection general regression fit vectors order obtain better predictive model individual may subset linear regression ridge regression complex like neural network develop general framework problem examine recent proposal called context combination methods based bootstrap analytic methods also derived compared number examples including best subsets regression regression trees finally apply ideas classification problems estimated combination weights yield insight structure problem
machine learning compare performance explanation abilities several machine learning algorithms problem predicting among different algorithms naive bayesian classifier seem appropriate analyze combination decisions several classifiers solving prediction problem show combined classifier improves performance explanation ability
using recurrent networks fit sequential inputoutput data paper suggests use activation functions fully recurrent neural networks main theoretical advantage principle problem recovering internal coefficients inputoutput data closed form
island model genetic algorithms linearly separable problems parallel genetic algorithms often reported yield better performance genetic algorithms use single large population case island model genetic algorithm argued multiple helps preserve genetic diversity since island potentially follow different search trajectory search space hand linearly separable functions often used test island model genetic algorithms possible island models particular suited separable problems look island models track multiple search trajectories using infinite population models simple genetic algorithm also introduce simple model better understanding island model genetic algorithms may advantage processing linearly separable problems
noise effective regularization technique bootstrap samples noise shown effective smoothness capacity control technique training feedforward networks statistical methods generalized additive models shown noisy bootstrap performs best conjunction weight decay regularization ensemble averaging problem highly nonlinear noisefree data used demonstrate findings combination noisy bootstrap ensemble averaging also shown useful generalized additive modeling also demonstrated known data
priors component structures autoregressive time series models new approaches prior specification autoregressive time series models introduced developed focus defining classes prior distributions parameters latent variables related latent components autoregressive model observed time series new priors naturally permit qualitative quantitative prior information number relative importance components represent low frequency high frequency residual noise components observed series class priors also naturally incorporates uncertainty model order hence leads posterior analysis model order assessment resulting posterior predictive inferences incorporate full uncertainties model order model parameters analysis also formally incorporates uncertainty leads inferences unknown initial values time series predictions future values posterior analysis involves easily implemented iterative simulation methods developed described one applied field evaluation latent structure especially structure critical importance connection issues global variability explore analysis data index one several series central recent sciences recent apparent indicators
bayesian inference component spectral structure time series summary detail illustrate time series analysis spectral inference autoregressive models focus underlying latent structure time series novel class priors parameters latent components leads new class smoothness priors autoregressive coefficients provides formal inference model order including high order models leads uncertainty model order summary inferences class prior models also allows subsets unit hence leads inference though stochastically timevarying time series applications analysis frequency time series time spectral domains illustrated study time series analyses demonstrates impact utility new class priors addressing model order uncertainty allowing unit root structure time domain decomposition time series estimated latent components provides important alternative view component spectral characteristics series addition data analysis illustrates utility smoothness prior unit root structure inference spectral densities particular framework problems spectral estimation autoregressive models using traditional model fitting methods
provably convergent dynamic training method multilayer perceptron networks paper presents new method training multilayer perceptron networks called dynamic multilayer perceptron method based upon approach builds networks form binary trees dynamically nodes layers needed individual nodes network trained using algorithm method capable handling realvalued inputs proof given concerning convergence properties basic model simulation results show performs favorably comparison learning algorithms
role representation search training regime architecture player playing program similar approach uses artificial neural network trained method temporal difference learning learn play game paper discusses relative contribution board representation search depth training regime architecture run time parameters strength produced system keywords temporal difference learning input representation search
using machine learning methods international conflict databases last research machine learning developed variety powerful tools inductive learning data analysis hand research international relations developed variety different conflict databases mostly analyzed classical statistical methods databases general symbolic nature provide interesting domain application machine learning algorithms paper gives short overview available conflict databases subsequently application machine learning methods analysis interpretation databases
selection relevant features examples machine learning survey review work machine learning methods handling data sets containing large amounts irrelevant information focus two key issues problem selecting relevant features problem selecting relevant examples describe advances made topics empirical theoretical work machine learning present general framework use compare different methods close challenges future work area
exploratory modelling multiple nonstationary time series latent process structure describe illustrate bayesian approaches modelling analysis multiple nonstationary time series begins univariate models related time series driven underlying processes dynamic latent factor processes focus models factor processes hence observed time series timevarying capable representing observed nonstationary characteristics concepts new methods time series decomposition infer characteristics latent components time series relate univariate decomposition analyses underlying multivariate dynamic factor structure application analysis multiple eeg traces ongoing eeg study study individuals generate multiple eeg traces various locations interest lies identifying dependencies across series addition multivariate nonstationary aspects series area provides new results decomposition time series latent components illustrated data analysis one eeg data set paper also discusses current future research directions research supported part national science foundation grant eeg data context discussions university medical center interactions valuable address correspondence institute statistics decision sciences university
efficient subsumption based graph algorithms subsumption problem crucial efficiency ilp learning systems discuss two subsumption algorithms based strategies suitable matching literals class clauses subsumption becomes polynomial deterministic clauses map general problem subsumption certain problem finding clique fixed size graph return show specialization pruning strategy clique algorithm provides reduction subsumption search space also present empirical results design data set
evaluating effectiveness derivation replay statespace planning casebased planning involves individual instances problemsolving episodes using new planning problems paper concerned derivation replay main component form casebased planning called analogy prior study implementations derivation replay based within statespace planning motivated superiority planners plan generation demonstrate planning also advantage replay will argue planning derivation order execution order plan steps provided planners enables exploit guidance previous cases efficient straightforward fashion hypothesis focused empirical comparison
stochastic background knowledge wellknown fact propositional learning algorithms require good features perform practice major step data engineering inductive learning construction good features domain experts features often represent properties structured objects property typically certain certain properties process feature engineering algorithm searches features defined algorithm stochastically topdown search firstorder clauses clause represents binary feature differs existing algorithms search capable considering clauses context almost arbitrary length size preliminary experiments support view approach promising
neural network model gold market neural network predictor gold market presented simple recurrent neural network trained recognize points gold market based history ten market indices network tested data held back training significant amount predictive power observed point predictions used time transactions gold gold mining stock index markets obtain significant paper test period training data daily prices ten input markets period five years point targets labeled training phase without help financial expert thus experiment shows useful predictions made without use extensive market data knowledge
causal discovery via mml learning causal models sample data key step toward incorporating machine learning decisionmaking reasoning uncertainty paper presents bayesian approach discovery causal models using minimum message length mml method developed encoding search methods discovering linear causal models initial experimental results presented paper show mml induction approach causal models generated data quite accurate original models results compare favorably program even algorithm supplied prior temporal information mml
reinforcement learning probability matching present new algorithm associative reinforcement learning algorithm based upon idea matching networks output probability probability distribution derived environments reward signal probability matching algorithm shown perform faster less local minima previously existing algorithms use probability matching train mixture experts networks architecture reinforcement learning rules fail converge reliably even simple problems architecture particularly suited algorithm compute arbitrarily complex functions yet calculation output probability simple
weighting features many casebased reasoning algorithms retrieve cases using knearest neighbor knn classifier whose similarity function sensitive irrelevant interacting noisy features many proposed methods reducing sensitivity similarity function feature weights focus methods automatically assign weight settings using little knowledge goal predict relative capabilities methods specific dataset characteristics introduce framework automated methods empirically compare methods along one dimensions summarize results four hypotheses describe additional evidence supports investigation methods correctly assign low weights completely irrelevant features methods use performance feedback demonstrate three advantages methods require less preprocessing better tolerate interacting features learning rate
inductive learning characteristic concept descriptions paper deals problem learning characteristic concept descriptions examples describes new generalization approach implemented system approach tries take advantage information induced descriptions objects using conceptual clustering algorithm experimental results various realworld domains strongly support hypothesis new approach correct possibly concept descriptions methods induced concept descriptions also used classify objects belong concepts present training data set paper describes generalization approach implemented presents experimental results obtained relational propositional real world data set
local selection local selection simple selection scheme evolutionary algorithms individual compared fixed threshold rather decide coupled fitness functions shared environmental resources maintains diversity way similar fitness sharing however generally efficient fitness sharing parallel implementations distributed tasks convergence applies minimal selection pressure upon population therefore appropriate stronger selection schemes certain problem classes papers one broad class problems consistently performs selection
stability potential combined images cover classification demonstrated test site recent papers goal develop classification algorithm stable terms applicability different regions unlike optical sensing techniques sensing provide data image signal solely determined physical structural electrical properties targets surface near hence classifier based object classes applicable new images without need train classifier article discusses design applicability classification algorithm based measured image data applicability compared two different test sites site found classes separate certain boundary conditions like comparable conditions observed
averaging data presenting analyzing results experiments data averaging data proceedings national conference artificial intelligence press california presenting analyzing results experiments abstract experimental results reported machine learning literature paper investigates common processes data averaging results terms mean standard results multiple trials data context neural networks one popular machine learning models processes result results conclusions demonstrate easily happen propose techniques avoiding important problems data averaging common presentation assumes distribution individual results gaussian however investigate distribution common problems find often approximate gaussian distribution may symmetric may multimodal show assuming gaussian distributions significantly affect interpretation results especially comparison studies controlled task find distribution performance towards better performance target functions towards worse performance complex target functions propose new guidelines performance provide information actual distribution data demonstrate optimization performance via experimentation multiple parameters lead significance assigned results due suggest precise descriptions experimental techniques important evaluation results need potential data biases experimental techniques selecting test procedure additionally important rely appropriate statistical tests ensure assumptions made tests valid distribution
survey research genetics brief survey biological research noncoding dna presented growing interest effects noncoding segments evolutionary algorithms better understand conduct research noncoding segments important understand biological background work paper begins review basic genetics describes different types noncoding dna surveys recent research
learning team strategies multiple agents soccer case study use simulated soccer study multiagent learning teams agents share action set policy may behave due inputs agents making team case goals conduct simulations varying team sizes compare two learning algorithms learning linear neural networks probabilistic incremental program evolution based evaluation functions mapping pairs expected reward searches policy space directly uses adaptive probabilistic prototype trees programs calculate action probabilities current inputs results show several difficulties learning appropriate shared however depend find good policies faster reliably suggests multiagent learning scenarios direct search policy space offer advantages approaches
changing functions stable systems consider problem characterizing possible functions given nonlinear system provide result allows freedom modification functions
combining linear discriminant functions neural networks supervised learning novel supervised learning method presented combining linear discriminant functions neural networks proposed method results treestructured hybrid architecture due constructive learning binary tree hierarchical architecture automatically generated controlled growing process specific supervised learning task unlike decision tree linear discriminant functions employed intermediate level tree partitioning large complicated task several smaller simpler proposed method component neural networks leaves tree constructive learning growing algorithms developed serve hybrid architecture proposed architecture provides efficient way apply existing neural networks perceptron solving large scale problem already applied proposed method universal approximation problem several benchmark classification problems order evaluate performance simulation results shown proposed method yields better results faster training comparison perceptron
control knowledge knowledgebased systems machine learning knowledge engineering machine learning knowledge engineering always strongly related introduction new representations knowledge engineering created gap paper describes research aimed applying machine learning techniques current knowledge engineering representations propose system part knowledge based system called control knowledge claim strong similarity redesign knowledge based systems incremental machine learning finally will relate work existing research
contextsensitive feature selection lazy learners
effective size neural network principal component approach often learning data one penalty term standard error term attempt simple models overfitting current penalty terms neural networks however often take account weight interaction critical since effective number parameters network usually differs dramatically total number possible parameters paper present penalty term uses principal component analysis help detect functional neural network results show new algorithm gives much accurate estimate network complexity standard approaches result new term able improve techniques make use penalty term weight decay weight pruning feature selection bayesian
functions predicting problem complexity
effect decision surface fitness dynamic multilayer perceptron networks dynamic multilayer perceptron network training method based upon approach builds networks form binary trees dynamically nodes layers needed paper introduces method compares using standard rule training method training individual nodes performance using genetic algorithm training basic model require use genetic algorithm training individual nodes results show convergence properties enhanced use genetic algorithm appropriate fitness function
kritik early casebased design system developed one early casebased design systems called kritik kritik generated preliminary conceptual qualitative designs physical devices retrieving adapting past designs stored case memory case system associated sbf device model explained structure device accomplished functions device models guided process modifying past design meet functional specification new design problem device models also design modifications new complete implementation kritik paper take view kritik early papers described kritik integrating casebased modelbased reasoning integration kritik also computational process casebased reasoning sbf content theory device sbf models provide methods many specific tasks casebased design design adaptation also provide whole process casebased design retrieval old cases storage new ones believe essential building theories casebased design
learning bayesian networks incomplete data much current research learning bayesian networks fails effectively deal missing data methods assume data complete make data complete using fairly methods methods deal missing data learn conditional probabilities assuming structure known present principled approach learn bayesian network structure conditional probabilities incomplete data proposed algorithm iterative method uses combination expectationmaximization techniques results presented synthetic data sets show performance new algorithm much better methods handling missing data
planning domain
learning coordinate without sharing information researchers field distributed artificial intelligence developing efficient mechanisms coordinate activities multiple autonomous agents need coordination arises agents share resources expertise required achieve goals previous work area includes using sophisticated information exchange investigating heuristics developing formal models possibilities conflict cooperation among agent order handle changing requirements continuous dynamic environments propose learning means provide additional possibilities effective coordination use reinforcement learning techniques block problem show agents learn policies follow desired path without knowledge theoretically analyze experimentally effects learning rate system convergence demonstrate benefits using learned coordination knowledge similar problems reinforcement learning based coordination achieved cooperative domains domains noisy communication channels stochastic characteristics present challenge using coordination schemes
comparative study id3 backpropagation english mapping performance error backpropagation id3 learning algorithms compared task mapping english text distributed output code developed sejnowski shown consistently outperforms id3 task several points three hypotheses explaining difference explored id3 overfitting training data able share hidden units across several output units hence learn output units better captures statistical information id3 conclude hypothesis correct id3 simple statistical learning procedure performance matched complex statistical procedures improve performance id3 substantially study residual errors suggests still substantial improvement learning methods mapping
acquiring mapping meaning thank programming analysis running simulations code division engineering block grant time san center members research groups helpful comments earlier versions work
generation gaps recent interest socalled steady state genetic algorithms gas among things replace individuals typically generation fixed size population size understanding advantages andor fraction population generation rather entire population goal research considerable progress understanding gas since overlapping generations remains somewhat issue however recent theoretical empirical results provide background much understanding issue paper review combine extend results way significantly insight
recognition exploitation contextual via incremental extended version daily experience shows real world meaning many concepts heavily depends implicit context changes context cause less changes concepts incremental concept learning domains requires ability recognize adapt changes paper presents solution incremental learning tasks domain provides explicit current context attributes characteristic values present general learning model realization system named learn detect certain types contextual context change model consists base level learner performs regular online learning classification task identifies potential contextual context learning detection occur regular online learning without separate training phases context recognition experiments synthetic domains realworld problem show robust variety dimensions produces substantial improvement simple learning situations changing contexts
creative design reasoning understanding paper investigates memory issues influence long term creative problem solving design activity taking casebased reasoning perspective exploration based example invention bell abstract reasoning understanding mechanisms appear time longterm creative design identify understanding mechanism responsible analogical design constraints analogical evaluation casebased design already understood design satisfy design problems still active background new mechanisms integrated computational model accounts creative
multiagent reinforcement learning independent cooperative agents intelligent human agents exist cooperative social environment facilitates learning learn also cooperation sharing information experience learned knowledge key paper given number reinforcement learning agents will cooperative agents outperform independent agents learning price cooperation using independent agents benchmark cooperative agents studied following ways sharing sharing episodes sharing learned policies paper shows additional another agent used efficiently sharing learned policies episodes among agents learning cost communication joint tasks agents significantly outperform independent agents although may learn beginning tradeoffs just limited multiagent reinforcement learning
genetic algorithms splitting paper submitted international conference genetic algorithms address address
application ilp musical database learning describe foil uses advanced stochastic search heuristic application learning application required learning relation training instances able efficiently deal learning task knowledge one complex learning task solved ilp system demonstrates ilp systems scale real databases topdown ilp systems use covering approach advanced search strategies appropriate knowledge discovery databases promising investigation
theory visual relative motion perception grouping binding organization human visual system sensitive relative motion objects absolute motion understanding motion perception requires understanding neural circuits group moving visual elements relative one another based upon hierarchical reference modeled visual relative motion perception using neural network architecture groups visual elements according principles exploits information behavior group predict behavior individual elements simple competitive neural circuit visual elements together representation visual object information spiking pattern neurons allows transfer object representation location location neural circuit object moves model exhibits characteristics human object grouping solves key neural circuit design problems visual relative motion perception
problem solving redesign analysis complex tasks like diagnosis design give better understanding tasks terms goals aim achieve different ways achieve goals paper present analysis redesign redesign viewed family methods based common principles number dimensions along redesign problem solving methods vary distinguished examining problemsolving behavior number existing redesign systems approaches collection problemsolving methods redesign developed structure redesign constructing system redesign large number choices decisions made order describe relevant choices redesign problem solving extend current notion possible relations tasks methods architecture realization task problemsolving method decomposition problemsolving method common relations architecture however suggest extend relations notions task refinement method refinement notions represent intermediate decisions structure task method refined without attention terms explicit representation kind intermediate decisions helps make represent decisions fashion
estimation dirichlet process mixture models bayesian density estimation prediction using dirichlet process mixtures standard exponential family distributions precision total parameter mixing dirichlet process critical strongly influences resulting inferences numbers mixture components note shows respect flexible class prior distributions parameter posterior may represented simple conditional form easily simulated result inference key may developed existing gibbs sampling algorithms fitting mixture models concept data important developing extension existing algorithm final section notes simple asymptotic posterior
strategies parallel training simple recurrent neural networks
unsupervised neural network learning procedures feature extraction classification technical report center neural systems university
using neural networks automatically refine expert system knowledge bases experiments max domain paper describe study applying knowledgebased neural networks problem local loops currently uses expert system called max aid human experts however effective learning algorithm place max allow easy different maintenance centers easy updating changes find machine learning algorithms better accuracy max neural networks perform better decision trees iii neural network ensembles perform better standard neural networks knowledgebased neural networks perform better standard neural networks ensemble knowledgebased neural networks performs best
learning onedimensional geometric patterns random misclassification noise
visual motion development intrinsic connections human vision systems integrate information across long spatial example moving stimulus appears viewed briefly yet viewed longer suggests visual systems combine information along trajectory matches motion stimulus selforganizing neural network model shows developmental moving stimuli direct formation motion integration pathways representations moving stimuli results account data potentially also model phenomena visual
explaining explaining away explaining away common pattern reasoning one cause observed event reduces need alternative causes explaining away also occur one cause increases belief another provide general qualitative probabilistic analysis reasoning identify property interaction among causes product determines form reasoning appropriate product extends qualitative probabilistic network formalism support qualitative inference directions change probabilistic belief relation also occams razor pruning search likely portions paper originally appeared proceedings second international conference principles knowledge representation reasoning supported national science foundation grant international science center
simulating access hidden information learning introduce new technique enables learner without access hidden information learn nearly learner access hidden information apply technique solve open problem showing concept class least number queries sufficient learning algorithm access arbitrary equivalence queries factor log least number queries sufficient learning algorithm access arbitrary equivalence queries membership queries previously known results imply log bound best possible describe analogous results two generalizations model function learning apply results bound difficulty learning models terms difficulty learning easier model bound difficulty learning concepts class terms difficulty learning bound difficulty learning noisy environment deterministic algorithms terms difficulty learning noisefree environment apply variant technique develop algorithm transformation allows probabilistic learning algorithms nearly cope noise second variant enables improve general lower bound paclearning model queries finally show many membership queries never help obtain computationally efficient learning algorithms supported air force office scientific research grant work done author supported fellowship
evolution navigation real mobile robot paper describe evolution discretetime recurrent neural network control real mobile robot experiments evolutionary procedure carried entirely physical robot without human show autonomous development set behaviors achieved constraints design interactions employed preliminary experiment emergent behavior based autonomous development internal neural topographic map allows robot choose appropriate trajectory function location remaining energy
evolution topology weights neural networks using genetic programming genetic programming methodology program development consisting special form genetic algorithm capable handling parse trees representing programs successfully applied variety problems paper new approach construction neural networks based genetic programming presented linear combined graph representation network new operators introduced allow evolution architecture weights simultaneously without need local weight optimization paper describes approach operators reports results application model several binary classification problems
best right now beyond greedy exploration
conceptual analogy conceptual clustering efficient analogical reasoning conceptual analogy general approach applies conceptual clustering concept representations facilitate efficient use past experiences cases analogical reasoning approach developed implemented see also support design nets building engineering paper task outlines conceptual clustering applied large amounts structured cases case classes provides concept representation used characterize case classes shows analogous solution new problems based concepts available however main purpose paper evaluate terms reasoning efficiency capability derive solutions beyond cases case base still preserve quality cases
efficient nonparametric estimation probability density functions accurate fast estimation probability density functions crucial satisfactory computational performance many scientific problems type density known priori problem becomes statistical estimation parameters observed values nonparametric case usual estimators make use kernel functions sequence random variables estimated probability density function kernel method computation values requires operations since kernel needs evaluated every propose sequence special weight functions nonparametric estimation requires almost linear time growing function increases without bound method requires arithmetic operations derive conditions convergence number metrics turn similar required convergence kernel based methods also discuss experiments different distributions compare efficiency accuracy computations kernel based estimators various values
advances neural information processing systems active learning multilayer perceptrons propose active learning method reduction multilayer perceptrons mlp first review active learning method point many methods applied mlp critical problem information matrix may solve problem derive condition information matrix propose active learning technique applicable
localized basis function networks nonlinear system estimation control stable neural network control estimation may viewed formally merging concepts nonlinear dynamic systems theory tools multivariate approximation theory paper extends earlier results adaptive control estimation nonlinear systems using gaussian radial basis functions online generation sampled networks using tools multiresolution analysis wavelet theory yields much compact efficient system representations preserving global stability approximation models employing basis functions localized space spatial frequency measure approximated functions spatial frequency content directly dependent reconstruction error result models means adaptively selecting basis functions according local spatial frequency content approximated function algorithm stable online adaptation output weights simultaneously node configuration class nonparametric models wavelet basis functions presented asymptotic bound error networks reconstruction derived shown dependent solely minimum approximation error associated steady state node configuration addition prior bounds temporal system identified controlled used develop criterion online selection radial ridge wavelet basis functions thus reducing rate increase networks size dimension state vector experimental results obtained using network predict path unknown light object air based robotic system given illustrate networks performance simple realtime application
bias stability bias stability bias research bias machine learning algorithms generally concerned impact bias predictive accuracy believe factors also play role evaluation bias one factor stability algorithm words results obtain two sets data phenomenon underlying probability distribution like learning algorithm induce approximately concepts sets data paper introduces method stability based measure agreement concepts also discuss relationships among stability predictive accuracy bias
performance simultaneous tuning selective pressure recombination many genetic algorithms applications objective find nearoptimal solution using limited amount computation given requirements difficult find good balance exploration exploitation usually balance found tuning various parameters like selective pressure population size mutation crossover rate genetic algorithm alternative propose simultaneous tuning selective pressure recombination operators experiments show combination proper selective pressure highly recombination operator yields superior performance reduction mechanism used strong influence optimal crossover using worst fitness strategy building blocks present current best individuals always crossover operator maintain good building blocks allows tune crossover improve search better individuals
computational learning theory natural systems chapter crossvalidation theories crossvalidation crossvalidation frequently used intuitively technique estimating accuracy theories learned machine learning algorithms testing machine learning algorithm foil new databases developed crossvalidation interesting phenomenon one theory found repeatedly responsible little crossvalidation error whereas theories found tend responsible majority crossvalidation error believe frequently found theory theory may accurate classifier unseen data theories however experiments showed theories accurate unseen data theories found less frequently crossvalidation theories may useful predicting crossvalidation poor estimate true accuracy offer explanations correspondence department computer science engineering university california san
learning controllers industrial robots one significant cost factors robotics applications design development realtime robot control software control theory helps linear controllers developed sufficiently support generation nonlinear controllers although many cases control nonlinear control essential achieving high performance paper discusses machine learning applied design nonlinear controllers several alternative function approximators including multilayer perceptrons mlp radial basis function networks fuzzy controllers analyzed compared leading definition two major families open field function function approximators locally receptive field function approximators shown fuzzy controllers bear strong similarities symbolic interpretation characteristics allows applying symbolic learning algorithms network set examples possibly background knowledge three integrated learning algorithms two original described evaluated experimental test cases first test case provided robot task whereas second represented classical prediction task time series experimental comparison appears fuzzy controllers examples excellent approximators practice even accurate
evolving robot behaviors paper discusses use evolutionary computation evolve behaviors exhibit emergent intelligent behavior genetic algorithms used learn navigation avoidance behaviors robots learning performed simulation resulting behaviors used control actual robot emergent behavior described detail
proceedings structural evaluation similarity important aspects human analogical processing paper explores modeled using sme simulation theory focus structural evaluation several principles plausible algorithms follow introduce specificity representations include information demonstrate via computational experiments structural evaluation performed including choice normalization technique preference implemented
study genetic algorithms find approximate solutions hard problems genetic algorithms used solve hard optimization problems ranging problem quadratic assignment problem show simple genetic algorithm used solve optimization problem derived conjunctive normal form problem separating populations small parallel genetic algorithms exploits inherent parallelism genetic algorithms convergence genetic algorithms using hillclimbing conduct genetic search space local optima hillclimbing less computationally expensive genetic search examine effectiveness techniques improving quality solutions problems
learning game evaluation functions hierarchical neural architectures
object oriented simulation environment structured connectionist nets class project report physics simulator structured development structured characterized need flexibility efficiency support design reuse modular take position fast objectoriented language like appropriate implementation achieve goals consists hierarchy classes correspond simulation new connectionist models realized combining specializing classes possible separated functional modules order keep basic hierarchy simple possible
induction decision trees recent years researchers made considerable progress worstcase analysis inductive learning tasks theoretical results impact practice must deal average case paper present analysis simple algorithm induces decision trees concepts defined single relevant attribute given knowledge number training instances number irrelevant attributes amount class attribute noise class attribute distributions derive expected classification accuracy entire instance space examine predictions analysis different settings domain parameters comparing results check reasoning
machine learning examples estimating attributes explanation ability compare performance several machine learning algorithms problem knearest algorithm bayesian classifier backpropagation weight elimination learning neural networks lookahead feature construction algorithm algorithms induction decision trees using information gain relieff search heuristics respectively compare accuracy explanation ability different classifiers among different algorithms bayesian classifier seem appropriate analyze combination decisions several classifiers solving prediction problems show combined classifier improves performance explanation ability
making sme greedy engine sme successfully modeled several aspects human consistent interpretations analogy useful theoretical aspect algorithm computationally inefficient sme contains mechanism focusing interpretations relevant goals paper describes modifications sme overcome describe greedy algorithm efficiently computes approximate best interpretation generate alternate interpretations necessary describe technique focuses mapping produce relevant yet novel inferences illustrate techniques via example evaluate performance using empirical data theoretical analysis analogical processing however two significant drawbacks sme constructs
understanding variance bias estimation research report
shrinkage functions procedure proven valuable signal nonparametric regression based principle wavelet coefficients towards zero remove noise broad asymptotic properties paper introduce new shrinkage scheme generalizes hard soft shrinkage study properties shrinkage functions demonstrate shrinkage offers advantages hard shrinkage uniformly smaller risk less sensitivity small perturbations data soft shrinkage smaller bias overall risk also construct approximate confidence intervals address problem threshold selection
algorithm active data collection learning feasibility study neural networks university technical report department computing school university new
attribute estimation regression one key issues discrete continuous class prediction machine learning general seems problem estimating quality attributes heuristic measures mostly assume independence attributes therefore successfully used domains strong dependencies attributes relief extension relieff statistical methods capable correctly estimating quality attributes classification problems strong dependencies attributes following analysis relieff extended continuous class problems relieff relieff provide unified view estimation quality attributes experiments show successfully estimates quality attributes used learning regression trees
global optimization means evolutionary algorithms
learning abduction investigate abduction induction integrated common learning framework notion abductive concept learning extension inductive logic programming ilp case background target theory abductive logic programs abductive notion entailment used coverage relation framework possible learn incomplete information examples exploiting reasoning abduction paper presents basic framework main characteristics illustrates potential addressing several problems ilp learning incomplete information multiple predicate learning algorithm developed extending topdown ilp method concept learning integrating abductive proof procedure abductive logic programming prototype system developed applied learning problems incomplete information particular role integrity constraints investigated showing hybrid learning framework integrates discriminant characteristic settings ilp
markov games framework multiagent reinforcement learning markov decision process mdp reinforcement learning single adaptive agent environment defined probabilistic transition function view secondary agents part environment therefore fixed behavior framework markov games allows view include multiple adaptive agents interacting competing goals paper considers step direction exactly two agents opposed goals share environment describes algorithm finding optimal policies demonstrates application simple game optimal policy probabilistic
soccer team coordination genetic programming genetic programming promising new method automatically generating functions algorithms natural selection contrast learning methods genetic automatic programming makes natural approach developing algorithmic robot behaviors paper present overview apply genetic programming team coordination soccer server domain result just soccer algorithm team learned play reasonable game soccer
selection behavior small robot evolved artificial neural networks control behavior small robots task many grid possible fixed period time number simulated robots small robot controlled processor performance compared simulations observed evolution effective means program control progress characterized periods improvement separated periods levels complexity simulated realized robots quite similarly realized robots cases simulated ones introducing random noise simulations improved fit somewhat hybrid selection evolutionary robots discussed
evolving behavioral strategies domain utilized conduct research distributed artificial intelligence genetic programming used evolve behavioral strategies agents utility strategies population allowed evolve time expected competitive learning cycle surface investigated simple algorithm consistently able capture algorithms
evolutionary algorithms old strategies optimization adaptation genetic algorithms evolution strategies main class algorithms based model natural evolution discussed basic working mechanisms differences application possibilities mechanism strategy parameters within evolution strategies turns major difference genetic algorithms since allows online adaptation strategy parameters without control
boosting trees classifications paper explores two boosting techniques tree classification situation misclassification costs change often one like one induction use induced model different misclassification costs thus robustness induced model cost changes combining multiple trees gives robust predictions change demonstrate ordinary boosting combined minimum expected cost criterion select prediction class good solution situation also introduce variant ordinary boosting procedure utilizes cost information training show proposed technique performs better ordinary boosting terms misclassification cost however technique requires induce set new trees every time cost changes empirical investigation also reveals interesting behavior boosting decision trees classification
incremental lifetime multiagent reinforcement learning previous approaches multiagent reinforcement learning either limited heuristic nature main reason agents environment continually changes learning keep changing traditional reinforcement learning algorithms properly deal convergence theorems require trials strong typically markovian assumptions environment paper however use novel general sound method multiple reinforcement learning living single life limited computational resources changing environment method called incremental properly takes account learns point may affect learning conditions later point learning algorithm embedded policy improve performance principle also improve way improves etc certain times life uses estimate single training example namely entire life far previously learned things still useful start based efficient procedure guaranteed make learning history history longterm reinforcement experiments demonstrate effectiveness one experiment learns sequence complex function approximation problems another multiagent system consisting three learns interesting stochastic strategies
strategy adaptation competing genetic algorithm depends set control parameters genetic operators paper shown strategy adaptation competing makes robust efficient uses different strategy numerical results number test functions
analogical problem solving adaptation schemes present computational approach acquisition problem schemes learning application analogical problem solving work background automatic program construction relies concept recursive program schemes contrast usual approach cognitive modelling computational models designed fit specific data propose framework describe certain empirically established characteristics human problem solving learning uniform formally sound way
road genetic algorithms fitness landscapes performance genetic algorithms gas play major role many systems often little detailed understanding performs little theoretical basis characterize types fitness landscapes lead successful performance paper propose strategy addressing issues strategy consists defining set features fitness landscapes particularly relevant experimentally studying various configurations features affect gas performance along number dimensions paper describe initial set proposed feature classes describe detail one class road functions present initial experimental results concerning role crossover building blocks landscapes constructed features class
neural network exploration using optimal experiment design consider question one act goal learn much possible building theoretical results apply techniques optimal experiment design guide selection neural network learner demonstrate techniques allow learner minimize generalization error exploring domain efficiently completely conclude much offer especially domains high computational costs report describes research done center biological computational learning artificial intelligence laboratory massachusetts institute technology support center provided part grant national science foundation contract asc9217041 author also atr human information processing laboratories siemens research nsf grant
case base exploration tool software tool interactive exploration case base integrated environment provides range display functions make possible knowledge extraction set cases motivated application training cases describe past used detect dependencies data acquire practical planning complex data clustering similar cases machine learning techniques selecting relevant features clustering cases forecasting unknown values adapted case base exploration
usefulness diagnostic solutions recent studies planning comparing plan reuse plan generation shown tasks may degree computational complexity even deal similar problems aim paper show kind results apply also diagnosis propose theoretical complexity analysis coupled experimental tests intended evaluate adaptation strategies reuse solutions past diagnostic problems order build solution problem solved results analysis show even diagnosis reuse complexity class diagnosis generation npcomplete problems practical advantages obtained exploiting hybrid architecture combining casebased modelbased diagnostic problem solving unifying framework
growing output space selforganizing feature map
pattern analysis synthesis attractor neural networks representation hidden variable models attractor neural networks studied memories stored dynamical attractor continuous fixed points illustrated linear nonlinear networks hidden neurons pattern analysis synthesis forms pattern recall stored memory analysis synthesis linear network performed bottomup topdown connections nonlinear network analysis computation additionally requires product hidden neurons one popular approach sensory processing based generative models assume sensory input patterns synthesized underlying hidden variables example speech synthesized sequence images face synthesized pose variables hidden variables useful simpler representation variables sensory input using generative model sensory processing requires method pattern analysis given sensory input pattern analysis hidden variables synthesized words analysis synthesis number approaches pattern analysis synthetic model embedded inside negative feedback another approach construct separate analysis paper explores third approach pairs embedded attractive fixed points attractors state space recurrent neural network attractors memories stored network analysis synthesis forms pattern recall memory approach illustrated linear nonlinear network architectures networks synthetic model linear principal
decision graphs extension decision trees technical report appeared statistics abstract paper examine decision graphs generalization decision trees present inference scheme construct decision graphs using minimum message length principle empirical tests demonstrate scheme compares decision tree inference schemes work provides metric comparing relative decision tree decision graph particular domain
reinforcement driven information acquisition nondeterministic environments agent living nondeterministic markov environment theory way acquiring information statistical properties answer design optimal sequences experiments performing action sequences maximize expected information gain notion implemented combining concepts information theory reinforcement learning experiments show resulting method reinforcement driven information acquisition explore certain much faster conventional random exploration
method vector quantization improvement inspired neural networks internal report
learning incomplete boundary queries using split graphs extended abstract consider learnability membership queries presence incomplete information incomplete boundary query model introduced assumed membership queries instances near boundary target concept may receive know answer show threshold functions efficiently learnable model learning algorithm uses split graphs boundary region generalization split give algorithm boundary region constant greater use notion concepts appropriate model
performance model knowledgebased systems techniques validation directed functional properties programs however properties programs also essential paper describes model average computing time knowledgebased system based structure example taken existing knowledgebased system used demonstrate use designing system
supporting combined human machine planning user interface version realistic complex planning situations require mixedinitiative planning framework human automated planners interact construct desired plan joint cooperation potential achieving better plans either human machine create alone human planners often take casebased approach planning past experience planning retrieving adapting past planning cases planning analogical reasoning generative casebased planning combined provides suitable framework study mixedinitiative integration however human user planning loop creates variety new research questions challenges found creating mixedinitiative planning system fall three categories planning paradigms differ human machine planning visualization plan planning process complex necessary task human users range across spectrum experience respect planning domain underlying planning technology paper presents approach three problems designing interface incorporate human process planning analogical reasoning interface allows user follow generative casebased planning supports visualization plan planning rationale addresses variance experience user allowing user control presentation information research sponsored part knowledge based planning scheduling grant number short version document appeared supporting combined human machine planning interface planning analogical reasoning eds casebased reasoning research development second international conference casebased reasoning
simpler look consistency one major goals early concept learners find hypotheses consistent training data goal achieve high degree predictive accuracy set test data later research partially belief however issue consistency yet completely examine issue consistency new perspective avoid overfitting training data considerable number current systems goal learning hypotheses consistent training instances setting goal hypothesis simplicity occams razor instead using simplicity goal developed novel approach addresses consistency directly words concept learner explicit goal selecting appropriate degree consistency training data begin paper exploring concept learning less perfect consistency next describe system adapt degree consistency response feedback predictive accuracy test data finally present results initial experiments begin address question hypotheses fit training data different problems
neural network approach blind separation enhancement images contribution propose new solution problem blind separation sources one dimensional signals images case sources unknown also number purpose multilayer neural networks associated adaptive learning algorithms developed primary source signals distribution andor computer experiments presented demonstrate validity high performance proposed approach
nonlinear information algorithm performs blind separation new learning algorithm derived performs online stochastic gradient mutual information outputs inputs network absence priori knowledge signal noise components input propagation information depends network detailed higherorder input density functions mutual information outputs individual network input independent components example application achieved separation ten mixed speech signals simulations lead believe network performs better blind separation network fact derived mutual information objective
environments classifier systems experiments adding memory xcs technical report
efficient extension mixture techniques prediction decision trees present method maintaining mixtures prediction decision tree extends larger class method includes efficient online weight allocation algorithm used prediction compression classification although set given tree much larger algorithm similar space time complexity previous mixture algorithms trees using general online framework freund schapire prove algorithm maintains correctly mixture weights bounded loss function also give similar algorithm logarithmic loss function corresponding weight allocation algorithm finally describe experiments comparing mixture models estimating probability next word english text show models
simulation approach convergence rates markov chain monte carlo algorithms markov chain monte carlo mcmc methods including gibbs sampler metropolishastings algorithm commonly used bayesian statistics sampling complicated highdimensional posterior distributions source uncertainty long sampler must run order converge approximately target stationary distribution presents method compute rigorous theoretical upper bounds number iterations required achieve specified degree convergence total variation distance conditions propose use auxiliary simulations estimate numerical values needed theorem simulation method makes possible compute quantitative convergence bounds models analytical computations difficult impossible hand although method appears perform example problems provide guarantees analytical proof acknowledgements thank
uncertain inferences uncertain conclusions uncertainty may taken characterize inferences conclusions three uncertainty inference never characterized uncertainty explore uncertainty argument involves uncertainty argue uncertainty characterize inference natural uncertainty uncertainty procedure argument show possible principle incorporate uncertainty uncertainty arguments valid argue human argument computationally costly gain simplicity obtained allowing uncertainty inference sometimes loss keywords uncertainty inference logic argument decision
heuristic tree search applied grammar induction field operation research artificial intelligence several stochastic search algorithms designed based theory global random search techniques iteratively sample search space respect probability distribution updated according result previous samples predefined strategy genetic algorithms gas greedy randomized adaptive search procedures two particular instances paradigm paper present search algorithm based fundamental mechanisms techniques however addresses class problems difficult design transformation operators perform local search intrinsic constraints definition problem problems procedural approach natural way construct solutions resulting state space represented tree aim paper describe underlying heuristics used address problems class performance analyzed problem grammar induction successful application problems recent dfa learning competition presented
analysis gibbs sampler model related estimators summary analyze hierarchical bayes model related usual empirical bayes formulation estimators consider running gibbs sampler model using previous results convergence rates markov chains provide rigorous numerical reasonable bounds running time gibbs sampler suitable range prior distributions apply results data different range prior distributions prove gibbs sampler will fail converge use information prove case associated posterior distribution acknowledgements grateful suggesting project suggesting use convergence theorem thank helpful thank useful comments
representation issues neighborhood search evolutionary algorithms evolutionary algorithms often presented general purpose search methods yet also know search method better another possible problems fact often good deal problem specific information involved choice problem representation search operators paper explore general properties representations relate neighborhood search methods particular expected number local optima neighborhood search operator overall possible representations number local optima neighborhood search operator standard binary standard binary codes developed explored one measure problem complexity also relate number local optima another metric designed provide one measure complexity respect simple genetic algorithm choosing good representation component solving search problem however choosing good representation problem difficult choosing good search algorithm problem free lunch theorem search algorithm better possible discrete functions extend notions also cover idea representations equivalent behavior considered average possible functions understand results first outline simple assumptions behind theorem first assume optimization problem discrete describes combinatorial optimization optimization problems solved computers since computers finite precision second fact points space free lunch result follows
predicting exchange rates connectionist networks investigate effectiveness connectionist networks predicting future temporal sequences problem overfitting particularly serious short records noisy data addressed method term network complexity added usual cost function backpropagation goal prediction accuracy analyze two time series benchmark series networks outperform traditional statistical approaches show network performance input units needed also extract part dynamics noisy exchange rates makes network solution
analysis max problem genetic programming hold cases present detailed analysis evolution genetic programming populations using problem finding program returns maximum possible value given function set depth limit program tree known max problem confirm basic message crossover together program size restrictions responsible convergence suboptimal solution show happen even population high level variety show many cases evolution suboptimal solution solution possible sufficient time allowed cases theoretical models presented compared actual runs
least generalizations sets clauses main operations inductive logic programming ilp generalization specialization make sense generality order ilp three important generality orders subsumption implication implication relative background knowledge two languages used often languages clauses languages horn clauses gives total six different ordered languages paper give systematic treatment existence least generalizations finite sets clauses six ordered sets survey results already obtained others also contribute answers main new results firstly existence computable least generalization implication every finite set clauses containing least one clause among necessarily clauses secondly show least generalization need exist relative implication even set generalized background knowledge give complete discussion existence six ordered languages
machine learning group determination causal probabilistic network following paper approach problem different machine learning algorithms show compared causal probabilistic networks terms performance comprehensibility
bayesian time series models computations analysis time series physical sciences discusses developments bayesian time series modelling analysis relevant studies time series physical engineering sciences references discuss bayesian inference computation various statespace models examples series modelling various components error time series time series significant latent nonlinear time series models based mixtures problems errors uncertainties observations development nonlinear models based stochastic time scales
modelling robustness issues bayesian time series analysis areas recent development current interest time series discussion bayesian modelling motivated substantial practical problems areas include nonlinear autoregressive time series modelling measurement error structures statespace modelling time series issues uncertainties time discussion needs opportunities work models robustness issues given context
annealed competition experts segmentation classification switching dynamics present method unsupervised segmentation data different unknown sources alternate time use architecture consisting competing neural networks memory included order inputoutput relations order obtain maximal specialization competition increased training method achieves almost perfect identification segmentation case switching chaotic dynamics input inputoutput relations small dataset needed training applications time series complex systems demonstrate potential relevance approach time series analysis shortterm prediction
error measures suboptimal training neural network pattern classifiers pattern classifiers trained supervised fashion multilayer perceptrons radial basis functions etc typically trained error measure objective function error classifiers theory yield optimal bayesian discrimination practice often fail explain identify number characteristics optimal objective function training classifiers must show classification possess optimal characteristics whereas error measures illustrate arguments simple example trained polynomial neural network approximates bayesian discrimination random number training samples minimum functional complexity necessary task comparable net yields significantly worse discrimination task research air force office scientific research grant views conclusions contained paper authors interpreted representing policies either expressed implied air force government
survival time patients machine learning many factors might influence survival patients suggested aim study determine factors known time might predict survival patients aim also assess relative importance factors identify potentially useful decision regression trees generated machine learning algorithms study included patients mean years treated institute patients classified categories according attributes history physical findings extent disease paper compare machine learning approach previous statistical evaluations problem univariate multivariate analysis show provide analysis improve understanding data
machine learning worstcase analysis temporaldifference learning algorithms study behavior family learning algorithms based method temporal differences online learning framework learning takes place sequence trials goal learning algorithm estimate discounted sum will received future setting able prove general upper bounds performance slightly modified version socalled algorithm bounds terms performance best linear predictor given training sequence proved without making statistical assumptions kind process producing learners observed training sequence also prove lower bounds performance algorithm learning problem give similar analysis closely related problem learning predict model learner must produce predictions whole batch observations reinforcement
dynamic parameter encoding genetic algorithms common use static binary codes realvalued parameters genetic algorithm forces either representational precision efficiency search dynamic parameter encoding mechanism avoids using convergence statistics derived population adaptively control mapping binary genes real values shown empirically effective analysis explore problem convergence gas two convergence models
adapting crossover genetic algorithm traditionally genetic algorithms upon point crossover operators many recent empirical studies however shown benefits higher numbers crossover points recent work focused uniform crossover involves average crossover points strings length despite theoretical analysis however appears difficult predict particular crossover form will optimal given problem paper describes adaptive genetic algorithm runs form optimal
evolving edge genetic programming edge signals image images apply genetic programming techniques production high
gibbs sampling approach paper reviews application gibbs sampling system prices using two relations gibbs sampling techniques used estimate bayesian perspective relations weights system extensive use spectral analysis made get insight convergence issues
improving performance radial basis function networks learning center locations
decision analysis augmented probability simulation provide generic monte carlo method find alternative maximum expected utility decision analysis define artificial distribution product space alternatives states show optimal alternative mode implied marginal distribution alternatives sample artificial distribution may use exploratory data analysis tools approximately identify optimal alternative illustrate method important types influence diagrams decision analysis influence diagrams markov chain monte carlo simulation
stochastic search approach grammar induction paper describes new heuristic tree search named presents analysis performance problem grammar induction last work inspired dfa learning competition place november one two competition second algorithm first proposed price implements new heuristic state merging version heuristic also described paper compared
supporting conversational casebased reasoning integrated reasoning framework conversational casebased reasoning conversational casebased reasoning ccbr successfully used assist case retrieval tasks however behavioral limitations ccbr search reasoning approaches paper briefly describes groups ongoing towards behaviors conversational casebased reasoning development tool named particular focus integrating machine learning modelbased reasoning generative planning modules paper defines ccbr briefly summarizes explains enhance overall system research focuses performance conversational casebased reasoning ccbr systems ccbr form casebased reasoning users problem solving initial problem description natural language text text assumed partial rather complete problem description ccbr system description suggesting solutions primary purpose provide focus attention user quickly provide solutions problem figure summarizes ccbr problem solving cycle cases ccbr library three components
evolving cooperation strategies identification design implementation strategies cooperation central research issue field distributed artificial intelligence propose novel approach construction cooperation strategies group problem based genetic programming paradigm class adaptive algorithms used evolve solution structures optimize given evaluation criterion approach based designing representation cooperation strategies present results experiments domain extensively studied cooperation problem domain key aspect approach minimal domain knowledge human construction good cooperation strategies promising comparison results prior systems
simulation adaptive agents environment
evolving nontrivial behaviors real robots autonomous robot objects recently new approach involves form simulated evolution proposed building autonomous robots however still clear approach may adequate face real life problems paper show control systems perform nontrivial sequence behaviors obtained methodology carefully designing conditions evolutionary process operates experiment described paper mobile robot trained recognize target object controller robot evolved simulation tested real robot
model selection based minimum description length
introduction pair makes spectrum open dimension performs best although whether real preserve gene high dimension perform studying question dimension encoding best given instance likely optimal dimension dependent size input graph topology interactions flexibility crossover yet unknown interaction number used crossover also open issue genes onto multidimensional simplest way via sequential assignment order section showed performance improves used two phenomenon will consistent cases perform detailed future although proved helpful linear multidimensional believe good approach multidimensional cases since considering alternative dimensional dimensional will provide improvement hyperplane synthesis genetic algorithms international conference genetic algorithms pages analyzing hyperplane synthesis genetic algorithms using schemata international conference evolutionary computation notes computer science
reinforcement learning algorithm partially observable markov decision problems increasing attention reinforcement learning algorithms recent years due theoretical analysis behavior markov environments markov assumption removed however neither generally algorithms analyses propose analyze new learning algorithm solve certain class decision problems algorithm applies problems environment markov learner restricted access state information algorithm involves policy evaluation combined policy improvement method similar markov decision problems guaranteed converge local maximum algorithm operates space stochastic policies space yield policy performs considerably better deterministic policy although space stochastic policies discrete action algorithm computationally tractable
mcmc sampling hierarchical models summary markov chain monte carlo mcmc algorithms bayesian practice simplest form parameters updated one time however often slow converge applied highdimensional statistical models problem block parameters groups updated simultaneously using either gibbs metropolishastings step paper construct several partially fully mcmc algorithms minimizing mcmc samples arising important classes data models exploit used context bayes factor computation show parameters general linear mixed model may updated single block improving convergence producing essentially independent posterior parameters interest also investigate value mixed models class binary response data models illustrate approaches detail three examples
feature selection methods genetic algorithms search paper presents comparison two feature selection methods importance score based search genetic method order better understand strengths limitations area application results experiments show strong relation nature data behavior systems importance score method efficient dealing little noise small number interacting features genetic algorithms provide robust solution increased computational effort keywords feature selection machine learning genetic algorithms search
simple synchrony networks learning across syntactic paper describes training algorithm simple synchrony networks reports experiments language learning using recursive grammar new connectionist architecture combining technique learning patterns across time simple recurrent networks temporal synchrony variable binding use means learn training set information test set experiments network trained sentences one embedded clause words restricted certain classes constituent testing network information learned sentences three embedded clauses words constituent results demonstrate learn across syntactic
learning recursive sequences via evolution programs use directed search techniques space computer programs learn recursive sequences positive specifically integer sequences factorial numbers studied given small finite sequence show three directed genetic programming crossover exhaustive iterative hill climbing hybrid crossover hill automatically discover programs exactly finite target moreover correctly produce remaining sequence underlying machines precision representation contains instructions arithmetic register manipulation comparison control flow also introduce output instruction allows sequences result values representation contain recursive operators needed automatically synthesized primitive instructions fixed set search parameters instruction set program size fitness criteria compare three directed search techniques four sequence problems parameter set search always outperforms exhaustive hill climbing random search since target sequence variable experiments approach sequence induction potentially quite general
discretetime analytic systems paper studies problem desired implication analytic systems several cases compact state space stability condition iii generic sense addition paper studies properties control sets recently introduced context dynamical systems studies finally various examples provided relating various lie introduced past work
bayesian networks causal networks paper demonstrates use graphs mathematical tool formal language processing causal information decision analysis show complex information external organized represented graphical representation used facilitate quantitative predictions effects first review theory bayesian networks show directed acyclic graphs offer scheme representing conditional independence assumptions logical consequences assumptions introduce account causation show defines simple transformation probability distribution will change result external system using transformation possible quantify data effects external specify conditions randomized experiments necessary example show effect cancer data using minimal set qualitative assumptions finally paper offers graphical interpretation model causal effects demonstrates equivalence account causation tradeoffs two approaches deriving nonparametric bounds treatment effects conditions imperfect portions paper presented session international statistical institute august
computational learning algorithm interesting classical result due allows polynomialtime learning function class dnf using membership queries since practical learning situations access membership oracle paper explores possibility computation might allow learning algorithm dnf relies example queries natural extension learning domain presented algorithm requires example oracle runs time result appears impossible algorithm unique among algorithms assume priori knowledge function operate includes possible basis states
algebraic techniques efficient inference bayesian networks number exact algorithms developed perform probabilistic inference bayesian belief networks recent years algorithms use techniques analyze exploit network topology paper examine problem efficient probabilistic inference belief network combinatorial optimization problem finding optimal factoring given algebraic expression set probability distributions define combinatorial optimization problem optimal factoring problem discuss application problem belief networks show optimal factoring provides insight key elements efficient probabilistic inference present simple easily implemented algorithms excellent performance also show use algebraic perspective permits significant extension belief net representation
modeling superscalar processors via statistical simulation
global self organization known protein sequences reveals inherent biological self organization method global classification currently known protein sequences performed every protein sequence segments distance calculated pair segments space segments first embedded euclidean space small metric distortion novel clustering algorithm applied embedded space euclidean resulting hierarchical tree clusters offers new representation protein sequences families compares favorably updated classifications based functional structural protein data domains hand others automatically correctly identified novel representation protein families introduced functional biological protein families demonstrated family
interface interactive design environments explanation important issue building interactive design environments human knowledge system may solve design problem consider two related problems explaining systems reasoning design generated system particular analyze content explanations design reasoning design solutions domain physical devices describe two complementary languages models explaining design reasoning models explaining device designs interactive kritik computer program uses representations illustrate systems reasoning result design explanation design reasoning interactive kritik context evolving design solution similarly explanation design solution context design reasoning
object oriented design neural network simulator implementation connection machine paper describe implementation backpropagation algorithm means object oriented library use library user details specific parallel programming paradigm time allows greater generated code provide existing solutions survey relevant implementations algorithm proposed far literature general purpose computers extensive experimental results show use library performance simulator contrary implementation connection machine comparable category
neural network overview typical systems utilize forms energy management sophisticated technology common use today automatic potential remains improving efficiency electric gas however physics energy design environmental control strategies neither energy management experts behavior patterns adaptive control seems alternative building adaptive control system infer appropriate rules operation systems based energy goals recent research demonstrated potential neural networks intelligent control constructing prototype control system actual using neural network reinforcement learning prediction techniques sensors provide information environmental conditions level sound motion control gas electric space gas paper presents overview project now
learning controllers examples motivation searching alternative empirical techniques generating controllers today great interest discovering methods allow faster design development realtime control software control theory helps linear controllers developed support generation paper discussed machine learning applied function locally receptive field function approximators three integrated learning algorithms two original described two experimental test cases first test case provided industrial robot task second classical prediction task chaotic series experimental comparison appears fuzzy controllers examples excellent approximators even accurate nonlinear controllers many cases motion control
soft computing convergence reasoning term soft computing represents combination problemsolving fuzzy logic probabilistic reasoning neural networks genetic algorithms gas provide complementary reasoning searching methods solve complex realworld problems brief description will analyze useful combinations use control gas parameters application gas evolve topologies weights tune controllers implementation controllers algorithms
case study dynamic belief networks monitoring fall prediction detection
simultaneous learning control laws local environment representations intelligent navigation robots two issues intelligent navigation robot addressed work first robots ability learn representation local environment use representation identify local environment done first extracting features sensors just various directions using features reduced representation local environment derived robot learns new environment types purpose identification matching criteria proposed robot tries match sensory input one library second issue addressed learning hill climbing control laws local environments unlike conventional reinforcement learning framework robot first learns model environment learns control law terms neural network proposed reinforcement function generated sensory inputs robot control action taken three key results shown work robot able build library even significant sensor noise different local able identify local environment accuracy library build robot able learn adequate hill climbing control laws take state local environment five different environment types
belief maintenance bayesian networks
parallel environments implementing neural networks artificial neural networks anns gain variety application domains critical models run fast generate results real time although number implementations neural networks available sequential machines implementations require amount time train run anns especially ann models large one approach implementation anns implement parallel machines paper surveys area parallel environments implementations anns desired characteristics look implementations
extension fills exact sampling algorithm chains provide extension fills exact sampler algorithm algorithm similar fills however makes assumptions regarding stochastic state space existence densities etc illustrate algorithm simple example
improved uniform test error bounds derive uniform test error bounds improve bounds validation show use knowledge test inputs improve bounds bounds require computation introduce method trade speed computation also compute bounds several test cases
brief history connectionist research established within scientific community especially within field cognitive science diversity however created environment makes difficult connectionist researchers recent advances field let alone understand field developed paper attempts address problem providing brief guide connectionist research paper begins defining basic next development connectionist research moving early psychological influences followed mathematical computing contributions connectionist research current research reviewed focusing specifically different types network architectures learning rules use paper concludes suggesting neural network least cognitive move towards models incorporate relevant functional principles inherent systems
treatment models proposes probabilistic measurement called ordinal probabilistic models specific assumptions two interesting ordinal assumptions special cases model include standard factor analysis models held constant model binary item responses closely related item response models see also generally latent variable models considered detail van purpose note provide connections current research foundations nonparametric latent variable item response modeling missing paper point important related work also discuss three major theorems paper three tasks provide researchers interested foundations measurement item response modeling opportunity give approach attention
characterization monotone latent variable models
computational models integration computational maps motor control integration system viewed attempting estimate state state environment integrating multiple sources information describe computational framework notion specific models integration adaptation result psychophysical results two systems integration adaptation maps estimation state hand arm movements presented analyzed within framework results suggest spatial information visual auditory systems integrated reduce variance localization effects relation visual auditory space predicted simple learning rule temporal propagation errors estimating state captured linear dynamic providing evidence existence internal model dynamic behavior arm
incremental evolution complex general behavior several researchers demonstrated complex action sequences learned neuroevolution evolving neural networks genetic algorithms however complex general behavior avoiding specific environments turns difficult evolve often system discovers strategies moving back help agent cope effective appear generalize new environments problem general strategy difficult evolution system discover directly paper proposes approach complex general behavior learned incrementally starting simpler behavior gradually making task challenging general task transitions implemented successive stages evolving modifications allows even populations adapt new task method tested stochastic dynamic task capture compared direct evolution incremental approach evolves effective general behavior also scale tasks
evolving neural networks play difficult game computers best programs still average human player since traditional game playing techniques proven inadequate new approaches computer need studied paper presents new approach learning play adaptive neuroevolution method used evolve networks capable playing small knowledge board networks able simple computer opponent evolved within generations significantly networks exhibited several aspects general playing suggests approach scale
testing robustness genetic algorithm floating building block representation recent studies floating building block representation genetic algorithm suggest many advantages using floating representation paper investigates behavior floating representation problems response three different types reduction amount genetic material available problem solving process functions building blocks noncoding segments results indicate gas performance floating representation problems robust significant genetic material length may made relatively small decrease performance effectively solve problems negative building blocks noncoding segments appears improve rather performance
survey theory methods invariant item ordering work university support university development grant social sciences university additional support provided office naval research cognitive sciences division grant national institute mental health training grant
will genetic algorithm outperform hill climbing analyze simple hillclimbing algorithm previously shown outperform genetic algorithm simple road function analyze genetic algorithm significantly faster gives lower bound speed identify features give rise speedup discuss features incorporated real
new parallel variable distribution algorithms consider recently proposed parallel variable distribution algorithm solving optimization problems variables distributed among processors processor primary updating block variables allowing remaining secondary variables change restricted fashion along easily computable directions propose useful generalizations consist general case exact global solution certain natural sufficient descent condition convex case solution algorithm modifications key features algorithm analyzed proposed modified algorithms practical make easier achieve good load balancing among parallel processors present general framework analysis class algorithms derive new improved linear convergence results problems weak minima order strongly convex problems also show synchronization schemes improves flexibility approach
canonical indicators financial markets eeg paradigm statistical mechanics financial markets fit multivariate financial markets using adaptive simulated annealing global optimization algorithm perform maximum likelihood defined path multivariate conditional probabilities canonical thereby derived used technical indicators recursive optimization process tune trading rules trading rules used outofsample data demonstrate model illustrate markets likely efficient methodology extended systems approach complex systems utility intuitive powerful formalism generate indicators used rulebased models management
networks spiking neurons third generation neural network models computational power formal models networks spiking neurons compared neural network models based neurons threshold respectively sigmoidal particular shown networks spiking neurons computationally powerful neural network models concrete biologically relevant function exhibited computed single spiking neuron biologically reasonable values parameters requires hidden units sigmoidal neural net article assume prior knowledge spiking neurons contains extensive list references currently available literature computations networks spiking neurons relevant results biology
genetic algorithms fast simulated comparison compare genetic algorithms functional search method fast simulated efficient search strategy also statistically guaranteed find function optima previously demonstrated competitive standard simulated annealing techniques presenting suite six standard test functions codes previous studies without additional tuning strongly suggests expected orders magnitude efficient
extending theory refinement mofn rules recent years machine learning research addressing problem known theory refinement goal theory refinement learner modify incomplete incorrect rule base representing domain theory make consistent set input training examples paper presents major revision either propositional theory refinement system two issues discussed first show run time efficiency greatly improved changing exhaustive scheme computing iterative greedy method second show extend either refine mofn rules resulting algorithm neither new either order magnitude faster produces significantly accurate results theories fit mofn demonstrate advantages neither present experimental results two realworld domains
representing belief competitive equilibrium market consider problem belief aggregation given group individual agents probabilistic beliefs set uncertain events formulate consensus probability distribution events researchers proposed many aggregation methods although question best general consensus consensus develop approach problem agents uncertain events outcomes agent acts market maximize expected utility given prices limited activity risk equilibrium prices market represent beliefs agents constant risk demonstrate probability exhibits several desirable properties related independently motivated techniques argue approach provides plausible mechanism belief aggregation multiagent systems directly addresses agent provide decisiontheoretic foundation expert weights often employed techniques
minimization produces wellknown feature neural computation accepted minimization exhibits various intuitive theoretical advantages many methods unsupervised reduction far however applications paper apply static real world images find without teacher without significant preprocessing system automatically learns generate distributed representations based wellknown feature orientation sensitive edge structures thus extracting simple features related considered useful image preprocessing compression
reinforcement learning policies learners components called policy algorithm modifies policy learning algorithm learning algorithm components represented part policy policy modify way modify etc interest situations initial learning algorithm improved experience call learning learn force stochastic better better algorithm ssa addresses question lifelong reinforcement learning context learners lifetime ssa called times computed according ssa uses empirically observed lifelong reward measured current ssa call evaluates longterm effects setting stage later ssa represent lifelong success history next ssa call build basis additional solely learners solve complex task partially observable environment whose state space far reported literature
machine learning inductive bias algorithm adaptive search incremental study task sequences allow learners average reward appropriate shifts inductive bias changes learners policy evaluate longterm effects bias shifts setting stage later bias shifts use algorithm ssa ssa called times may depend policy uses bias shifts empirically observed longterm reward measured current ssa call bias shifts ssa represent lifelong success history next ssa call considered useful build basis additional bias shifts ssa allows wide variety learning algorithms novel adaptive extension search method learners policy modification strategy within policy incremental inductive transfer case studies involve complex partially observable environments traditional reinforcement learning fails
learning relations small datasets inductive logic programming system created demonstrate advantage induction logical implication rather subsumption procedures allow induce recursive relations using minimum number examples whereas inductive logic programming algorithms based subsumption require many examples solve induction tasks however input examples must carefully chosen must along inverse resolution path hypothesize extension efficiently induce recursive relations without requirement introduce generalization named capability empirically evaluate ability induce recursive relations
temporal difference learning submitted popular family algorithms approximate policy evaluation large mdps works incrementally updating value function observed transition two major drawbacks makes inefficient use data requires user tune schedule good performance case linear value function approximations algorithm parameters improves data efficiency paper extends work three significant ways first presents simpler derivation algorithm second generalizes arbitrary values resulting algorithm shown practical formulation supervised linear regression third presents novel intuitive interpretation modelbased reinforcement learning technique
importance sampling technical report department statistics university toronto abstract simulated annealing moving tractable distribution distribution interest via sequence intermediate distributions traditionally used method handling isolated modes markov chain samplers shown one use markov chain transitions annealing sequence define importance sampler markov chain aspect allows method perform even highdimensional problems finding good importance sampling distributions otherwise difficult use importance weights estimates found converge correct values number annealing runs increases annealed importance sampling procedure second transitions seen generalization variant sequential importance sampling also related integration methods estimating constants annealed importance sampling attractive isolated modes present estimates constants required may also generally useful since independent sampling allows one problems convergence markov chain samplers
genetic programming genetic programming optimization method john koza koza variant genetic algorithms search space problem domain consists computer programs represented parse trees crossover operator realized exchange empirical analyses show large parts trees never used evaluated means parts trees irrelevant solution redundant paper concerned identification mathematical description behavior conclusions drawn description among others explain size problem phenomenon average size trees population grows time
discussion design principles efficient crossover operators graph coloring problems new graph coloring problems introduced shown computer experiments clear benefits approach graph coloring many applications areas scheduling assignments classified algorithm since based population search periods local optimization phases new configurations created earlier configurations local minima previous iterative improvement process new population created using crossover operators genetic algorithms paper discuss methodology inspired competitive analysis may relevant problem designing better crossover operators area local crossover competitive analysis crossover
decision trees equivalence propositional operations wellknown concept decision trees used inductive inference study natural concept equivalence two decision trees equivalent represent hypothesis present simple efficient algorithm establish whether two decision trees equivalent complexity algorithm bounded product sizes decision trees hypothesis represented decision tree essentially boolean function just like although every boolean function represented way show conjunctions decision trees efficiently represented decision trees simply may require exponential size representation trees
integrated approach study object features visual recognition propose assess relevance theories synaptic modification models feature extraction human vision using derived synaptic weight patterns parts stimulus images psychophysical experiments experiment reported found derived principal component analysis object images effective reducing generalization performance human subjects derived another method feature extraction bcm based higherorder statistics images
evolution chaos embedded noise analog two dimensional oscillator model exhibits chaos parameters model noise typical context presented real using algorithm particularly handling nonlinear systems approach shows promise investigate whether chaos predicted models noisy contexts
pruning generalization based weight purpose architecture optimization schemes improve generalization presentation suggest estimate weight associated change generalization error weight detail implementation storage scheme extending scheme extending illustrate approach chaotic time series
using genetic programming evolve board evaluation functions paper employ genetic programming paradigm enable computer learn play strategies evolving board evaluation functions problem terms board evaluation functions made feasible evaluate fitness game playing strategies using fitness evaluation game elements strategy approach learns strategies enable computer play consistently reasonably level
memorybased active learning optimizing noisy continuous functions learning field beyond prediction paper introduces new algorithm optimizing expected output noisy continuous function designed need experiments avoids strong assumptions form function autonomous requires little four existing approaches problem response surface methods numerical optimization supervised learning evolutionary methods requirement box behavior combined need experiments uses instancebased determination convex region interest performing experiments conventional instancebased approaches learning neighborhood defined query point contrast defines neighborhood new geometric procedure captures size shape possible optimum locations also optimizes weighted combinations outputs finds inputs produce target outputs compare noisy functions several problems including simulated noisy process nonlinear continuous dynamics components results encouraging terms speed
using classifier constructive induction paper propose approach constructive induction idea improvement classification accuracy based iterative modification input data space process independently repeated pair classes finally gives input data subspaces attributes optimal discrimination appropriate pairs classes use genetic algorithms constructive induction engine final classification obtained weighted majority voting rule according classifier approach computational experiment performed medical data set obtained results point advantage using model classifier constructive induction relation analogous approach
statistical mechanics combat human factors highly project extends previous work combat modeling descriptions decisionmaking human factors complex activities previous paper established first theory statistical mechanics combat developed using modern methods statistical mechanics empirical data national training center previous project also established computer providing statistical capability scenarios mathematical formulation extension include human factors methodology previously developed context vehicles similar scenarios crucial decision points will used data model decision making combat results may used improve present human factors algorithms computer approach nonlinear stochastic equations scenarios establish order description combat practice equivalent representation used suitable numerical formal work representation theoretically equations nested within larger set nonlinear stochastic include human factors decisions study propose operator theory order set equations subsets scenarios fit order originally considered similarly split distinguish decisionmaking new methods fast simulated developed previous project will used fitting models empirical data
applications statistical mechanics financial markets
application statistical mechanics models modelling application
evaluating improving steady state evolutionary algorithms constraint satisfaction problems
improving performance evolutionary optimization dynamically scaling evaluation function traditional evolutionary optimization algorithms assume static evaluation function according solutions evolved incremental evolution approach dynamic evaluation function time order improve performance evolutionary optimization paper present empirical results demonstrate effectiveness approach genetic programming using two domains game task demonstrate incremental evolution successful applied near beginning evolutionary run also show incremental evolution successful intermediate evaluation functions difficult target evaluation function easier target function
toward unified theory spatiotemporal processing
effectiveness evolutionary search highdimensional offer ability assess performance evolutionary algorithms problems different degrees paper study performance six algorithms low high dimension keeping amount interactions constant results show compared genetic local search algorithms performance standard genetic algorithms employing crossover mutation significantly decreases increasing problem size furthermore increasing crossover based algorithms cases outperformed mutation based algorithms however relative performance differences algorithms grow significantly dimension search space indicating important consider highdimensional landscapes evaluating performance evolutionary algorithms
rational belief revision preliminary report theories rational belief revision recently proposed many important issues strong correct revisions make strong assumptions information available guide revisions theories according standard preferences used select among alternative possible revisions multiple partial specifications preferences ways closely related theory employs information available practice offers flexible ways selecting revisions formally compare notion rational belief revision adapt results universal default theories prove universal method rational belief revision examine formally different limitations affect belief revision
family algorithms independent component analysis independent component analysis ica statistical signal processing technique whose main applications blind source separation blind feature extraction estimation ica usually performed optimizing contrast function based higherorder paper shown almost error function used construct contrast function perform ica estimation particular means one use contrast functions robust practical method relevant contrast functions iteration scheme introduced resulting algorithms quite simple converge fast reliably algorithms also enable estimation independent components using simple scheme
toward market model bayesian inference present methodology representing probabilistic relationships model specifically define precise mapping bayesian network binary nodes market price system trade uncertain demonstrate correspondence equilibrium prices probabilities represented bayesian network computational market model may provide useful framework belief aggregation distributed probabilistic inference resource allocation uncertainty problems uncertainty
change point change curve modeling stochastic processes spatial statistics article will appear journal applied statistical science adrian raftery professor statistics department statistics university washington seattle research supported onr contract grant raftery latter two article written article presentation conference applied change point analysis university march parts article review research others like express namely
management systems paper describe functional architecture software devoted development new generation intelligent environmental decision support systems based approach system design exploitation new architecture problem solving integrates casebased reasoning constraint reasoning developed objectoriented environment upon will developed first
planning complex real domain dimensions complexity raised definition system aimed supporting planning initial presented discussed complexity deriving highly dynamic domain one integration planning techniques suitable domain complexity addressing problem taking account role user supported system finally complexity architecture able integrate different particular focus constraints definition planning approach domain constraints satisfied completely current planning paradigms propose approach based planning case based reasoning techniques constraint reasoning specifically temporal constraints used two steps planning process plan fitting adaptation resource scheduling work development system software architecture methodology progress
mixing floating learning examine efficient implementation back type algorithms vector processor fixed point engine designed neural network simulation matrix formulation back matrix back shown efficient using matrix back achieve asymptotically optimal performance forward phases possible standard online method since high efficiency convergence poor due use fixed point arithmetic use mixture fixed floating point operations key observation precision fixed point sufficient good convergence range appropriately chosen though expensive computations implemented fixed point achieve rate convergence comparable floating point version time taken fixed floating point also shown reasonable
preliminary investigation evolution form design strategy describe preliminary version software generative genetic genetic operations interact generate novel forms allows evolutionary algorithms architecture tasks
perform part classifier set tree viewed generalization decision trees empirically characterize domains particularly relative decision trees specifically show
rule generation paper discuss approach learning classification rules data two modules architecture namely knowledge acquisition tool domains automatically generating classes examples incrementally works unsupervised strategy output representation conceptual structure domain terms classes input used generate set classification rules original training set generate conjunctive disjunctive rules present application techniques data obtained real treatment order help construction rule base rule will used knowledgebased system whole process
computation integration
grammar emergent soft constraints reading sentence interpretation changes event back online automatically appears based dynamically combining strengths association keywords two neural networks good modeling behavior learn word soft constraints interpretation dynamically combine constraints form likely interpretation hand difficult show systematic language structures relative clauses system network learn associate specific contexts able process new combinations look understanding embedded clauses shows humans systematic processing grammatical structures either example next difficult understand whereas car bit difference semantic constraints work task chapter will show combined highlevel control allows system process novel combinations relative clauses systematically still sensitive semantic constraints
generalization performance backpropagation learning task investigated generalization capabilities backpropagation learning feedforward recurrent feedforward connectionist networks assignment boundaries representations difficult task constraints interact leading input patterns compared results different symbolic pattern matching approaches generalization scheme related knearest approach using similarity metric relative information entropy positions training patterns results indicate generalization performance backpropagation learning task better best symbolic pattern matching approaches generalization
pruning strategies constructive learning algorithm present framework incorporating pruning strategies constructive neural network learning algorithm pruning involves elimination redundant elements connection weights neurons network considerable practical interest describe three sensitivity based strategies pruning neurons experimental results demonstrate significant reduction network size without networks generalization performance
independent component analysis general nonlinear learning rules number neural learning rules recently proposed independent component analysis ica rules usually derived informationtheoretic criteria maximum entropy minimum mutual information paper show fact ica performed simple hebbian learning rules may weak relations quantities rather nonlinear function used learning rule provided term chosen correctly addition mechanism weight vector constrained unit norm data results imply one choose optimize desired statistical numerical criteria
submitted circuits systems signal processing neural network constructive algorithms trading generalization learning efficiency currently several types constructive algorithms available training feedforward neural network paper describes explains main ones using fundamental approach multilayer perceptron problemsolving mechanisms convergence properties algorithms verified using just two mapping theorems consequently enables algorithms unified basic mechanism algorithms compared fundamental reasons actual success algorithms extracted used suggest might applied current neural network difficulties one must along line learning efficiency promise developed argument generalization abilities will lie average backpropagation
generalized modelbased reinforcement learning method attempts focus agents limited computational resources achieve good estimate value environment states choose effectively costly planning step uses simple heuristic focus computation states likely largest errors paper introduce generalized principled method generating estimates manner allows extend beyond explicit representation deal compact representations necessary dealing large state spaces apply method generalized model approximators bayesian networks describe preliminary experiments compare approach classical
selection distance metrics feature subsets knearest neighbor classifiers
constructive neural network learning algorithms pattern classification
difficulties learning logic programs cut real logic normally use cut effective learning procedure logic programs able deal cut predicate procedural meaning clauses containing cut learned using evaluation method done learning systems hand searching space possible programs instead space independent clauses alternative solution generate first candidate base program covers positive examples make consistent cut appropriate problem learning programs cut investigated seems natural reasonable approach generalize scheme investigate difficulties arise major shortcomings actually caused general need evaluation analysis paper suggests precise technical learning cut difficult current induction techniques probably restricted purely declarative logic languages
gamma mlp speech recognition define gamma multilayer perceptron mlp mlp usual synaptic weights gamma filters proposed associated gain terms layers derive gradient descent update equations apply model recognition speech find gamma filters layers synaptic gains improves performance gamma mlp compare gamma mlp mlp iir mlp architectures local approximation scheme find gamma mlp results substantial reduction error rates
learning rules independent component analysis neural learning rules problem independent component analysis ica blind source separation introduced new algorithms every ica neuron develops one independent components learning rules use simple constrained learning feedback may added speed convergence stochastic gradient descent rules novel algorithm introduced
book review new block way field connectionist modeling connectionist models collection papers representing wide variety research topics book distinguished single feature papers almost contributions graduate students active field students selected rigorous review process two long school devoted state true presented opportunity sample words imply two ways approach book book must just random collection scientific papers also challenge evaluate field school actually third series previous ones held proceedings school reviewed pattern school held
complexity theory revision knowledgebased system uses database theory produce answers queries receives unfortunately answers may incorrect underlying theory standard theory revision systems use given set labeled queries query correct answer transform given theory adding andor either rules andor related theory accurate possible formally defining theory revision task paper provides sample computational complexity bounds process first specifies number labeled queries necessary identify revised theory whose error close minimal high probability considers computational complexity finding best theory polynomial time algorithm identify nearoptimal revision even given exact distribution queries except trivial situations also shows except trivial situations polynomialtime algorithm produce theory whose error even close within particular polynomial factor optimal results suggest reasons theory revision effective learning scratch also many aspects standard theory revision systems including practice hillclimbing theory based given set labeled queries paper extends short article appeared proceedings international joint conference artificial intelligence august helpful comments especially comments
constructing conjunctions using systematic search decision trees paper investigates dynamic method constructing conjunctions new attributes decision tree learning searches conditions attributevalue pairs paths form new attributes compared new attribute construction methods new idea method systematic search pruning path tree select conditions generating conjunction therefore conditions constructing new attributes dynamically search empirically evaluation set artificial realworld domains shows dynamic method improve performance selective decision tree learning terms higher prediction accuracy lower theory complexity addition shows performance advantages fixed method fixed rulebased method learning decision trees
outperform many long time algorithms numerous recent papers focus standard recurrent nets problems long time relevant signals propose rather sophisticated alternative methods show many problems used test previous methods solved quickly random weight
computational view population genetics preliminary version paper study nonlinear dynamical systems computational perspective systems inherently powerful linear markov chains wide impact computer science seem likely play increasing role future however yet general techniques available handling computational aspects discrete nonlinear systems even simplest examples seem hard analyze focus paper class quadratic systems widely used model population genetics also genetic algorithms systems describe process random occur via mechanism known crossover genetic material different parents according random rule results two fundamental quantitative properties crossover systems develop general technique computing
efficient algorithms inverting evolution evolution stochastic process operates dna species evolutionary process leaves dna used construct evolutionary trees set species maximum likelihood methods evolutionary tree likely produced dna methods widely accepted computationally intractable paper address methods follows introduce metric evolutionary stochastic process show metric giving learnability true phylogeny terms metric measure result simple efficient algorithm inverting stochastic process evolution building tree observations dna species put another way show though many heuristics suggested problem algorithm first algorithm guaranteed convergence rate rate within polynomial rate establish algorithm also first polynomial time algorithm guaranteed converge correct tree
learning continuous domains delayed rewards much done develop learning techniques delayed reward problems worlds actions andor states approximated discrete representations although acceptable applications many situations approximation difficult instance applications robotics real machines interact real world learning techniques use real valued continuous quantities required presented paper extension qlearning uses real valued states actions achieved introducing activation strengths system robot allow active continuous amount simultaneously learning occurs incrementally adapting expected future reward goal evaluation function function respect system
connection stochastic smoothing filtering estimation incomplete data connections stochastic estimation incomplete data investigated shown right scheme estimator characterized estimate based stochastic motivated result potentially useful approach estimation convergence incomplete data proposed estimators characterized sometimes reduce filters described system stochastic equations recent results convergence stochastic stochastic differential equations applied address convergence issues problem framework closed form estimator proposed convergence properties studied theory plays role entire analysis approach method
inductive cbr support past years paradigm rapidly hardware particular traditional issues service characteristics network control modern issues network service management area service management extremely high negative impact problem handling problem handling knowledge activity particularly increase number complexity available trials several support already demonstrated potential casebased reasoning technology improving current practice problem detection diagnosis major cost involved implementing casebased system building initial case base subsequent maintenance case base time paper shows inductive machine learning combined casebased reasoning produce intelligent system capable extracting knowledge raw data automatically reasoning knowledge addition discovering knowledge existing data integrated system may used acquire revise knowledge continually experiments suggested integrated approach demonstrate promise next step
training subset selection methods supervised learning genetic programming using genetic programming algorithm difficult problem large set training cases large population size needed large number evaluations must carried paper describes reduce number evaluations selecting small subset training data set actually algorithm three subset selection methods described paper dynamic subset selection using current run select difficult andor cases historical subset selection using previous runs random subset selection compared large classification problem produce better results less time taken nearly match results perhaps surprisingly approach results compared smaller problem hybrid dynamic fitness function based proposed
boolean even parity problem genetic programming fitness standard paper presents limited error fitness modification standard supervised learning approach genetic programming individuals fitness score based many cases ordered training set individual error limit training set order error limit dynamically response performance individual previous generation
pruning decision trees misclassification costs describe experimental study pruning methods decision tree classifiers goal minimizing loss rather error addition two common methods error minimization pruning pruning study extension pruning loss one pruning variant based correction perform empirical comparison methods evaluate respect loss found applying correction estimate probability distributions leaves pruning methods unlike error minimization somewhat surprisingly performing pruning led results methods terms evaluation criteria main advantage pruning reduction decision tree size sometimes factor ten method others datasets even domain different pruning mechanisms better different loss matrices
genetic algorithms selection effects noise report
implementation issues fourier transform algorithm fourier transform boolean functions play important role proving many important learnability results aim demonstrate fourier transform techniques also useful practical algorithm addition powerful theoretical tool describe changes introduced algorithm ones crucial without performance algorithm one benefits present confidence level prediction measures likelihood prediction correct
small populations many generations large populations generations genetic programming paper use small populations genetic programming literature appears towards using large population possible requires memory resources less efficient dynamic subset selection limited error fitness two different adaptive variations standard supervised learning method used paper compares performance case classification problem using small population size similar comparison done larger case classification problem problems small population size consistently produces better answer using fewer tree evaluations runs using much larger populations even standard seen perform much smaller population size indicating exploratory run three small population size assuming large population size necessary interesting notion smaller mean faster better
data mining knowledge discovery adaptive detection one method detecting check changes user behavior paper describes automatic design user methods purpose detection using series data mining techniques specifically use program indicators behavior large database transactions indicators used create set monitors profile behavior indicate finally outputs monitors used features system learns combine evidence generate system applied problem detecting cellular based database call records experiments indicate automatic approach performs better methods detecting furthermore approach adapt changing conditions typical detection environments
learning neural networks bayesian prototypes given set samples probability distribution set discrete random variables study problem constructing good neural network model underlying probability distribution approach based unsupervised learning scheme samples first separate clusters cluster coded single vector bayesian prototype vectors consist conditional probabilities representing attributevalue distribution inside corresponding cluster using prototype vectors possible model underlying joint probability distribution simple bayesian network tree realized feedforward neural network capable probabilistic reasoning framework learning means choosing size prototype set partitioning samples corresponding clusters constructing cluster prototypes describe prototypes determined given partition samples present method evaluating likelihood corresponding bayesian tree also present greedy heuristic searching space different partition schemes different numbers clusters optimal approximation probability distribution
context preserving crossover genetic programming paper introduces two new crossover operators genetic programming contrary regular crossover operators presented attempt preserve context appeared trees simple coordinate scheme nodes tree proposed allowed nodes exactly partially matching
analysis hierarchical genetic programming hierarchical genetic programming hgp approaches rely discovery modification use new functions accelerate evolution paper provides qualitative explanation improved behavior hgp based analysis evolution process dual perspective diversity causality static point view use hgp approach enables manipulation population higher diversity programs higher diversity increases exploratory ability genetic search process demonstrated theoretical experimental fitness distributions structural complexity individuals dynamic point view report analyzes causality crossover operator causality changes structure object effect changes changes properties behavior object analyses crossover causality suggests hgp discovers exploits useful structures bottomup hierarchical manner diversity causality complementary exploration exploitation genetic search unlike machine learning techniques need extra control tradeoff hgp automatically exploration exploitation
learning without partially observable markovian decision processes reinforcement learning algorithms provide sound theoretical basis building learning control architectures embedded agents unfortunately theory much practice see exception limited markovian decision processes mdps many realworld decision tasks however inherently state environment known learning agent paper consider partially observable mdps pomdps useful class decision processes previous approaches problems combined computationally expensive techniques learning control paper investigates learning pomdps without form state estimation present results td0 qlearning will applied pomdps shown conventional discounted framework inadequate deal pomdps finally develop new framework learning without pomdps including stochastic policies search space defining value utility states
fall diagnosis using dynamic belief networks task monitor patterns give early using sensors describe dynamic belief network model fall diagnosis given evidence sensor observations outputs beliefs current makes predictions regarding future model represents possible sensor error allow individual
explanation evaluation like thank valuable guidance research thank cognitive science helpful comments draft paper research described conducted primarily university supported part defense advanced research projects agency office naval research contract air force office scientific research contract
two methods hierarchy learning reinforcement environments paper describes two methods temporal behaviors first intuitive grouping together common sequences events single units may treated individual behaviors system problems however units binary meaning behaviors must execute completely construction good training algorithms system also runs difficulty one unit active time second system hierarchy transition values hierarchy dynamically modifies values specify degree one unit follow another values continuous allowing use gradient descent learning furthermore many units active time part systems normal
learning learn learning strategies paper introduces incremental paradigm unlike previous methods incremental reinforcement learning system improve way learns improve way improves way learns without significant theoretical limitations system able shift inductive bias universal way major features explicit difference learning kinds information processing using turing machine equivalent programming language system initially highly random programs modify probabilities future action sequences including future programs system probability modifications computed useful programs payoff reward reinforcement per time previous programs computation payoff per time takes account computation time required learning entire system life considered boundaries learning trials particular implementation based novel paradigm presented designed exploit conventional digital machines good fast storage addressing arithmetic operations etc experiments illustrate systems mode operation keywords reinforcement learning note revised extended version earlier report november
neural network architecture database query processing artificial neural networks ann due inherent parallelism potential fault offer attractive paradigm robust efficient implementations large modern database knowledge base systems paper explores neural network model efficient implementation database query system application proposed model library query system retrieval multiple items based partial match specified query criteria stored records performance ann realization database query module analyzed compared techniques commonly current computer systems results analysis suggest proposed ann design offers attractive approach realization query modules large database knowledge base systems especially retrieval based partial matches
neural network architecture analysis
power equivalence queries showed class combinatorial property called approximate identified exactly using many equivalence queries polynomial size show necessary condition every class without approximate identification strategy makes polynomial number equivalence queries furthermore class technical sense computational power required strategy within polynomialtime hierarchy proving non learnability least hard showing
instruction level parallel scheduling application blocks code scheduling exploit instruction level parallelism ilp critical problem compiler optimization research light increased use machines unfortunately optimum scheduling computationally intractable one must carefully heuristics practice scope application scheduling heuristic limited basic blocks considerable performance loss may block boundaries overcome basic blocks across branches form larger regions blocks literature regions typically using algorithms either profile information assumption process region fully utilized profile information use profile information classical scheduling techniques believe even simple case linear code regions blocks additional performance improvement gained utilizing profile information scheduling propose general paradigm list scheduler scheduler technique developed via theoretical analysis simplified abstract model general problem scheduling acyclic code region scoring measure ranking branch instructions ranking profile information useful property scheduling respect provably good minimizing expected time region within limits abstraction ranking scheme computationally intractable general case blocks suggests heuristic present paper scheduling blocks experiments show heuristic offers substantial performance improvement prior methods range integer several machine models
genetic programming propose extension genetic programming paradigm allows users traditional genetic algorithms evolve computer programs end introduce mechanisms like genetic programming demonstrate feasibility approach using develop programs prediction sequences integer numbers
faster learning multilayer networks handling generalized rule known backpropagation probably one widely used procedures training multilayer feedforward networks sigmoid units despite reports success number interesting problems slow set weights meet desired error criterion several modifications improving learning speed proposed literature known suffer phenomenon flat direct consequence together formulation learning rule paper proposes new approach minimizing error suggested mathematical properties conventional error function effectively occurring output layer robustness proposed technique demonstrated number datasets widely studied machine learning community
sequential methods bayesian filtering report present overview sequential methods bayesian filtering nonlinear dynamic models includes general framework numerous methods proposed independently various areas science proposes original developments
construction trees science fitting gene species fast comparison evolutionary trees technical report dimacs university
case retrieval nets basic ideas extensions efficient retrieval relatively small number relevant cases case base crucial casebased reasoning article present case retrieval nets memory model recently developed task main idea apply activation process case memory order retrieve cases similar query case summarize basic ideas suggest useful extensions present initial experimental results suggest successfully handle case bases larger considered usually cbr community
applying case retrieval nets diagnostic tasks technical domains paper presents case retrieval nets memory model developed application casebased reasoning task technical diagnosis key idea store cases observed network enhance network object model encoding knowledge devices application domain
priors gibbs sampling generalized linear models professor department statistics university school university research first author supported part nsf grant second author supported part grant authors thank valuable comments
monte carlo implementation gaussian process models bayesian regression classification technical report department statistics university toronto abstract gaussian processes natural way defining prior distributions functions one input variables simple nonparametric regression problem function gives mean gaussian distribution observed response gaussian process model easily implemented using matrix computations feasible datasets cases hyperparameters define covariance function gaussian process sampled using markov chain methods regression models noise distribution logistic models classification applications implemented sampling latent values underlying observations software now available implements methods using covariance functions hierarchical models defined way discover highlevel properties data inputs relevant predicting response
minimum rule sets rule induction systems generate rule sets optimal complexity rule set paper develops formal proof problem generating simplest rule set accurately predicts examples training set particular type generalization algorithm algorithm complexity measure proof extended cover spectrum complexity measures learning algorithms
simulation using reinforcement learning many optimization problems control scheduling reliability formulated markov decision processes primary goal problems find policy minimizes average cost paper describes new algorithm called finding policies continuous time decision processes paper presents detailed experimental study large production problem outperforms two wellknown reliability heuristics industrial engineering key feature study integration reinforcement learning algorithm directly two commercial simulation way approach applied many optimization problems already exist simulation models
efficient locally weighted polynomial regression predictions locally weighted polynomial regression popular instancebased algorithm learning continuous nonlinear mappings two three inputs computational predictions discuss drawbacks previous approaches dealing problem present new algorithm based multiresolution search augmented without tree make fast predictions arbitrary local weighting functions arbitrary kernel arbitrary queries paper begins new faster algorithm exact predictions next introduce approximation achieves speedup accuracy increasing certain approximation parameter achieves greater speedups still larger accuracy nevertheless useful operations early stages model selection optima surface also show approximations permit realtime optimization kernel width conclude brief discussion potential extensions tractable instancebased learning datasets large fit main memory
condition ordinal matrices keywords additive algorithm evolution ordinal phylogeny ordinal evolutionary context form species similar species species distance matrix given species ordinal binary character defined species paper present several results concerning inference evolutionary trees ordinal particular present condition distance matrices whose ordinal binary pairwise characterization analogous condition additive matrices optimal algorithm number species recovering phylogeny ordinal binary distance matrix satisfies condition result determining phylogeny ordinal binary given distance matrix
xofn attributes versus nominal xofn attributes constructive induction case study xofn set containing one attributevalue pairs given instance value corresponds number attributevalue pairs true paper explore characteristics performance xofn attributes versus nominal xofn attributes constructive induction nominal powerful former suffer problem although mechanisms help solve problem two approaches constructive induction using described perform better nominal ones domains need one cut point domains need xofn representations one cut point nominal perform better ones experimental results set artificial realworld domains support
comparison constructive induction different types new attribute paper studies effects decision tree learning constructing four types attribute conjunctive disjunctive mofn xofn representations reduce effects factors tree learning methods new attribute search strategies search starting points evaluation functions stopping criteria single tree learning algorithm developed different settings construct four different types new attribute factors fixed study reveals conjunctive disjunctive representations similar performance terms prediction accuracy theory complexity variety concepts even dnf concepts usually thought suited one two kinds representation addition study demonstrates stronger representation power mofn conjunction stronger representation power xofn three types new attribute performance decision tree learning terms higher prediction accuracy lower theory complexity
investigation algorithms retrieval analogy casebased reasoning systems scale large case bases important analyze various methods used retrieving identify features problem appropriate paper reports one analysis comparison retrieval activation semantic network activation method developed retrieving large knowledge base analysis two complementary components theoretical model retrieval time based number problem characteristics experiments showing retrieval time approaches varies knowledge base size two components taken together suggest likely able scale retrieval large knowledge bases
dna sequence classification using induction identification problems forms models depend absolute locations assume independence locations paper describes new class learning methods called induction towards sequence learning problems arise learning dna sequences central idea use text compression techniques dna sequences means generalizing sample sequences resulting methods form models based important relative locations dependence locations also provide suitable framework biological domain knowledge learning process present initial range methods demonstrate potential methods dna sequence identification tasks
model rapid memory formation hippocampal system ability events situations daily life demonstrates ability rapidly acquire new memories broad consensus hippocampal system plays critical role formation retrieval memories computational model described demonstrates may rapidly transform pattern activity representing event situation structural encoding via longterm longterm
comparison neural net conventional techniques control compare two techniques control actual detect level four sensing points independently set one levels task determine device levels achieve particular configuration sensor one technique explored uses neural network approximate mapping sensor device levels technique examined uses conventional feedback control loop neural network approach appears superior require experimentation hence light levels times deal complex interactions conventional control techniques handle comparison performed part adaptive project described briefly directions control
convergence norm expectationmaximization type algorithms provide sufficient condition convergence general class type estimation algorithms respect given norm class includes approximate algorithms convergence analysis extended include algorithms condition monotone convergence used establish distance successive limit point algorithm approaches zero apply results estimation rate parameters establish final iterations converge weighted euclidean norm
refining controllers using neural networks approach uses neural networks refine knowledge written form simple propositional rules extend idea presenting algorithm mathematical equations controller determine topology initial weights network trained using backpropagation apply method task controlling temperature producing gains accuracy standard neural network approach controller furthermore using knowledge weights network produces statistically less variation accuracy compared networks small random numbers
von type statistics single site updated local interaction random fields random field models image analysis spatial statistics usually local interactions simulated markov chains update single site time updating rules typically condition sites approximate expectation bounded function make better use simulations empirical estimator describe empirical estimator computationally feasible lead considerable variance reduction method idea behind generalized von statistics consider mainly nearest neighbor random fields gibbs sampler
object recognition using unsupervised feature extraction proposed feature extraction method related recent statistical theory based biologically motivated model neuronal plasticity method recently applied feature extraction context recognizing objects single views gold describe experiments designed analyze nature extracted features relevance theory object recognition
modeling dynamical evolutionary machine organization group hypothesis notion problem decomposition assembly solutions many test problems structure based many problems use fitness functions model bits within block however model consistent type interaction used paper discusses various test problems literature concept formulate principled model hierarchical applied many levels consistent manner introduce hierarchical canonical example present empirical results gas showing population diversity tight able identify many levels assembly hypothesis suggests
achieving computer performance array processor music system signal processor system intelligent communication parallel distributed memory architecture based digital signal processors system processor elements peak performance electrical power less including air two applications backpropagation algorithm neural net learning molecular dynamics simulations run times faster times faster performance price system range
dynamical behavior artificial neural networks random weights paper report monte carlo study dynamics large feedforward neural networks randomly chosen weights feedback analysis consists systems exhibit chaos largest lyapunov correlation dimensions systems become complex increasing inputs neurons probability chaos approaches correlation dimension typically much smaller system dimension
effect analog noise discretetime analog computations introduce model analog computation discrete time presence analog noise flexible enough cover important concrete cases noisy analog neural nets networks spiking neurons model classical model digital computation presence noise show presence arbitrarily small amounts analog noise reduces power analog computational models finite automata also prove new type upper bound
improved model spatially correlated binary responses paper extend basic model include sampling effort model applied sampled data instead traditional use image analysis complete data available bayesian develop hybrid gibbs sampling estimation procedure using simulated examples show model sample data improves predictions compared simple logistic regression model standard model without
learning high utility rules incorporating search control guidance committee
evolving deterministic finite automata using cellular encoding programming cellular encoding programs evolved paper presents method initial results evolution deterministic finite
connectionist simulator constructing simulating structured connectionist networks requires significant programming effort system tools greatly reduce effort required providing conceptual structure within work make large complex network simulations possible connectionist simulator system tool designed aid specification construction simulation connectionist networks report describes tool detail provided use details implementation make designing connectionist networks easier also development refinement connectionist research tools
relationship distributed behavioural complexity individuals
integrity constraints ilp using monte carlo approach many ilp systems require large numbers negative examples avoid considerable many ilp applications namely program synthesis small sparse example sets realistic integrity constraints first order clauses play role negative examples inductive process one integrity constraint replace long list negative examples however consistency program set integrity constraints usually involves propose efficient constraint satisfaction algorithm applies wide variety useful integrity constraints uses monte carlo strategy generation queries program method allows use integrity constraints instead together negative examples consequence programs induce specified rapidly user ilp system tends obtain accurate definitions average running times greatly affected use integrity constraints compared negative examples
symposium title tutorial makes human explanations effective
trade network game selection paper develops evolutionary trade network game combines evolutionary game play selection successive generations choose trade basis continually updated expected trade selection takes place modified matching mechanism implemented using trade strategies evolved via specified genetic algorithm trade resulting matching mechanism shown stable optimal successive trade cycle nevertheless computer experiments suggest static optimality properties may inadequate measures optimality evolutionary perspective
using neural networks identify neural network method identifying presented idea find efficient mapping certain observed variables done expansion terms network sigmoidal functions using gradient descent procedure errors network method able separate monte carlo generated events accuracy result independent model used approach used study socalled string effect addition identified level just observing particular able separate efficiency comparable expected also neural network method used different schemes dimensionality state space
finding neural using neural network classifier able separate monte carlo generated events accuracy numbers
selforganizing networks extracting features selforganizing neural networks briefly reviewed compared supervised learning algorithms like backpropagation power selforganization networks capability typical features manner successfully demonstrated two applications physics model discrimination separation light
updating multilayer perceptrons updating rule noise added weights learning presented shown improve learning problems initially particularly important multilayer perceptrons many hidden layers often addition updating shown similar effect
approximating learning sets pac learning rectangles studied found experimentally yield excellent hypotheses several applied learning problems also sets rectangles studied recently common dnf circuits randomized approximate distribution independent random variables present improved upper bounds class problems approximating highdimensional rectangles arise pac learning
functional transfer knowledge disease diagnosis distinction two forms task knowledge transfer representational functional reviewed followed discussion modified version multiple task learning neural network method functional transfer method employs separate learning rate task output node varies function measure task primary task interest network applied diagnostic domain four levels disease results experiments demonstrate ability develop predictive model one level disease superior diagnostic ability models produced either single task learning standard multiple task learning
genetic algorithms adaptive planning path trajectory mobile robot paper proposes genetic algorithms gas path planning trajectory planning autonomous mobile robot approach advantage gas work even environment timevarying unknown therefore suitable offline online motion planning first presents path planning simulation results performance randomly generated shown discuss extensions solving path planning trajectory planning simultaneously
vapnikchervonenkis dimension recurrent neural networks work vapnikchervonenkis dimension neural networks focused feedforward networks however recurrent networks also widely used learning applications particular time relevant parameter paper provides lower upper bounds dimension networks several types activation functions discussed including threshold polynomial sigmoidal functions bounds depend two independent parameters number weights network length input sequence contrast feedforward networks dimension bounds expressed function important difference recurrent feedforward nets fixed recurrent net receive inputs arbitrary length therefore particularly interested case multiplicative constants main results roughly following architectures activation fixed nonlinear polynomial dimension architectures activation fixed polynomial dimension architectures activation threshold nets dimension log log standard sigmoid dimension earlier version paper appeared proc workshop computational learning theory pages
abstraction decomposition hillclimbing design optimization performance hillclimbing design optimization improved abstraction decomposition design space methods automatically finding exploiting abstractions presented paper technique called operator importance analysis finds useful abstractions determining given set operators important given class design problems hillclimbing search runs faster performed using smaller set operators technique called operator interaction analysis finds useful measuring pairwise interaction operators uses measurements form ordered partition operator set partition used hillclimbing algorithm runs faster ordinary hillclimbing operator set implemented techniques tested domain design experimental results show two methods produce substantial speedups little loss quality resulting designs
learning separable boolean functions linear threshold unit trees networks paper investigates algorithm construction decisions trees linear threshold units also presents novel algorithm learning separable boolean functions using networks decision trees construction networks discussed performance learning compared standard backpropagation sample problem many irrelevant attributes introduced algorithm also explored within architecture means learning presence many irrelevant attributes learning ability architecture larger necessary networks also explored
causal inference path analysis recursive structural equations models methodology research program research program research primary trial results parts journal medical association january pearl pearl aspects graphical models connected causality technical report cognitive systems laboratory submitted short version proceedings session international statistical institute papers august book
generating neural networks induction threshold logic unit trees paper investigates generation neural networks induction binary trees threshold logic units initially describe framework tree construction algorithm show helps bridge gap pure connectionist neural network symbolic decision tree paradigms also show trees threshold units induce transformed neural network topology several methods learning linear discriminant functions node tree structure examined shown produce accuracy results comparable classical information theoretic methods constructing decision trees use single feature tests node produce trees smaller thus easier understand moreover results also show possible simultaneously learn topology weight settings neural network simply using training data set initially given
experiments algorithm technical report revised august
learning visual concepts dnf formulae consider problem learning dnf formulae pac models develop new approach called polynomial shown useful learning new dnf formulae known learnable unlike previous learnability results dnf formulae limited number terms number variables per term yet contain corresponding classes special cases apply dnf results problem learning visual concepts obtain learning algorithms several natural visual concepts appear natural boolean hand show learning natural visual concepts hard learning class dnf formulae also consider robustness results various types noise
context plan recognition application traffic monitoring typical approaches plan recognition start representation agents possible plans reason observations agents actions assess plausibility various candidates view task consistent prior work accounts context plan generated mental state planning process agent consequences agents actions world present general bayesian framework view focus context exploited plan recognition demonstrate approach problem traffic monitoring objective induce plan observation vehicle movements starting model generates plans show context appropriately influence interpretation observed
logarithmic time parallel bayesian inference present parallel algorithm exact probabilistic inference bayesian networks networks variables worstcase time complexity olog parallel machine processors constant number evidence variables arbitrary networks time complexity log processors log processors maximum range variable induced width maximum clique size network
learning convex sets probability data several theories inference decision employ sets probability distributions fundamental representation belief paper investigates connection empirical data convex sets probability distributions building earlier work framework advanced sequence random outcomes described drawn convex set distributions rather just single distribution extra generality detected observable characteristics outcome sequence paper presents new asymptotic convergence results laws large numbers probability theory concludes comparison approach approaches based prior constraints university
report submitted laboratory available via final report rational distributed reason maintenance abstract efficiency plans largescale distributed activities revised incrementally parts plans revised expected utility identifying revising improve expected utility using original plan problems identifying affected changed circumstances goals closely related problems revising beliefs new changed information gained traditional techniques reason standard method belief revisions arbitrarily global notions consistency may mean beliefs plan elements step develop revision methods revise beliefs plans revising tolerate less costly revision effort use artificial market planning revision tasks overall present representation qualitative preferences permits capture common forms dominance information
reconstruction neural network feedforward neural network method developed invariant approach illustrated produced neural network method yields results superior conventional methods neural network application differs classification ones sense analog number computed network rather binary decision made application clearly demonstrates need using intelligent variables instances amount training instances limited
dna new asocs model improved implementation potential new class massively parallel computing models called asocs adaptive selforganizing concurrent systems proposed current analysis suggests may problems implementing asocs models using hierarchical network structures originally proposed problems inherent models rather technology used implement led development new asocs model called dna asocs depend hierarchical node structure success three areas dna model briefly discussed paper flexible nodes dna problems models nodes dna operates processing learning
using casebased reasoning mobile robot navigation paper presents approach mobile robot path planning using casebased reasoning together path planning path planner used solutions paths information planning paths according former experience least
determining successful strategies evolutionary approach successful open multiagent environments autonomous agents must capable adapting strategies circumstances end present empirical study showing relative success different strategies different types opponent different environments particular evolutionary approach strategies correspond genetic material genetic algorithm conduct series experiments determine successful strategies see strategies evolve depending context agents opponent
bayesian estimation model choice item response models
toward rational planning rational reason maintenance reasoning qualitative preferences formal notions efficiency plans largescale distributed activities revised incrementally parts plans revised expected utility identifying revising improves expected utility using original plan problems identifying affected changed circumstances goals closely related problems revising beliefs new changed information gained traditional techniques reason standard method belief revisions arbitrarily global notions consistency may mean beliefs plan elements step address problems developed revision methods aimed revising beliefs plans revising less costly revision effort artificial market planning revision tasks overall representation qualitative preferences permits capture common forms dominance information view activities intelligent agents simultaneous planning execution observation model plan construction process agents continually evaluate revise plans light world planning necessary organization largescale activities decisions actions taken future direct impact done term even value plan changing circumstances resources information objectives original course action inappropriate changes occur execution plan may necessary construct new plan starting scratch revising previous plan portions plan actually affected changes given information plan execution remaining parts original plan ways parts changed incremental first involves potential changes identifying subset beliefs plans occur involves choosing identified beliefs plans keep change efficiency choices plan revise revise based coherent preferences among consequences different alternatives rational sense decision theory work toward rational planning four main issues paper latter three issues approach first see incremental local manner requires planning procedures identify assumptions made planning plan elements assumptions may change portions plan dependent upon assumptions question new information consequently problem revising plans account changed conditions much
induction selective bayesian classifiers paper examine previous work naive bayesian classifier review limitations include sensitivity correlated features problem naive bayesian induction scheme within algorithm greedy search space features hypothesize approach will improve asymptotic accuracy domains involve correlated features without reducing rate learning ones report experimental results six natural domains including comparisons decisiontree induction support hypotheses discuss approaches extending naive bayesian classifiers outline directions future research
comparison induction algorithms selective bayesian classifiers paper present novel induction algorithm bayesian networks selective bayesian network classifier selects subset attributes predictive accuracy prior network learning phase thereby learning bayesian networks bias small networks compare performance classifier selective naive bayesian classifiers show selective bayesian network classifier performs significantly better versions naive bayesian classifier almost databases analyzed hence enhancement naive bayesian classifier relative bayesian network classifier selective bayesian network classifier generates networks computationally simpler evaluate display predictive accuracy comparable bayesian networks model features
minimax estimation via wavelet shrinkage attempt unknown function noisy sampled data using bases supported develop nonlinear method works wavelet domain simple nonlinear shrinkage empirical wavelet coefficients shrinkage nearly minimax wide range smoothness constraints asymptotically minimax besov linear estimates achieve even minimax rates besov classes method significantly outperform every linear method kernel smoothing spline minimax sense variants method based simple threshold nearly minimax method interpretation spatial using kernel may vary shape point point depending data least distributions certain besov scales generate objects sparse wavelet many real objects similarly sparse suggests minimax results relevant practical problems paper discuss practical implementation spatial adaptation properties applications inverse problems acknowledgements work first author berkeley research supported nsf contract grant foundation second author supported part nsf grants grant earlier version optimal function estimation november technical reports statistics berkeley
evolving data structures genetic programming selection good software engineering practice partitioning established good software engineering practice ensure programs use memory via abstract data structures lists provide interface program memory program memory management details data structures implement main result presented automatically generate typically abstract data structures support multiple operations put get show simultaneously evolve operations data structure implementing operation independent program tree consists fixed number independent program trees moreover crossover genetic material program trees implement operation program trees interact via shared memory shared automatically defined functions
theory correlations stochastic neural networks one main experimental tools interactions neurons measurement correlations activity general however interpretation observed correlations difficult since correlation pair neurons influenced direct interaction also dynamic state entire network belong thus comparison observed correlations predictions specific model networks needed paper develop theory neuronal correlation functions large networks several highly connected stochastic dynamic rules networks asynchronous states relatively weak relative order size interacting populations using general equations express matrix terms mean neuronal activities effective interaction matrix presented effective interactions synaptic gain neurons matrix expressed sum exponentially modes correspond effective interaction matrix theory extended networks random connectivity randomly networks allows comparison contribution internal common input direct
note dirichlet process prior bayesian nonparametric inference partial technical report department statistics university washington assistant professor adrian raftery professor statistics department statistics university washington box seattle research supported onr grant grants
local feedforward networks although feedforward neural networks suited function approximation applications networks experience problems learning desired function one problem interference occurs learning one area input space causes another area networks less interference spatially local networks understand properties theoretical framework consisting measure interference measure network localization developed incorporates network weights architecture also learning algorithm using framework analyze sigmoidal multilayer perceptron mlp networks employ learning algorithm address familiar sigmoidal networks inherently demonstrating given sufficiently large number parameters sigmoidal made arbitrarily local ability represent continuous function compact domain
mixture experts model considerable body evidence face recognition object recognition indicates visual system specialized functional area mechanisms appropriate face processing present modular neural network composed two expert networks one network task learning recognize faces individuals classifying objects members one three classes learning task network tends two expert modules one expert specializing face processing specializing object processing training observe networks performance test set one experts results roughly agree data reported patients damage face expert increases networks face recognition performance decreases dramatically object classification performance conclude datadriven competitive learning two unbiased functional units give rise localized face processing selective damage system
modeling cortical plasticity based adapting lateral interaction neural network model called cooperative selforganization afferent lateral connections cortical maps applied modeling cortical plasticity selforganization maps dynamic equilibrium input like cortex response simulated cortical model predicts adapting lateral interactions fundamental cortical suggests techniques following sensory cortical
genetic programming playing filter functions
stochastic logic programs one way represent machine learning algorithms bias hypothesis instance space pair probability distributions approach taken within bayesian learning schemes framework however obvious inductive logic programming ilp system best provided probability distribution paper extends results previous paper author introduced stochastic logic programs means providing structured definition probability distribution stochastic logic programs generalisation stochastic grammars stochastic logic program consists set clauses interval clause stochastic logic program semantics one assigns probability distribution predicate base clauses probabilities assigned according strategy employs stochastic selection rule shown probabilities computed directly logic programs arbitrary logic programs stochastic proof strategy used provide three distinct functions method sampling base used provide selected targets example sets ilp experiments measure information content examples hypotheses used guide search ilp system simple method conditioning given stochastic logic program samples data functions used measure generality hypotheses ilp system supports implementation bayesian technique learning positive examples paper extension paper title appeared
inductive constraint logic problem novel approach learning first order logic formulae positive negative examples incorporated system named icl inductive constraint logic icl examples viewed interpretations true false target theory whereas present inductive logic programming systems examples true false facts clauses furthermore icl uses clausal representation corresponds conjunctive normal form forms constraint positive examples whereas classical learning techniques concept representations disjunctive normal form present experiments new system problem experiments illustrate differences systems indicate approach work least classical approaches
probability chaos large dynamical systems monte carlo study paper report result monte carlo study probability chaos large dynamical systems use neural networks basis functions system dynamics choose parameter values networks randomly results show dimension system complexity network increase probability chaotic dynamics increases since neural networks dense set dynamical systems large systems chaotic
evolution frequency source identification problem using genetic programming automated synthesis analog circuits difficult problem genetic programming used evolve topology component circuit perform source identification correctly signal categories
maximum likelihood algorithms independent component analysis somewhat biologically plausible involving bell sejnowski derived blind signal processing algorithm nonlinear feedforward network information maximization viewpoint paper first shows algorithm viewed maximum likelihood algorithm optimization linear generative model third paper gives partial proof mixture sources separable ica algorithm
algorithms pca present expectationmaximization algorithm principal component analysis pca algorithm allows extracted large high dimensional data computationally efficient space time also naturally missing information also introduce new variant pca called principal component analysis defines proper density model data space learning also done algorithm report results synthetic real data showing algorithms correctly efficiently find leading covariance datasets iterations using dimensions
training algorithms hidden markov models using entropy based distance functions present new algorithms parameter estimation hmms adapting framework used supervised learning construct iterative algorithms maximize likelihood observations also attempting close current estimated parameters use bound relative entropy two hmms distance measure result new iterative training algorithms similar algorithm training hmms proposed algorithms composed step similar expectation step new update parameters maximization step algorithm takes time per iteration approximated version uses expectation step evaluate experimentally new algorithms synthetic natural speech data sparse models models relatively small number parameters proposed algorithms require significantly fewer iterations
boolean functions fitness spaces investigate distribution performance boolean functions boolean inputs particularly parity functions parity functions uniform random sampling sampling random full trees expected dramatically changes fitness distributions cases minimum size threshold distribution performance approximately independent program length however distribution performance full trees different trees varies tree depth consider testing free lunch theorems functions
analysis markov chain sampler technical report unit university abstract analyse convergence simple markov chain model several markov chain sampling methods used practice theoretical numerical results show indeed lead improvements behavior simple markov chain sampling schemes analysis uses probabilistic techniques explicit thank david help
neural architecture content storage recall theory applications
mixtures probabilistic principal component principal component analysis pca one popular techniques processing data although effectiveness limited global nonlinear variants pca proposed alternative paradigm capture data complexity combination local linear pca however conventional pca correspond probability density unique way combine pca models previous attempts formulate mixture models pca therefore extent paper pca formulated within framework based specific form gaussian latent variable model leads mixture model probabilistic principal component whose parameters determined using algorithm discuss advantages model context clustering density modelling local dimensionality reduction demonstrate application image compression handwritten digit recognition
early stopping linear networks
new methodology reducing genetic programming optimized extended twodimensional programs independently evolved using fixed fitness cases programs subsequently tested large representative fixed population determine relative effectiveness paper describes implementation original modified systems summarizes results tests
evolution using genetic programming genetic programming used evolve topology numerical values component low
objective functions neural map formation institute neural computation technical report series january university california san abstract computational models neural map formation considered least three different levels abstraction detailed models including neural activity dynamics weight dynamics abstract neural activity dynamics approximation objective functions weight dynamics may derived gradient paper present example objective function derived detailed nonlinear neural dynamics systematic investigation reveals different weight dynamics introduced previously derived objective functions generated terms includes dynamic link matching special case neural map formation focus particular role coordinate transformations derive different weight dynamics objective function coordinate transformations also important deriving normalization rules constraints several examples illustrate objective functions help understanding generating comparing different models neural map formation techniques used analysis may also useful investigating types neural dynamics
continuous sigmoidal belief networks trained using slice sampling realvalued random hidden variables useful modelling latent structure explains correlations among observed variables propose simple unit adds gaussian noise input sigmoidal function units produce variety useful behaviors ranging deterministic binary stochastic continuous stochastic show slice sampling used inference learning topdown networks units demonstrate learning two simple problems
sequential update bayesian network structure obvious need improving performance accuracy bayesian network new data observed errors model construction changes dynamics domains information new data sequential update parameters fixed structure accomplished using standard techniques sequential update network structure still open problem paper investigate sequential update bayesian networks parameters structure expected change introduce new approach allows flexible manipulation tradeoff quality learned networks amount information past observations formally describe approach including necessary modifications scoring functions learning bayesian networks evaluate effectiveness empirical study extend case missing data
observations cortical mechanisms object recognition learning paper several aspects cortical architecture visual object recognition based recent computational model scheme relies modules learning examples networks basic components models intended precise theories biological rather capture class explanations call memorybased models contains sparse population coding memorybased recognition prototypes unlike sigmoidal units artificial neural networks units consistent usual description cortical neurons multidimensional optimal stimuli will describe example may realized terms cortical mechanisms consistent psychophysical data number predictions techniques made describes research done within center biological computational learning department brain cognitive sciences artificial intelligence laboratory massachusetts institute technology research sponsored grants office naval research grant national science foundation contract asc9217041 award includes provided program additional support provided organization atr visual perception research laboratories electric corporation siemens support laboratorys artificial intelligence research provided contract supported college
collective adaptation sharing building blocks
using qualitative relationships bounding probability distributions exploit qualitative probabilistic relationships among variables computing bounds conditional probability distributions interest bayesian networks using qualitative relationships implement abstraction operations guaranteed bound distributions interest desired direction evaluating incrementally improved approximate networks algorithm obtains bounds converge exact distributions utility functions bounds reduce set decision alternatives
latent item response models
serial parallel discrimination parallel algorithm proposed fundamental problem machine learning discrimination algorithm based minimizing error function associated set highly structured linear characterize separation sets maximum functions error function continuous gradient allows use fast serial parallel minimization algorithms serial algorithm considerably faster previous linear programming parallel gradient distribution algorithm used problem preliminary computational results given
comparison crossover mutation genetic programming paper presents large systematic body data relative effectiveness mutation crossover combinations mutation crossover genetic programming literature traditional genetic algorithms contains related studies mutation crossover differ traditional significant ways paper present results large experimental data set equivalent approximately typical runs system systematically exploring range parameter settings resulting data may useful optimize parameters runs also exploring issues role building blocks
random markov chain monte carlo using ordered technical report department statistics university toronto markov chain monte carlo methods gibbs sampling simple forms metropolis algorithm typically move distribution sampled via random walk complex highdimensional distributions commonly encountered bayesian inference statistical physics distance iteration algorithms will usually small difficult impossible transform problem eliminate dependencies variables inherent taking small steps greatly algorithm operates via random walk case moving point steps away will typically take around iterations random sometimes using variants gibbs sampling algorithm methods largely restricted problems full conditional distributions gaussian present markov chain monte carlo algorithm based order statistics widely applicable particular algorithm applied full conditional distributions distribution functions inverse distribution functions efficiently computed method demonstrated inference problem simple hierarchical bayesian model
functional relation recognition error report reviews various optimum decision rules pattern recognition namely bayes rule rule optimum tradeoff recently proposed rule latter provides optimum tradeoff error rate average number selected classes new general relation error rate average number classes presented error rate directly computed function turn estimated patterns simply theoretical practical implications discussed future research directions proposed
distributed collective adaptation applied hard combinatorial optimization problem utilize collective memory integrate weak strong search heuristics find family graphs construct pruning partial solutions will weak heuristic maintains local collective memory examine impact distributed search various characteristics distribution collective memory search algorithms family graphs find distributed search performs better individuals even though space partial solutions
knowledge acquisition machine learning system paper investigate integration knowledge acquisition machine learning techniques argue existing machine learning techniques made useful knowledge acquisition tools allowing expert greater control interaction learning process describe number extensions multistrategy learning program greatly enhanced power knowledge acquisition tool particular attention utility maintaining connection rule set examples explained rule objective research make modification domain theory analogous use prototype knowledge acquisition tool constructed order evaluate strengths weaknesses approach
defining relative likelihood structures starting likelihood preference order worlds extend likelihood ordering sets worlds natural way examine resulting logic earlier considered notion relative likelihood context studying assumed total preference order worlds arise examining partial orders present total orders involving exact approach order worlds order sets worlds addition logic relative likelihood case partial orders gives insight connection relative likelihood default reasoning
search boolean satisfiability problems give searching start empirically investigate properties search space behavior hillclimbing search solving hard random boolean satisfiability problems experiments frequently observed rather attempting extensive search better completely new random initial state optimum point search determined empirically range problem sizes rate optimum faster linear number features although exact rate determined based empirical results simple runtime heuristic proposed determine give searching heuristic closely approximates empirically determined optimum values range problem sizes consequently allows search algorithm automatically strategy particular problem without prior knowledge problems complexity
rapid quality estimation neural network input representations choice input representation neural network impact accuracy classifying novel instances however neural networks typically computationally expensive train making difficult test large numbers alternative representations paper introduces fast quality measures neural network representations allowing one quickly accurately estimate collection possible representations problem best show measures ranking representations accurate previously published measure based experiments three difficult realworld pattern recognition problems
results controllability properties discretetime nonlinear systems abstract controllability questions discretetime nonlinear systems addressed paper particular search conditions notion implies stronger property forward show implication holds states weak stability property globally exists global attractor system
reformulation smooth smoothing methods globally convergent method propose algorithm solving systems monotone equations combines point projection important property algorithm whole sequence always globally convergent solution system without additional assumptions moreover standard assumptions local rate convergence achieved opposed classical strategies methods computing use aimed value function instead approximate direction used construct appropriate hyperplane current solution set step followed current onto hyperplane global convergence algorithm computational cost iteration method order classical method crucial advantage method globally convergent particular get stationary point function presented algorithm motivated hybrid point method proposed
transmission information genetic programming paper shows performance genetic programming system improved addition mechanisms transmission information individuals previously shown genetic programming systems enhanced addition memory mechanisms individual programs paper show memory mechanism changed allow communication individuals within across generations show effects memory performance genetic programming system symbolic regression problem problem world agent problems show reduce computational effort required solve problems conclude discussion possible improvements
kind adaptation cbr systems need review current practice paper reviews large number cbr systems determine sort adaptation currently used three proposed taxonomy cbr systems taxonomy tasks performed cbr systems taxonomy adaptation knowledge extent set existing systems constraints feasible review shows interesting dependencies different tasks systems achieve adaptation needed meet system goals cbr system may find partition cbr systems division adaptation knowledge suggested paper useful moreover paper may help focus initial stages systems development suggesting basis existing work types adaptation knowledge supported new system addition paper provides framework preliminary evaluation comparison systems
analysis decision boundaries generated constructive neural network learning algorithms constructive learning algorithms offer approach incremental construction artificial neural networks pattern classification examples algorithms include algorithms construct multilayer networks threshold logic units multilayer perceptrons algorithms differ terms topology networks construct turn biases search decision boundary correctly training set paper presents analysis algorithms perspective analysis helps better characterization search bias employed different algorithms relation distribution examples training set simple experiments non linearly separable training sets support results mathematical analysis algorithms suggests possibility designing efficient constructive algorithms dynamically choose among different biases build networks pattern classification
problem references diffusion limit class randomly growing binary trees probability theory gene evidence sequence dna sequences boundaries proceedings national science prediction human donor acceptor sites dna sequence journal molecular biology information hand mathematical information theory reliable communication john john validity calculations molecular biological sequence journal theoretical biology root introduction theory random signals noise john complexity used map functional domains dna gene sequences research exons science mathematical theory communication bell system peter entropy measure sequence variability proteins measurements effects coding protein dna sequence use finding genes research scientific van trees detection estimation theory molecular biology gene improved version algorithm transactions information theory string matching theorems applications data compression statistics thesis university universal algorithm sequential data compression ieee transactions information theory
models modeling world mixture time scales temporaldifference learning used just predict rewards commonly done reinforcement learning also predict states learn model worlds dynamics present theory algorithms models world different levels temporal abstraction within single structure models used modelbased reinforcementlearning architectures dynamic programming methods place conventional markov models enables planning higher levels abstraction may prove useful methods hierarchical planning reinforcement learning paper prediction learning model value function case fixed agent behavior within context establish theoretical foundations models derive algorithms learning two small computational experiments presented test illustrate theory work extension generalization work sutton
abstract paper scientific comparison two code generation techniques identical goals generation best possible software code computers instruction level parallelism variants modulo scheduling framework generation software pipelines otherwise quite one technique developed used compiler production compiler systems based processor essentially possible schedules extensive pruning method heuristic way also interaction register allocation scheduling second technique produce optimal results scheduling register allocation problem integrated integer linear programming ilp problem idea received much recent literature knowledge previous implementations preliminary detailed measurement evaluation particular believe first published measurement runtime performance ilp based generation software pipelines particularly valuable result study evaluation heuristic technology compiler one behind research optimal software practical use production useful evaluation validation comparison indeed provided quantitative validation leading increased confidence techniques
instructions paper available paper published compiler construction pages delayed exceptions speculative execution abstract superscalar processors execute basic blocks use much instruction level parallelism speculative execution proposed execute basic blocks parallel pure software approach low performance instructions executed propose delayed exceptions combination hardware compiler extensions provide high performance correct exception handling speculative execution delayed exceptions exploit fact exceptions compiler assumes typical case exceptions schedules code runtime code ensure correct execution exceptions happen
reward reinforcement learning applied fuzzy rule tuning fuzzy rules control effectively via reinforcement learning reinforcement learning weak learning method requires information success failure control application tuning process allows people generate fuzzy rules accurately perform control rules provide smooth control paper explores new simplified method using reinforcement learning tuning fuzzy control rules shown learned fuzzy rules provide control pole balancing domain another approach
automatic generation adaptive programs automatic generation adaptive programs animals
discovery hierarchical behaviors procedural representations control policies two advantages problem learning tasks first implicit potential inductive generalization large set situations second facilitate paper compare several randomized algorithms learning modular procedural representations main algorithm called adaptive representation learning genetic programming extension relies discovery suitable learning hierarchies constructing policies complex tasks successfully tested typical reinforcement learning problem controlling agent dynamic nondeterministic environment discovered correspond agent behaviors
journal convex analysis accepted hybrid point algorithm propose modification classical point algorithm finding maximal monotone operator space particular approximate point iteration used construct hyperplane current solution set problem step followed projection current onto separating hyperplane information required projection operation available end approximate step therefore projection additional computational cost new algorithm allows significant relaxation requirements solution point yields practical framework weak global convergence local linear rate convergence established suitable assumptions additionally presented analysis yields alternative proof convergence exact point method allows geometric interpretation somewhat intuitive classical proof
resource framework integrating register allocation local global present resource framework integrating register allocation instruction scheduling based measure reduce paradigm technique measures resource requirements program uses measurements code better resource allocation technique applicable allocation different types resources programs resource requirements register functional unit resources first measured using unified representation measurements used find areas resources either utilized called resource sets respectively conditions determined increasing resource resource conditions applicable local global code motion
learning distributions random introduce new model distributions generated random graphs model suggests variety learning problems using definitions models distribution learning defined framework general enough model previously studied distribution learning problems suggest new applications describe special cases general problem investigate relative difficulty present algorithms solve learning problem various conditions
learning decision structures decision rules system decision structure acyclic graph specifies order tests applied object situation decision object simple powerful tool decision process paper proposes methodology learning decision structures oriented toward specific decision making situations methodology consists two phases determining declarative rules describing decision process deriving online decision structure rules first step performed expert inductive learning program learns decision rules examples decisions second step decision rules decision structure suitable given decision making situation system implementing second step applied problem construction engineering experiments outperformed programs applied problem terms accuracy simplicity generated decision structures key words machine learning inductive learning decision structures decision rules attribute selection
constructing nominal xofn attributes constructive induction researchers focus new boolean attributes paper reports new constructive induction algorithm called xofn constructs new nominal attributes form xofn representations xofn set containing one attributevalue pairs given instance value corresponds number attributevalue pairs true promising preliminary experimental results artificial realworld domains show constructing new nominal attributes form xofn representations significantly improve performance selective induction terms higher prediction accuracy lower theory complexity
coevolution pursuit simulation methods results abstract previous paper presented scientific rationale simulating coevolution pursuit strategies present overview simulation methods results results follows first coevolution works produce good good pure process types rather adapted current second eyes also within simulated species example usually evolved eyes like usually evolved eyes even like third kinds coevolution allowing spatially distributed populations gene explicitly spatial program eyes allows symmetry paper concludes possible applications simulated biology
long shortterm memory learning store information extended time intervals via recurrent backpropagation takes long time mostly due insufficient error back flow briefly review analysis problem address introducing novel efficient method called long shortterm memory learn bridge time steps constant error flow constant error within special units multiplicative units learn open close access constant error flow update complexity per time step number weights experimental comparisons recurrent nets neural sequence leads many successful runs learns much faster also solves complex long time tasks never solved previous recurrent network algorithms works local distributed realvalued noisy pattern representations
learners best geometric generalisation linear familiar many analysis perceptron learning method concept forms novel dimension along learning methods present paper shows geometric defined demonstrates accurately predicts performance least one empirical learning method
based motion detection inspired visual motion detection model computational architecture used early designed employs correlation model report onedimensional field motion scene real time using analog techniques successfully tested using standard process
generalization scaling reinforcement learning associative reinforcement learning environment generates input vectors learning system generates possible output vectors reinforcement function computes feedback signals inputoutput pairs task discover inputoutput pairs generate rewards especially difficult cases occur rewards since expected time algorithm grow exponentially size problem reinforcement function regularities learning algorithm exploits learning time reduced algorithms paper describes neural network algorithm called complementary reinforcement backpropagation reports simulation results problems designed offer opportunities generalization
canonical distortion measure feature space classification prove canonical distortion measure optimal distance measure use classification show reduces squared euclidean distance feature space function classes expressed linear combinations fixed set features bounds given required learn experiment presented neural network used classification
voting schemata schema theorem states implicit parallel search behind power genetic algorithm fitness candidate schemata maintain population binary strings schemata string population works solving problem domain fitness schema population solve original problem
mcmc methods bayesian regression analysis model selection objective statistical data analysis describe behaviour system also propose construct check model observed processes bayesian methodology offers one possible approaches estimation unknown components model parameters functional components framework chosen model type however many instances evaluation bayes posterior distribution bayesian solutions difficult intractable even help numerical approximations cases bayesian analysis may performed help simulation techniques called markov chain monte carlo present paper reviews best known approaches mcmc generation deals several typical situations data analysis model construction mcmc methods successfully applied special attention devoted problem selection optimal regression model constructed regression splines functional units
evolving networks using genetic algorithm connectionist learning artificial life studies science complexity eds vol use genetic algorithms back propagation training feedforward neural networks ieee international conference neural networks vol fast genetic selection features neural network classifiers ieee transactions neural networks vol automatic design cellular neural networks means genetic algorithms finding feature third ieee international workshop cellular neural networks applications ieee new efficient reinforcement learning evolution machine learning vol genetic algorithms van new york algorithm selective pressure proceedings third conference genetic algorithms morgan san van hinton neural network simulator department computer science university toronto toronto
data mining association rules unsupervised neural networks results gaussian mixture models factor analysis discussed
associative reinforcement learning functions agent must learn act world trial error faces reinforcement learning problem quite different standard concept learning although good algorithms exist problem general case often quite inefficient exhibit generalization one strategy find restricted classes action policies learned efficiently paper strategy developing algorithms efficiently learn action maps algorithms compared existing methods empirical trials shown good performance
utilizing connectionist learning procedures symbolic case retrieval nets paper describes method certain circumstances allows automatically learn similarity measures ideas connectionist learning procedures particular related hebbian learning combined casebased reasoning engine
note acceptance rate criteria algorithms note considers positive recurrent markov chains probability remaining current state arbitrarily close specifically conditions given ensure central limit theorems averages chain results motivated applications metropolishastings algorithms constructed terms probability involves remaining current state two examples commonly used algorithms given independence sampler metropolis algorithm examples rather although cases problems arise typical problems commonly occurring particular algorithm used like thank useful subject paper
two convergence properties hybrid samplers
reinforcement difference time space reuse time reinforcement learning system limited computational resources unknown environment goal maximize reward obtained limited unknown lifetime system policy arbitrary algorithm mapping environmental inputs internal states outputs new internal states problem realistic unknown environments policy modification process occurring system life may influence environmental states rewards later time existing reinforcement learning algorithms properly deal neither naive exhaustive search among policy candidates even case small search spaces fact reasonable way measuring performance improvements general typical situations missing define measure based novel reinforcement criterion given time satisfied beginning computed currently valid policy modification followed longterm average reinforcement computation time later taken account present method called reinforcement guaranteed achieve neither care whether systems policy allows changing whether multiple interacting learning systems consequences sound theoretical framework success depends success later setting stage sound theoretical framework multiagent learning principles implemented single system using programming language modify policy system consisting multiple agents agent fact just connection fully recurrent reinforcement learning neural net research general reinforcement learning algorithm nets preliminary experiments illustrate theory
overview evolutionary computation evolutionary computation uses computational models evolution processes key elements design implementation problem solving systems paper provide overview evolutionary computation describe several evolutionary algorithms currently interest important similarities lead discussion important issues need items future research
task spatial frequency effects face specialization strong evidence face processing localized brain face recognition occurring brain damage visual object difficulty recognizing kinds complex objects indicates face object recognition may partially independent mechanisms brain neural specialization learned suggest specialization result competitive learning mechanism development neural resources tasks best performing suggest specialization arises interaction task requirements developmental constraints paper present feedforward computational model visual processing two modules classify input stimuli one module receives low spatial frequency information receives high spatial frequency information task identify faces simply classifying objects low frequency network shows strong specialization faces combination tasks inputs shows strong specialization take results support idea face processing module
theoretical rates convergence markov chain monte carlo present general method proving rigorous priori bounds number iterations required achieve convergence markov chain monte carlo describe bounds specific models gibbs sampler obtained general method discuss possibilities obtaining bounds generally
correlated action effects decision theoretic regression much recent research decision theoretic planning markov decision processes mdps model choice make solution tractable exploiting problem structure one particular algorithm structured policy construction achieves means decision theoretic analog goal regression using action descriptions based bayesian networks treestructured conditional probability tables algorithm presented able deal actions correlated effects describe new decision theoretic regression operator straightforward extension requires somewhat complicated technical approach
better trained problem programming artificial follow santa repeatedly used benchmark problem recently shown performance several techniques much better best performance using uniform random search suggested program fitness landscape difficult hill problem also difficult genetic algorithms contains multiple levels problem approximately correct order simple genetic programming system size depth restriction show perform approximately three times better improved training function
abstract general machine learning process use heuristic knowledge problem solution example genetic programming uses type information reduce search space improve performance unfortunately also generality generated programs suitable inputs specified type improves allowing type information expressed generic manner yet still constraints search space paper describes system generate programs programs take inputs one type produces outputs one type also demonstrate operation generation map program
boosting naive bayesian learning although socalled naive bayesian classification makes assumption values attributes example independent given class example learning method successful practice uniformly better learning method known boosting general method combining multiple classifiers due freund schapire paper shows boosting applied naive bayesian classifiers yields combination classifiers equivalent standard feedforward multilayer perceptrons result naive bayesian classification nonparametric nonlinear generalization logistic regression training algorithm naive bayesian learning quite different backpropagation advantages boosting requires linear time constant space hidden nodes learned incrementally starting important realworld datasets method far generalization performance good better best published result using learning method unlike standard learning algorithms naive bayesian learning without boosting done logarithmic time linear number parallel computing units learning methods highly plausible computationally models animal learning arguments suggest plausible also
improving class prediction using feature weights paper addresses problem handling class distributions within casebased learning framework first present baseline algorithm apply three data sets natural language processing class distributions although overall performance baseline algorithm good show algorithm exhibits poor performance class instances present two algorithms designed improve performance class predictions variation creates feature weights first observing path taken test case decision tree created learning task using information gain values create appropriate weight vector use case retrieval applied data sets algorithms shown significantly increase accuracy class predictions maintaining improving overall classification accuracy
codes introduce class iteratively codes generalization codes codes product codes code multiple interact define allowed set result interactions single code state space grows exponentially block length however codes approximate maximum using algorithm factor graph describes code present two new families codes codes codes give results show codes perform regime codes
finding exploiting relevant prior explanations paper present research identifying modeling strategies human use integrating previous explanations current explanations used work develop computational model partially implemented explanation existing system known implementing system uses casebased reasoning identify previous situations explanations potentially affect explanation constructed identified heuristics constructing explanations exploit information ways similar observed human dialogue exploit aspects known context including previous previous seem even previous must taken account order relate new information effectively recently material avoid old material student new thus strategies using dialogue history generating explanations great importance research natural language generation tutorial applications goal work produce computational model effects context explanations implement model intelligent system maintains dialogue history uses planning explanations based study developed taxonomy types contextual effects occur data according functions serve paper focus one important category taxonomy situations explicitly refers previous explanation order point similarities differences material currently explained material presented earlier explanations implementing system uses casebased reasoning identify previous situations explanations potentially affect explanation constructed identified heuristics constructing explanations exploit information ways similar observed produced human building computer system capability will able systematically evaluate hypothesis useful strategy order test hypotheses effects previous explanations building explanation component existing intelligent training system intelligent practice environment training complex using solve problems minimal interaction review behavior session produced human
fixed size storage time complexity learning algorithm fully recurrent continually running algorithm fully recurrent continually running networks williams requires computations per time step number units describe method suited online learning computes exactly gradient requires storage order average time complexity per time step
applications constant convergence rate markov chains quantitative geometric rates convergence reversible markov chains closely related constant hard calculate general state spaces article describes geometric argument bound constant chains bounded subsets
estimating error kernel estimator monitoring convergence markov samplers many markov chain monte carlo problems target density function known normalization constant paper take advantage knowledge facilitate convergence diagnostic markov sampler estimating error kernel estimator firstly propose estimator normalization constant shown asymptotically normal mixing conditions secondly error kernel estimator estimated using normalization constant estimator ratio estimated error true error shown converge probability similar conditions propose sequential estimated error tool monitor convergence markov sampler finally dimensional example given illustrate proposal assistant professor department statistics university california berkeley research supported part research grant university california berkeley grants research office grant national science foundation author grateful peter many helpful discussions comments draft special due help simulation professor per draft two two markov samplers compared example using proposed diagnostic
plausibility measures default reasoning recent years number different semantics defaults proposed structures semantics structures shown characterized set known properties viewed show almost giving yet another semantics defaults uses plausibility measures new approach modeling uncertainty generalize approaches probability measures belief functions possibility measures show earlier approaches default reasoning embedded framework plausibility provide necessary sufficient condition properties sound additional condition necessary sufficient properties complete conditions easily seen hold earlier approaches thus explaining characterized properties
constructive belief rational representation artificial intelligence agents explicit beliefs two parts beliefs explicitly represented memory implicitly represented constructive beliefs repeatedly needed rather many theories knowledge view relation constructive beliefs logical relation beliefs representing constructive beliefs logic belief view however limits ability theory incomplete sets beliefs useful ways argue view belief result rational representation theory agent obtains constructive beliefs using beliefs preferences sense decision theory choose useful conclusions beliefs
roles reasoning theory equal mathematical logic importance reasoning survey growing literature basic notions probability utility rational choice coupled practical limitations information resources influence design analysis reasoning representation systems
new algorithm dna sequence assembly running title new algorithm dna sequence assembly
structured pattern matching approach sequence assembly paper propose efficient reliable sequence assembly algorithm based scheme robust noise sequences data algorithm uses exact matches short patterns randomly selected data identify construct map finally consensus sequence show statistical made explicit approach easily exploited correctly results even presence extensive sequences approach fast practice successfully whole approximately roughly time real data existing algorithms expected run several data moreover experiments data real dna sequences wide range organisms including human dna containing extensive regions demonstrate algorithms robustness noise presence sequences example correctly human dna sequence less time support research provided part office naval research grant
programming environment high performance parallel intelligent communication institute high performance parallel music processor system intelligent communication developed applications neural network simulation molecular dynamics show conventional electric power requirements reduced factor reduced factor price reduced factor software development key using parallel system report focus programming environment music system applications
evolutionary training neural networks paper concerned integration constraint logic programming systems systems based genetic algorithms resulting framework applications require first phase number constraints need generated second phase optimal solution constraints produced first phase carried second one present specific framework common logic programming system genetic algorithm numerical optimization constrained problems integrated framework called computational intelligence plus constraint logic programming system applied training problem neural networks consider constrained networks neural networks shared weights constraints weights example domain constraints hardware implementation etc used generate representation together constraints ensure cases network specified exactly one thus problem becomes constrained optimization problem optimization criterion optimize error network used find optimal solution note work second author partially supported department national foundation scientific research work carried third author author university
logical approach reasoning uncertainty tutorial paper will appear interaction communication eds much work performed author research center support
comparison evolutionary algorithm grouping genetic algorithm graph coloring report also available
geometric gibbs block gibbs samplers hierarchical random effects model
weights using regularities paper study global optimization methods like genetic algorithms used train neural networks introduce notion studying properties error function search space artificial way regularities used generate constraints weights network order find set constraints use constraint logic programming system training network becomes constrained optimization problem also relate notion socalled network transformations
learning programs paper study paclearning algorithms specialized classes deterministic finite automata dfa particular study programs investigate influence width program difficulty learning problem first present algorithm learning programs also give algorithm proper learning programs uniform distribution labeled samples show existence efficient algorithm learning programs imply existence efficient algorithm learning dnf known case finally show existence algorithm learning programs also yield algorithm learning restricted version parity noise
parameterized complexity sequence alignment consensus common problem examined point view parameterized computational complexity several different ways parameters problem number sequences analyzed length common size lower bounds complexity basic problem imply lower bounds number sequence alignment consensus problems issue theory parameterized complexity whether problem takes input solved time independent argued appropriate asymptotic model feasible problems small range parameter values covers important applications situation holds many problems biological sequence analysis main results show common parameterized number sequences analyzed hard problem problem parameterized length common hard problem parameterized number sequences length common complete results obtained sizes fixed size problems tractable remains hard
constructing new attributes decision tree learning
computer view life computable may much terms information requirements compute computable instead just apply basic concepts complexity theory set possible true life generalization learning given assumptions long time great program runs possible computer possible means computable evolves discrete time scale state given time finite number bits one many despite evolved claim computable let arbitrary universal turing machine output input output symbols possible input programs ordered program etc let program list output will finite infinite string sequence separated will interpreted evolution includes least one let represents state time step represented sequence corresponds different algorithms may compute finite whose programs producing outputs point others know important choice turing machine important due compiler theorem universal turing machine exists constant possible programs output response program identical output response compiler programs equivalent programs possibly
candidates metropolishastings algorithms metropolishastings algorithm estimating distribution based choosing candidate markov chain moves candidate produce chain known invariant measure traditional methods use candidates essentially based invariant develop onedimensional distributions class candidate distributions towards high density areas produce metropolishastings algorithms convergence rates appear considerably better known traditional candidate choices random walk particular wide classes choices may effectively help reduce problem illustrate behaviour examples exponential polynomial logistic regression model using gibbs sampling algorithm
data values ieee published proceedings december research personal use material however permission material purposes creating new collective works lists reuse component work works must obtained ieee ieee service center box
learning design characterizing dimensions working systems application machine learning solve practical problems complex recently due increased promise solving real problems difficulty use issue attention difficulty arises complexity learning problems large variety available techniques order understand complexity begin overcome important construct characterization learning situations building previous work practical use set dimensions developed another recent proposal illustrated project development system design general research opportunities emerge development dimensions discussed leading toward working systems simple model presented setting research selecting learning tasks within large projects central development concepts discussed paper use future projects limitations failures
log log learning algorithm dnf uniform distribution show dnf terms size approximated function non zero fourier coefficients expected error squared respect uniform distribution property used derive learning algorithm dnf uniform distribution learning algorithm uses queries learns respect uniform distribution dnf terms size time polynomial log interesting implications case constant case algorithm learns dnf polynomial number terms time olog log dnf terms size olog log log polynomial time
multivariate decision trees technical report december abstract multivariate decision trees overcome representational limitation univariate decision trees univariate decision trees restricted splits instance space features paper discusses following issues constructing multivariate decision trees representing multivariate test including symbolic numeric features learning coefficients multivariate test selecting features include test pruning multivariate decision trees present new review wellknown methods multivariate decision trees methods compared across variety learning tasks assess methods ability find accurate decision trees results demonstrate multivariate methods effective others addition experiments confirm allowing multivariate tests improves accuracy resulting decision tree univariate trees
using stochastic gradient method fit regression models technical report university washington department statistics seattle washington abstract introduced methodology uses adaptively selected linear splines products model conditional class probabilities authors develop methodology work small size problems scale large problems however version developed large problems required two time apply large data set modification methodology involving use stochastic gradient online method fitting models given sets basis functions developed makes methodology applicable large data sets particular successfully applied recognition problem involving features cases training sample basis functions unknown parameters comparisons neural networks made original problem
emergent hierarchical control structures learning relationships reinforcement environments use hierarchical structures reduce complexity learning control common however learning hierarchical structure important step towards general learning many things required less bounded learning single specified learning presented paper reinforcement learning algorithm called nested qlearning generates hierarchical control structure reinforcement learning domains emergent structure combined learned bottomup reactive results reactive hierarchical control system effectively learned hierarchy otherwise evaluation function many smaller evaluation functions without loss previously learned information
online selection using multiplicative updates present online algorithm achieves almost best determined actual market outcomes algorithm employs multiplicative update rule derived using framework introduced algorithm simple implement requires constant storage computing time per stock trading period tested performance algorithm real stock data new york stock exchange period data algorithm clearly outperforms best single stock covers universal selection algorithm also present results situation access additional side information
semantics belief revision systems consider belief revision operators satisfy postulates present logic revision operator result revision described sentence logic logic fact agents set beliefs represented sentence know operator intuitively fact agent represented sentence usual way represents update defined revised beliefs represented sentence show every revision operator satisfies postulates model logic beliefs implied sentence model correspond exactly sentences implied theory results revising means reasoning changes agents beliefs reduces model certain sentences negative result paper type formal account revision extended situation agent able reason beliefs fully introspective agent use construction reason results revisions
learning bayesian networks using feature selection paper introduces novel enhancement learning bayesian networks bias small networks new approach selects subset features predictive accuracy prior network learning phase examine explicitly effects two aspects algorithm feature selection node ordering approach generates networks computationally simpler evaluate display predictive accuracy comparable bayesian networks model attributes
learning hierarchical control structures multiple tasks changing environments need hierarchies within control systems apparent also clear many researchers hierarchies learned learning structure component behaviors difficult task benefit learning hierarchical structures behaviors decomposition control structure smaller allows previously learned knowledge applied new related tasks presented paper improvements nested qlearning allow realistic learning control hierarchies reinforcement environments also presented simulation simple robot performing series related tasks used compare hierarchical learning techniques
ensemble training recent experiments data recent findings suggest classification scheme based ensemble networks effective way address overfitting study optimal methods training ensemble networks recent experiments character data suggest weight decay may optimal method controlling variance classifier
variational gaussian process classifiers
model merging dynamic learning recognition model merging general technique dynamically choosing structure neural related architecture avoiding overfitting applicable learning recognition tasks often generalizes significantly better fixed structures demonstrate approach applied tasks choosing radial basis functions function learning choosing local models curve constraint surface modelling choosing structure maximize efficiency access
geometric convergence given stationary distributions describe algorithms estimating given measure known constant based large class extending model invariant show weak conditions one choose class way converge exponential rate one even ensure convergence independent starting point algorithm convergence less exponential show often polynomial known rates consider methods diffusion time find methods convergence rates continuous time process contrast behaviour naive discretization behave even simple cases
classification eeg signals using sparse polynomial technical report
analysis application hebbian rules linear networks reported development structured receptive fields simulations using synaptic plasticity rule feedforward linear network synapses develop dynamics determined matrix closely related covariance matrix input cell activities analyse dynamics learning rule terms matrix represent independently evolving weight structures general theorems presented regarding properties general covariance matrix four principal parameter predicted gaussian layer network analytic numerical solutions layer presented three dynamics synapses oriented symmetric analysis circumstances vectors yields explanation emergence structures structures criteria developed estimating boundary parameter regime structures emerge application analysis higher layers covariance functions briefly discussed
weak convergence optimal scaling random walk metropolis algorithms paper considers problem scaling proposal distribution multidimensional random walk metropolis algorithm order maximize efficiency algorithm main result weak convergence result dimension sequence target densities converges proposal variance appropriately according sequence stochastic processes formed first component markov chain converge appropriate limiting diffusion process limiting diffusion approximation straightforward efficiency maximization problem resulting asymptotically optimal policy related asymptotic acceptance rate proposed moves algorithm asymptotically optimal acceptance rate quite general conditions main result proved case target density symmetric product form extensions result discussed
learning representations
reactive behaviors keywords reactive systems planning learning planning proposed means potentially slow response times planners still making progress toward long term goals rapid response complexity many environments make difficult tune coordinate reactive behaviors consistency neural networks address tuning problem less useful decomposition coordination hypothesize interacting decomposed separate behaviors separate networks interaction tuning mechanism higher level controller explore issues implemented neural network architecture reactive component two layer control system simulated car varying architecture test whether separate behaviors leads superior overall performance coordination learning convergence
teaching learner introduce formal model teaching teacher particular learner yet teaching designed possible surprisingly model aspects models teacher must successfully consistent learner prove class exactly identified deterministic polynomialtime algorithm access rich set queries computationally teacher polynomialtime learner addition present general results relating model teaching various previous results also consider problem designing pairs teacher learner polynomialtime algorithms describe pairs classes decision lists horn sentences
simple randomized quantization algorithm neural network pattern classifiers paper explores algorithms automatic quantization realvalued datasets using codes pattern classification applications experimental results indicate relatively simple randomized code generation technique result datasets used train simple perceptrons yield generalization test data substantially better obtained
using modeling knowledge guide design space search automated search space candidate designs seems attractive way improve traditional engineering design process make approach work however automated design system must include knowledge modeling limitations method used evaluate candidate designs also effective way use knowledge influence search process suggest approach include knowledge implementing set model constraint functions measure much modeling assumptions influence search using values model constraint functions constraint inputs standard constrained nonlinear optimization numerical method test idea domain conceptual design aircraft experiments indicate model constraint communication strategy decrease cost design space search one orders magnitude
towards concept formation perception action mobile robot recognition objects hence descriptions must environment terms sensor data argue concepts used classify objects used perform actions objects integrate perceptual features action features present symbolic representation concepts moreover concepts learned show approach learning concepts
learning perceptual features robot navigation
improving rbf networks feature selection approach dimensionality one problems concerning application rbf networks number rbf nodes therefore number training examples needed grows exponentially intrinsic dimensionality input space one way address problem application feature selection data preprocessing step paper propose approach determination optimal feature subset first possible reduced best discrimination properties application fast robust filter technique secondly use approach feature subsets leads rbf networks least complexity best classification accuracy experiments show improvement rbf networks feature selection approach
update rules parameter estimation bayesian networks paper problem parameter estimation bayesian networks missing values hidden variables perspective recent work online learning provide unified framework parameter estimation online learning model continuously adapted new data cases traditional batch learning set samples used model selection process batch case framework gradient projection algorithm algorithm bayesian networks framework also leads new online batch parameter update schemes including parameterized version provide empirical theoretical results indicating parameterized allows faster convergence maximum likelihood standard
knowledge compilation speedup learning continuous task domains many techniques speedup learning knowledge compilation focus learning optimization control rules task domains characterized using search paradigm however characterization fit class task domains problem solver required perform continuous manner example many robotic domains problem solver required monitor realvalued perceptual inputs vary motor control parameters continuous online manner successfully task domains discrete symbolic states operators difficult define improve performance continuous problem domains problem solver must learn modify use continuous operators continuously map input sensory information appropriate control outputs additionally problem solver must learn contexts continuous operators applicable propose learning method experiences continuous operators used improve performance problem solver method task performance results improvements quality resulting solutions method implemented robotic navigation system evaluated extensive
query learning study distribution dependent form pac learning uses probability distributions related complexity relate model defined standard model give general technique results
knowledge content case memory systems discussions casebased reasoning often reflect implicit assumption case memory system will become better will increase knowledge cases added paper considers knowledge content necessary preliminary analysis performance casebased reasoning systems particular interested modelling learning aspects casebased reasoning order study performance casebased reasoning system changes problemsolving experience current paper presents semantics recent casebased classification within framework paper explores various issues illustrates knowledge content case memory system seen chosen similarity measure cases
knowledge based systems technical report
case study tuning genetic algorithms using performance evaluation based experimental design paper proposes four performance measures genetic algorithm enable compare different gas problem different choices parameters values performance measures defined terms observations simulation frequency optimal solutions fitness values frequency evolution number generations needed reach optimal solution present case study parameters robot path planning performance optimized performance evaluation using measures especially one performance measures used demonstrate robot path planning also propose process systematic tuning based techniques design experiments
learnability acyclic probabilistic finite automata working draft propose analyze distribution learning algorithm subclass acyclic probabilistic finite automata subclass characterized certain property states though results known learning distributions generated general prove algorithm indeed efficiently learn distributions generated subclass consider particular show distribution generated target source distribution generated hypothesis made small high confidence polynomial time present two applications algorithm first show model written resulting models part complete recognition system second application demonstrate used build models words evaluate based models labeled speech data good performance terms obtained test data achieved small amount time needed learning suggests learning algorithm might powerful alternative commonly used probabilistic models
natural language grammatical inference recurrent neural networks paper examines inductive inference complex grammar neural networks specifically task considered training network classify natural language sentences grammatical thereby kind power provided principles parameters linguistic framework theory neural networks trained without division learned components assumed attempt produce data recurrent neural network possess linguistic capability properties various common recurrent neural network architectures discussed problem exhibits training behavior often present smaller grammars training initially difficult however implementing several techniques aimed improving convergence gradient descent time training algorithm significant learning possible found certain architectures better able learn appropriate grammar operation networks training analyzed finally extraction rules form deterministic finite state automata investigated
fast bounded smooth regression lazy neural trees propose lazy neural tree appropriate architecture realization smooth regression systems hybrid decision tree neural network neural network smoothness generated function incremental conceptual simplicity decision tree topology initial parameter setting efficient sequential implementation outperforms traditional neural network simulations order speed achieved lazy evaluation speedup obtained application scheme region interesting results restricted
using mixtures factor segmentation pose estimation category visual processing preference handwritten digit string helpful image separate digits bottomup segmentation heuristics often fail digits substantially describe system stochastic generative model digit class show knowledge required segmentation system uses gibbs sampling construct perceptual interpretation digit string segmentation arises naturally explaining away effects occur bayesian inference using conditional mixtures factor possible extract explicit compact representation parameters describe pose digit parameters used inputs higher level system models relationships digits technique used model individual digits parameters parts
neural networks statistical models proceedings annual users group international conference
arguments predicates ilp paper investigate problem choosing arguments new predicate identify relevant terms considered arguments propose methods choose among based propositional
method identifying splice sites start sites paper describes new method determining consensus sequences signal start boundaries exons introns donor acceptor sites method takes account dependencies bases contrast usual technique considering position independently coupled dynamic program compute likely sequence new consensus sequences emerge consensus sequence information conditional probability matrices used signals dna greater sensitivity specificity conventional matrices versions matrices especially effective distinguishing true false sites
evolutionary selection preferences evolutionary selection preferences evolutionary forces optimizing natural selection generally assumed dynamics selection need case particularly class selective mechanisms called preferences previous simulation research showed preferences cause populations split separate species paper show preferences cause populations space form selection without influence natural selection preferences free evolve always evolve point direction selection thus take life preferences within species become distinct important part environment species adapt results suggest adaptive behavior potential becomes important finding avoiding present framework simulating wide range preferences discuss practical scientific applications selection
technical diagnosis knowledge sources domain knowledge common knowledge investigated case based reasoning cbr uses knowledge former experiences known cases since special knowledge expert mainly subject experiences cbr techniques good base development expert systems investigate problem technical diagnosis diagnosis considered classification task process guided computer experience corresponds flexible case approach flexibility also needed expert view interest cases
learning semantics simple recurrent networks paper investigates possibilities using simple recurrent networks map sequential natural language input semantics networks perform sentences containing single main predicate encoded applied objects encoded shows robustness inputs second set experiments deals sentences containing embedded structures network able process multiple levels one level turns consequence networks information outputs intermediate phases processing two extensions original recurrent network architecture introduced
table approximation techniques regularisation introduction
emergent control planning autonomous vehicle use connectionist network trained reinforcement control autonomous robot vehicle simulated robot show given appropriate sensory data structure network learn control robot simple navigation problem investigate complex problem examine behavior
applying casebased reasoning control robotics proposed architecture experimentally evaluated two real world domains results compared machine learning algorithms applied problem
complexity learning distributions
tracking concepts minimizing paper consider problem tracking subset domain called target changes gradually time single unknown probability distribution domain used generate random examples learning algorithm measure speed target changes clearly rapidly target moves algorithm maintain good approximation target therefore evaluate algorithms based much movement target examples predicting accuracy furthermore complexity class possible targets measured also effects difficulty tracking target concept show problem minimizing number sample among concepts class approximated within factor simple tracking algorithm achieve probability making target movement rate constant times vapnikchervonenkis dimension also show properly efficient randomized algorithm high probability approximately minimizes within factor efficient tracking algorithm rates constant times addition prove complementary results classes showing maximum rate algorithm even computational power tolerate constant times
feature subset selection using genetic algorithm time needed learning sufficiently accurate practical pattern classification knowledge discovery problems require selection subset attributes features much larger set represent patterns classified due fact performance classifier usually induced learning algorithm cost classification sensitive choice features used construct classifier exhaustive evaluation possible feature subsets usually infeasible practice large amount computational effort required genetic algorithms belong class randomized heuristic search techniques offer attractive approach find nearoptimal solutions optimization problems paper presents approach feature subset selection using genetic algorithm advantages approach include ability accommodate multiple criteria accuracy cost classification feature selection process find feature subsets perform particular choices inductive learning algorithm used construct pattern classifier experiments several benchmark realworld pattern classification problems demonstrate feasibility approach feature subset selection automated many practical pattern classification tasks medical diagnosis require learning appropriate classification function assigns given input pattern typically represented using vector attribute feature values one finite set classes choice features attributes measurements used represent patterns presented classifier affect among things accuracy classification function learned using inductive learning algorithm decision tree induction algorithm neural network learning algorithm features used describe patterns implicitly define pattern language language expressive enough fail capture information necessary classification hence regardless learning algorithm used accuracy classification function learned limited lack information design neural networks pattern classification knowledge discovery
prediction gaussian processes linear regression linear prediction beyond appear main aim paper provide tutorial regression gaussian processes start bayesian linear regression show change viewpoint one see method gaussian process predictor based priors functions rather priors parameters leads general discussion gaussian processes section section deals issues including hierarchical modelling setting parameters control gaussian process covariance functions neural network models use gaussian processes classification problems
general learn ing mechanism machine learning learning university also appears report quantitative results concerning utility explanationbased learning proceedings national conference artificial pages
challenges evolving controllers physical robots paper discusses feasibility applying evolutionary methods automatically generating controllers physical mobile robots overview state field describe main approaches discuss key challenges problems promising directions
general convergence results linear discriminant updates abstract problem learning linear discriminant concepts solved various update procedures including family algorithms wellknown perceptron algorithm paper define general class algorithms includes perceptron special cases give single proof convergence covers much class including perceptron also many novel algorithms proof introduces generic measure progress seems capture much algorithms converge using measure develop simple general technique proving bounds apply new algorithms existing algorithms applied known algorithms technique automatically produces close variants existing generally obtain known bounds within constants thus showing certain sense seem diverse results
retrieval tool software paper present prototype flexible retrieval system flexibility supported allowing specified query moreover algorithm allows retrieved items relevant initial context specified query presented system used supporting tool software repository also discuss system evaluation concerns usefulness applicability evaluation system three domains gives encouraging results integration real software repository retrieval tool ongoing
inductive learning casebased reasoning paper describes application inductive learning techniques casebased reasoning introduce two main forms induction define casebased reasoning present combination evaluation proposed system called carried classification task namely character recognition show inductive knowledge improves knowledge representation turn flexibility system performance terms classification accuracy
casebased reasoning approach fall symposium flexible computation intelligent systems results issues opportunities cambridge abstract paper presents casebased reasoning system address flexibility casebased reasoning process namely flexible retrieval relevant experiences using novel similarity assessment theory advantages approach experimentally evaluated system compared performance performance version machine learning algorithms several domains
planning medical using partially observable markov decision processes diagnosis disease treatment separate activities instead often dependent time mostly due uncertainty underlying disease uncertainty associated response patient treatment varying cost different treatment diagnostic procedures framework particularly suitable modeling complex decision process partially observable markov decision process unfortunately problem finding optimal within standard framework also computationally costly paper investigate various structural extensions standard framework approximation methods allow model construction process larger problems solve faster problem target specifically management patients disease
market framework consider group probability distribution set uncertain events derives single consensus distribution events representative group whole several functions proposed particular assumptions measures many researchers many years form consensus method best propose procedure analyze properties uncertain event maximize expected consensus probability event defined corresponding equilibrium price market framework provides explicit allows agents maintain individual limited arguments ensure equilibrium prices form probabilities show events exponential utility market derives result logarithmic similarly logarithmic utility yields linear cases prove groups behavior rational individual whose beliefs equal equilibrium prices
performance enhanced genetic programming genetic programming increasing basis wide range learning algorithms however technique date successfully applied tasks performance evolving large number data structures many correspond valid program address problem directly demonstrate evolutionary process achieved much greater efficiency use representation strong report initial experimental results demonstrate technique exhibits significantly better performance previous work
dna sequences domain theory dna sequences domain theory database become popular testing systems integrate empirical analytical learning note reports simple change domain theory terms mofn concepts involving learning results accuracy items database moreover exhaustive search space mofn domain theory interpretations indicates expected accuracy randomly chosen interpretation maximum accuracy achieved cases demonstrates domain theory without understanding interactions various learning algorithms theory addition results help characterize difficulty learning using dna theory
classification pairwise coupling discuss strategy classification involves estimating class probabilities pair classes coupling estimates together coupling model similar method comparisons study nature class probability estimates arise examine performance procedure real simulated datasets classifiers used include linear nearest neighbors support vector machine
receptive field role plasticity single site cortex produces large increase number neurons region corresponding receptive field little effect position size response site large changes observed following several restricted region localized region produced training frequency discrimination task improves performance suggested changes caused competitive learning excitatory pathways almost simultaneously excitatory inhibitory excitatory inhibitory cortical neurons within thus paper investigates implications possibility lateral inhibitory pathways may synaptic plasticity lateral inhibitory pathways may also synaptic plasticity animals conditioning exin afferent excitatory lateral inhibitory synaptic plasticity rules
note testing variables draft paper
partial memory incremental learning methodology application computer detection
classification methodology domains work present classification methodology discover concepts domains hierarchies order achieve aim uses conceptual learning techniques classification final target build knowledge bases expert validation techniques improvement results classification step used like using partial expert knowledge classification rules causal structural dependencies attributes delayed cluster objects also comparisons wellknown systems shown
data mining association rules unsupervised neural networks results gaussian mixture models factor analysis discussed
classification using machines constructive function approximation technical report january abstract new classification algorithm combines version linear machine known machine nonlinear function approximator constructs features algorithm finds nonlinear decision boundaries constructing features needed learn necessary discriminant functions algorithm proven separate consistently training instances even linearly separable input variables algorithm illustrated variety tasks
management contextsensitive features review strategies paper review five heuristic strategies handling contextsensitive features supervised machine learning examples discuss two methods recovering implicit contextual information evidence hybrid strategies effect show work several machine learning researchers framework claim strategies possibilities appears framework includes techniques found published literature contextsensitive learning
case retrieval nets foundations properties implementation results
automated discovery linear feedback models
adaptive penalty approach constrained optimization paper describe new adaptive penalty approach handling constraints genetic algorithm optimization problems idea start relatively small penalty increase decrease optimization empirical results several engineering design domains demonstrate proposed approach
structured analysis markov decision processes recent research decision theoretic planning making solution markov decision processes mdps feasible develop family algorithms structured analysis mdps suitable initial state set states known using compact structured representations mdps bayesian networks methods vary tradeoff complexity accuracy produce structured descriptions estimated states used eliminate variables variable values problem description reducing size mdp making easier solve one contribution work extension ideas deal distributed nature action representations typically within bayes nets problem correlated action effects also demonstrate algorithm made complete using constraints instead binary constraints another contribution compact representation constraints exploited several existing exact approximate abstraction algorithms mdps
extraction restrict hypothesis space ilp systems incorporating foil many ilp systems foil take advantage user supplied restrict hypothesis space form type information arguments predicate learned information whether certain argument predicate dependent arguments supplied mode information knowledge explicitly supplied ilp system addition data present paper argues many cases knowledge extracted directly raw data three algorithms presented learn type mode symmetric data algorithms incorporated existing ilp systems form need user explicitly provide information many cases algorithms extract knowledge user either information used ilp system restrict hypothesis space
learning positive data gold showed even regular grammars exactly identified positive examples alone since known learn natural grammars almost examples result used theoretical support theory human linguistic abilities paper new results presented show within bayesian framework grammars also logic programs learnable arbitrarily low expected error positive examples addition show upper bound expected error learner bayes posterior probability learning positive examples within small additive term one mixture positive negative examples inductive logic programming implementation described avoids greedy search global function local construction individual clauses hypothesis results testing implementation datasets reported results agreement theoretical predictions
reconstruction defined wavelet domain prove two results estimator smooth high probability least smooth wide variety smoothness measures adapt estimator comes nearly close mean square estimator uniformly two broad scales smoothness classes two properties several ways proof results develops new facts abstract statistical inference connection acknowledgements results described symposium wavelet theory held connection university author like thank professor conference interesting discussions correspondence related topics author also university california berkeley
adaptive simulated annealing lessons learned control annealing
evolutionary trees general markov model paper study sequence evolution general markov model incorporates every stochastic model found literature particular study error estimation evolutionary dependence sample sequence deriving large results applicable evolutionary tree building algorithm show greedy short algorithms evolutionary tree high probability sequences polynomial length number nodes
optimize analyze application neural network tools patient monitoring results reported application tools optimizing analyzing neural networks patient monitoring task neural network synthesized rulebased classifier optimized set normal classification error rate separate larger test set reduced factor sensitivity analysis synthesized optimized networks differences analysis weights unit activations optimized network reduction size network factor without loss accuracy
modeling dynamic receptive field changes produced localized site cortex produces large increase cortical representation region represented neurons little effect neurons location size exin afferent excitatory lateral inhibitory learning rules used model changes exin model produces similar observed experimentally possible role inhibitory learning producing effects studied simulating exin model lateral inhibitory learning model also produces increase cortical representation region represented compared artificial conditioning suggested lateral inhibitory learning may general principle cortical plasticity
abstract general machine learning process use additional
prices theorem max problem present detailed analysis evolution populations using problem finding program returns maximum possible value given function set depth limit program tree known max problem confirm basic message crossover together program size restrictions responsible convergence suboptimal solution show happen even population high level variety show many cases evolution suboptimal solution solution possible sufficient time allowed cases theoretical models presented compared actual runs experimental evidence presented prices covariance selection theorem applied populations practical effect program size restrictions finally show covariance gene frequency fitness first generations used predict course runs
probabilistic actions present symbolic probabilistic causal information given domain produces probabilistic effect actions impact observations two types conditioning operators ordinary bayes conditioning represents observation causal conditioning probability constant action given mixture causal sentences together topology causal graph derives new conditional probabilities types thus one quantify effects actions observations
cooperative coevolutionary approach function optimization general model coevolution species presented model tested domain function optimization compared traditional function results encouraging two suggest ways performance improved suggest new approach evolving complex structures neural networks rule sets
learning easier learning first paper investigates learning lifelong context lifelong learning addresses situations learner faces whole stream learning tasks scenarios provide opportunity transfer knowledge across multiple learning tasks order generalize accurately less training data paper several different approaches lifelong learning described applied object recognition domain shown across board lifelong learning approaches generalize consistently accurately less training data ability transfer knowledge across learning tasks
utility knowledge inductive learning running knowledge inductive learning
universal without costs constant strategy distribution among set period period recently work online strategies competitive best constant determined cover cover cover cover cover universal algorithm cover cover provide simple analysis naturally extends case fixed cost question raised cover cover cover cover cover addition present simple randomized implementation significantly faster practice conclude explaining algorithms applied problems combining predictions statistical language models resulting guarantees
locally connected recurrent networks young computer science department university new email technical report abstract fully connected recurrent network using online training method real time recurrent learning computationally expensive computational complexity storage complexity number units locally connected recurrent model much lower complexity computational time storage space recurrent network simplest kind locally connected corresponding complexity respectively number input hidden output units respectively compare performance sequence recognition time series prediction tested networks ability temporal power time ability sequence recognition task time series prediction task used networks train predict three series series noise deterministic chaotic series data tasks show needs much training time performance comparable
interpretation complex scenes using bayesian networks object recognition systems interactions objects scene best interpretation considered set objects matches number image features show image interpretation problem finding explanation bayesian network models visual physical object interactions problem determine exact conditional probabilities network shown since goal find configuration objects calculate absolute probabilities furthermore show evaluating configurations feature equivalent calculating joint probability configuration using restricted bayesian network derive assumptions probabilities necessary make bayesian formulation reasonable
practical monte carlo implementation bayesian learning
note scheduling algorithms processors lookahead
universal default theories research default reasoning identified several important criteria alternative default inferences theories reasoning based criteria may uniformly viewed theories rational inference reasoner selects states belief though researchers cases apparent conflict preferences supported different theories special theories reasoning may combined universal logic reasoning show different categories preferences conflict realized adapt formal results social choice theory prove every universal theory default reasoning will least one reasonable principle rational reasoning results interpreted demonstrating within framework expect much improvement priority mechanisms proposed conflict resolution
predicting binary sequence almost optimal biased apply exponential weight algorithm introduced problem predicting binary sequence almost best biased first show case logarithmic loss derived algorithm equivalent bayes algorithm prior studied probabilistic assumptions derive uniform bound holds sequence also show empirical distribution sequence bounded away length sequence increases difference bound corresponding bound average case algorithm asymptotically optimal case show gap necessary calculating optimal algorithm problem showing asymptotic upper bound tight also study application algorithm square loss show algorithm derived case different bayes algorithm better prediction worstcase
game theory online prediction boosting study close connections game theory online prediction boosting brief review game theory describe algorithm learning play repeated games based online prediction methods analysis algorithm yields simple proof von theorem method approximately solving game show online prediction model obtained applying algorithm appropriate choice game boosting obtained applying algorithm dual game
global unified resource requirements representation instruction level parallelism ilp integration optimization phases lead improvement quality code generated however since several different representations program used various phases partial integration achieved date present program representation combines resource requirements information control data dependence information representation enables integration several optimizing phases including transformations register allocation instruction scheduling basis integration simultaneous allocation different types resources define representation show constructed formulate several optimization phases use representation achieve better integration
evolving control structures automatically defined evolving control structures automatically defined
evolution communication schemes continuous channels many problems design multiagent systems least information agents others hand implement communication semantics explore method communication evolve experiments described model agents connectionist networks agent number channels implemented addition input output units channel output units environmental signals whose decay distance environmental noise agent receive input individuals rather agents input agents output signals along channel use realvalued activations agents using realvalued vectors evolutionary program agents communication scheme continuous channels information
field genetic programming application increases need parallel implementations becomes necessary system presented koza one parallel implementations today implementation proposed parallel using simd architecture except approach although others exploited one reason apparent difficulty dealing parallel evaluation different single instruction executed time every processor aim chapter present implementation parallel simd system processor efficiently evaluate different implemented approach computer will present results extent simd machines like available offer cycles scientific experimentation useful approach idea simulating mimd machine using simd architecture new one original ideas connection machine simulate parallel architectures indeed processor simd architecture simulate universal turing machine different turing machine specifications stored local memory processor simply state table state simulation performed basic operations simultaneously course simulation inefficient difficult program advantage mimd simd processor state simulated machine now let consider alternative idea simd processor simulate individual stored program computer using simple instruction set step simulation simd system execute possible instruction subset processors whose next instruction match typical assembly language even reduced instruction set processors time however set instructions implemented virtual processor small approach case genetic programming instruction set composed specified set functions designed task will show step simply adding conditional instruction get effective mimd simulation running
study effects group formation evolutionary search
information processing pathways experiments results
theoretical modeling superscalar processor performance current simulation approach determine superscalar processor performance widely used shortcomings modern generate extremely long traces resulting problems data storage long simulation run times simulation generally provide significant insight factors determine performance characterization interactions paper proposes theoretical model superscalar processor performance addresses shortcomings performance viewed interaction program parallelism machine parallelism program machine decomposed multiple component functions methods measuring computing functions described functions combined provide model interaction program machine accurate estimate performance computed performance based model compared simulated performance six suite several configurations instruction set architecture
prediction human donor acceptor sites dna sequence artificial neural networks applied prediction splice site location human joint prediction scheme prediction transition regions introns exons level splice site assignment able predict splice site locations confidence levels far better previously reported literature problem predicting donor acceptor sites human genes presence numerous amounts false paper distribution false splice sites examined possible mechanism presented method true donor acceptor sites makes less false donor site assignments less false acceptor site assignments large data set used study means average one false donor sites per true donor site six false acceptor sites per true acceptor site joint assignment method true donor sites around one true acceptor sites detected without false positive predictions highly splice sites isolated widely used weight matrix method separate splice site networks complementary relation confidence levels separate splice site networks observed many weak splice sites transitions signal many stronger splice sites transitions coding noncoding
automated mapping plans plan recognition coordinate agents environment agent needs models agents trying communication impossible expensive information must acquired via plan recognition typical approaches plan recognition start specification possible plans agents may following develop special techniques among possibilities perhaps desirable uniform procedure mapping plans general structures supporting inference based uncertain incomplete observations paper describe set methods plans represented flexible procedural language observation models represented belief networks
local splits binary tree infer splits via one inference dimacs technical report dimacs university university research bell laboratories dimacs nsf science technology center contract also receives support new science technology
constructing trees short sequences construction evolutionary trees fundamental problem biology yet methods evolutionary trees reliable comes accurate topologies large evolutionary trees realistic length sequences address problem present new polynomial time algorithm evolutionary trees called short method consistent greater statistical power polynomial time methods approximation algorithm variant algorithm nearest tree problem study indicates method will produce correct topology sequences guaranteed using methods
artificial life theoretical biology real science computer simulation artificial life alife research offers among things new style computer simulation understanding biological systems processes current alife work show enough methodological good theoretical biology first step towards developing stronger methodology alife paper identifies methodological arising computer science alife suggests methodological heuristics alife theoretical biology notes strengths alife methods versus previous research methods biology examines open questions theoretical biology may benefit alife simulation argues strong alife relevant utility theoretical biology introduction simulating way
approximation subspaces complete characterization given closed subspaces provide specified approximation order space principal generated single function characterization terms fourier transform special case obtain classical conditions without requiring generating function decay approximation order general closed space shown already realized principal
learning model bias minimum number examples learn single task paper problem learning appropriate bias addressed shown achieved learning many related tasks domain theorem given bounding number tasks must theorem tasks known possess common internal representation preprocessing number examples required per task good generalisation learning tasks simultaneously scales like support theoretical results reported
probabilistic principal component analysis principal component analysis pca technique data analysis processing one based upon probability model paper demonstrate principal set observed data vectors may determined estimation parameters latent variable model closely related factor analysis consider properties associated likelihood function giving algorithm estimating principal iteratively discuss advantages definition probability density function pca
modeling belief dynamic systems part revision update study belief change active area recent years two special cases belief change belief revision belief update studied detail paper introduce new framework model belief change framework combines temporal notion plausibility allowing examine change beliefs time paper show belief revision belief update captured framework allows compare assumptions made method better understand principles underlying particular shows notion belief update depends several strong assumptions may limit applicability artificial intelligence finally analysis allow identify notion minimal change broad range belief change operations including revision update work done authors research center first author also much work done support work also supported part air force office scientific research contract grant nsf grants first author also supported part graduate fellowship science center preliminary version paper appears eds principles knowledge representation reasoning proc international conference title knowledgebased framework belief change part revision update
differential evolution simple efficient adaptive scheme global optimization continuous spaces new heuristic approach minimizing possibly nonlinear non continuous space functions presented means extensive includes functions will demonstrated new method converges faster adaptive simulated annealing annealed approach powerful new method requires control variables robust easy use parallel computation
stimulus specificity perceptual learning consequence experiments also stimulus specific keywords
bias temporaldifference learning present results three new algorithms setting parameters temporaldifference learning methods overall task learning predict outcome unknown markov chain based repeated observations state trajectories new algorithms select parameters online way eliminate bias normally inherent temporaldifference methods compare algorithms conventional monte carlo methods monte carlo methods natural way setting step size state use step size number times state close achieving comparable algorithms one new algorithm uses schedule achieve effect processing state td0 remains completely incremental another algorithm uses time equal estimated transition probability current transition present empirical results showing improvement convergence rate monte carlo methods conventional limitation results present apply tasks whose state trajectories contain cycles
gas identification system using temperature sensor neural net interpretation
model cells simulations cell behavior
testing non musical higher order signal contain information non gaussian non linear properties system created since non musical signal usually signal linear spectral characteristics spectral information higher order statistical properties residual signal estimated input signal obtained inverse filtering sound current paper show values residual used characterization important sound properties families strings parameter shown closely related function calculated original signal interpretation statistical test signal linear non gaussian model results compared tests non time series exhibit similar classification results finally regarding higher order statistics signal feature vector statistical distance measure space suggested
case retrieval nets distributed processing paper discuss two approaches applying memory model case retrieval nets applications distributed processing information required distinguish two types applications namely case distributed case libraries case distributed cases solution former straightforward latter requires extension case retrieval nets provides kind partitioning entire net structure extended model even allows concurrent implementation retrieval process use agents retrieval keywords casebased reasoning case retrieval memory structures distributed processing
structures document reuse document important problemsolving task wide variety design task requiring complex adaptation case reuse paper proposes framework document reuse based explicit representation structure underlying documents explicit representation structure facilitates interpretation previous documents explain construction documents document issue specifications rapidly retrieve documents similar structure documents
hierarchical latent variable model data visualization visualization proven powerful tool analysis interpretation multivariate data visualization algorithms aim find projection data space twodimensional visualization space however complex data sets living highdimensional space single twodimensional projection interesting structure therefore introduce hierarchical visualization algorithm allows complete data set level clusters data points levels algorithm based hierarchical mixture latent variable models whose parameters estimated using expectationmaximization algorithm demonstrate principle approach data set apply algorithm visualization synthetic data set dimensions obtained simulation pipelines data dimensions derived images software implementation algorithm available web
differential evolution function optimization differential evolution recently proven assumed otherwise generates new parameter vectors adding weighted difference two population vectors third vector resulting vector yields lower objective function value population generated vector vector compared next generation otherwise old vector basic principle however extended comes practical variants example existing vector adding one weighted difference vector cases also parameters old vector one comparing objective function values several variants proven useful will described
applying ilp structure present novel application ilp problem structure compounds low molecular weight based significant chemical commercial interest use lead compounds search new structure based usually done human experts specialized background knowledge peak patterns chemical structures process assigned number corresponds proper place classified one possible types address problem learning classification rules database peak patterns known structure recently propositional learning successfully applied learn classification rules assigned numbers assignment numbers difficult process possibly classification process apply ilp relational learning problem classifying without assigned numbers
naive bayesian learning adapted
intelligent model selection hillclimbing search design models physical systems differ according computational cost accuracy precision among things depending problem solving task hand different models will appropriate several recently developed methods automatically selecting among multiple models physical systems research novel developing model selection techniques specifically suited design approach based idea performance models design chosen light design decisions required support developed technique called gradient magnitude model selection principle operates context hillclimbing search process selects simplest model needs hillclimbing algorithm operates using domain design research implemented used hillclimbing search decide computationally expensive program algebraic approximation analyze performance experimental tests show makes design process faster expensive model used design evaluations achieves performance improvement little quality resulting design
fast pruning using principal components present new algorithm eliminating parameters improving network generalization supervised training method principal components pruning based principal component analysis node activations successive layers network simple implement effective requires network involve calculating full cost function weight node activity correlation matrices layer nodes required demonstrate method regression problem using polynomial basis functions time series prediction problem using feedforward network
intelligent search defined design spaces numerical optimization complex engineering designs offers promise rapidly producing better designs however methods generally assume objective function constraint functions continuous smooth defined unfortunately realistic tend assumptions present rulebased technique computing presence show gradient computation method used part numerical optimization system tested resulting system domain conceptual design aircraft found using rulebased decrease cost design space search one orders magnitude
learning rules casebased iterative design seen casebased reasoning system first step casebased design systems select initial prototype database previous designs retrieved prototype modified given goals particular design goal selection starting point design process effect quality design overall design time present technique automatically constructing effective rules technique applies standard algorithm c45 set training data describing particular prototype best choice goal encountered previous design session tested technique domain design comparing learned selection rules several competing methods results show inductive method leads better final designs design process guided noisy evaluation function learned rules will often efficient competing methods many automated design systems begin retrieving initial prototype library previous designs using given design goal index guide retrieval process retrieved prototype modified set design modification operators selected design given goals many cases quality competing designs using evaluation functions cases process often research numerous discussions members project thank crossvalidation code john comments previous draft paper research supported grant context casebased design systems choice initial prototype affect quality final design computational cost obtaining design three reasons first prototype selection may impact quality prototypes lie search spaces particular systems design modification operators prototype prototype choice initial prototype will restrict set possible designs obtained search process poor choice initial prototype may therefore lead suboptimal final design second prototype selection may impact quality design process guided nonlinear evaluation function unknown global properties since known method guaranteed find global optimum arbitrary nonlinear function design systems rely iterative local search methods whose results sensitive initial starting point finally choice prototype may impact time needed design modification different starting points may yield final design take different amounts time get design problems evaluating even just single design take amounts time selecting appropriate initial prototype determining factor success failure design process paper describes application inductive learning form rules selecting appropriate prototype designs paper structured follows section describe inductive method learning rules section describe domain design tested methods describe experiments
combining data mining machine learning effective user paper describes automatic design methods detecting behavior much design accomplished using series machine learning methods particular combine data mining constructive induction standard machine learning techniques design methods detecting cellular based behavior specifically use rule learning program indicators behavior large database cellular calls indicators used create serve features system combines evidence multiple generate experiments indicate automatic approach performs nearly best methods detecting
fitness causes mutation comparison runs without fitness selection pressure prices theorem shows solutions grow size caused fitness based selection argue inherent using fixed evaluation function discrete variable length representation simple static evaluation search converges mainly finding trial solutions fitness existing trial solutions general variable length allows many long representations given solution short ones thus search without length bias expect longer representations occur often representation length tend increase fitness based selection leads
learning classify sensor data inductive bias supervised bayesian learning minimum description length
learning polynomial functions feature construction present method learning higherorder polynomial functions examples using linear regression feature construction regression used set training instances produce weight vector linear function feature set hypothesis imperfect new feature constructed product two features effectively predict squared error current hypothesis algorithm repeated extension method specific pair features combine selected measuring joint ability predict hypothesis error
bayesian experimental design review non bayesian experimental design linear models reviewed recent book reviewed non bayesian design nonlinear models bayesian design linear nonlinear models reviewed argue design problem best considered decision problem best solved maximizing expected utility experiment paper considers marginal way appropriate theory non bayesian design
class discretization present methodology enables use classification algorithms regression tasks implement method system regression problem classification one uses classification system solve new problem transformation consists mapping continuous variable ordinal variable grouping values appropriate set intervals use misclassification costs means reflect implicit ordering among ordinal values new variable describe set alternative discretization methods based experimental results need approach choose best method experimental results confirm validity approach class discretization accuracy benefits adding misclassification costs
nonparametric bayesian approach modelling nonlinear time series bayesian multivariate adaptive regression spline methodology extended cope nonlinear time series financial datasets nonlinear time series model closely related adaptive spline threshold autoregressive method financial models thought bayesian versions simple autoregressive conditional models
evolving coordination genetic programming problems solved multiagent teams using genetic programming produce teams one faces several design decisions first questions team diversity strategy one commonly used scheme teams consist single individuals individuals normal way form teams fitness evaluation contrast teams also consist distinct individuals case one either allow free members different teams one restrict various ways second design decision concerns types mechanisms provided individual team members range sensors various complex communication systems paper examines three strategies free restricted three coordination mechanisms sensing sensing evolving teams agents world simple environment among conclusions fact simple form restricted outperforms free teams distinct individuals fact sensing consistently outperforms sensing
numerical
fast simple algorithms perfect phylogeny graphs paper presents algorithm determining whether set species perfect phylogeny number used describe species maximum number states character perfect phylogeny algorithm leads algorithm graph
runtime versus instruction scheduling superscalar processors performance tradeoffs development processors increasing degrees instruction level parallelism ilp order full potential processors multiple instructions must executed single cycle consequently instruction scheduling plays crucial role optimization context early attempts instruction scheduling limited approaches current aimed providing dynamic support hardware paper present results detailed comparative study performance advantages derived spectrum instruction scheduling approaches limited compiler novel hardware significant experimental study via simulations devoted understanding performance advantages runtime scheduling results indicate effective extracting ilp inherent program trace wide range machine program parameters furthermore also show effectiveness enhanced simple scheduler compiler optimizes presence runtime scheduler target current designed take advantage feature demonstrate fact presenting novel scheduling algorithm sensitive lookahead hardware target processor proceedings third international conference high performance computing
multiple scales interactions images excellent book written scientific accuracy linear method used subject research resolutions lack accuracy experimental paradigm used eeg complementary studies images excellent introduction research cognitive science written illustrated presents concepts manner suited technical student many people involved research agree importance recognizing emergent properties brain function neurons clear sparse references book intended review broad field scientific development must expected proposed many cognitive mechanisms study tools yet developed yield better resolutions
universal formulas treatment effects data paper formulas used bound actual treatment effect experimental study treatment assignment random subject imperfect formulas provide bounds average treatment effect inferred given distribution assignments responses results even high rates experimental data yield significant sometimes accurate information effect treatment population
exploration machine learning researchers machine learning built learning systems assumption external work learning experiences recently however several machine learning designed systems play active role choosing situations will learn activity generally called exploration paper describes exploratory learning projects reported literature attempts extract general account issues involved exploration
learning dnf study learnability dnf formulas boolean formulas disjunctive normal form dnf maximum number variable bounded number terms satisfied assignment investigation class dnf formulas present algorithm unknown dnf formula learned high probability finds equivalent dnf formula using equivalence membership queries algorithm runs polynomial time log log log number input variables
extraction facial features recognition using neural networks
bayesian design normal linear model unknown error variance bayesian theory optimal experimental design normal linear model developed assumption variance known special cases specific design criteria specific prior assumptions variance demonstrated general result show way bayesian optimal designs affected prior information variance paper important distinction expected utility functions optimality criteria examines number expected utility functions possess interesting properties wider use derives relevant bayesian optimality criteria normal assumptions unifying useful proving main result paper issue designing normal linear model unknown variance
scheduling mapping software presence structural hazards proposed formulation recently software methods based ilp integer linear programming framework successfully applied derive schedules architectures involving pipelines pipelines without structural hazards problem architectures beyond pipelines remains open one challenge unified ilp framework simultaneously represent resource constraints pipelines assignment mapping operations loop pipelines paper provide framework exactly addition constructs software schedules
models temporally abstract planning planning learning multiple levels temporal abstraction key problem artificial intelligence paper summarize approach problem based mathematical framework markov decision processes reinforcement learning current modelbased reinforcement learning based models represent actions lunch object paper generalizes prior work temporally abstract models sutton extends prediction setting include actions control planning introduce general form temporally abstract model model establish planning learning relationship bellman equations paper summarizes theoretical framework models illustrates potential advantages need hierarchical abstract planning fundamental problem see hinton modelbased reinforcement learning offers possible solution problem integrating planning realtime learning decisionmaking williams sutton however current modelbased reinforcement learning based models represent actions modeling actions requires ability handle different levels temporal abstraction new approach modeling multiple time scales introduced sutton based prior work sutton approach enables models environment different temporal scales producing temporally abstract models however work concerned predicting environment paper summarizes extension approach including actions control environment sutton particular generalize usual notion planning task
evaluation casebased classifiers paper proposes generalisation capabilities casebased reasoning system evaluated comparison algorithm uses simple generalisation strategy two algorithms defined expressions classification accuracy derived function size training sample series experiments using artificial natural data sets described learning curve casebased learner compared trivial learning algorithms results show number plausible situations learning curves simple casebased learner majority distinguished although domain demonstrated performance casebased learner observed suggests casebased reasoning similar problems similar solutions may useful basis generalisation strategy selected domains
cellular encoding interactive evolutionary robotics research robotics programming two direct hand approach uses explicit model behavioral model subsumption architecture machine learning community uses neural network andor genetic algorithm claim hand programming learning complementary two approaches used together orders magnitude powerful approach taken separately propose method combine includes three concepts syntactic constraints restrict search space problem decomposition hand given fitness use method solve complex problem needs less evaluations compared genetic algorithm used alone
rates convergence metropolis algorithms apply recent results markov chain theory metropolis algorithms either independent symmetric candidate distributions provide necessary sufficient conditions algorithms converge geometric rate distribution independence case indicate geometric convergence essentially occurs candidate density bounded multiple symmetric case show geometric convergence essentially occurs geometric also evaluate recently developed computable bounds rates convergence context examples show theoretical bounds inherently extremely although chain stochastically monotone bounds may effective
gas sensors neural networks
cognitive computation extended abstract cognitive computation discussed links together cognitive psychology artificial intelligence
worst case prediction sequences log loss consider game probabilities future data based past observations logarithmic loss making probabilistic assumptions generation data consider situation player tries minimize loss relative loss best distribution target class worst sequence data give bounds minimax terms metric target class respect suitable distributions
pac analyses similarity learning algorithm simple instancebased learning algorithm weighted similarity measure cases paper presents pac analysis motivated pac learning framework demonstrates two main ideas relevant study instancebased learners firstly hypothesis spaces learner different target concepts compared predict difficulty target concepts learner secondly helpful consider constituent parts instancebased learner explore separately many examples needed infer good similarity measure many examples needed case base applying approaches show learns quickly variables representation irrelevant target concept relevant variables paper overall behaviour behaviour constituent parts
learning iterative bootstrap induction extended abstract paper concerned problem inducing recursive horn clauses small sets training examples method iterative bootstrap induction presented first step system generates simple clauses properties required definition properties represent generalizations positive examples simulating effect larger number examples properties used subsequently induce required recursive definitions paper describes method together series experiments results support thesis iterative bootstrap induction indeed effective technique general use ilp
wavelet shrinkage considerable effort directed recently develop asymptotically minimax methods problems recovering objects curves densities spectral densities images noisy data rich complex body work evolved nearly exactly minimax estimators obtained variety interesting problems unfortunately results often practice variety reasons sometimes similarity known methods sometimes computational sometimes lack spatial discuss method curve estimation based noisy data one translates empirical wavelet coefficients towards amount method different methods common use today computationally practical spatially adaptive thus avoids number previous minimax estimators time method nearly minimax wide variety loss functions error global error measured global error estimation derivatives wide range smoothness classes including standard classes classes bounded variation much previously proposed minimax literature finally theory underlying method interesting exploits correspondence statistical questions questions optimal complexity acknowledgements results described december annual january work supported nsf authors like thank organized supporting authors like thank personal correspondence
group despite just subjects study strongly supports identification causal effects figure figure show prior distribution follows flat prior prior respectively figure figure show posterior distribution obtained system run data using flat prior prior respectively bounds pearl follows assumption figure prior posterior distributions specified counterfactual query improved taken drug given improve without corresponds flat prior prior paper identifies demonstrates new application area inference techniques management causal analysis experimentation techniques originally developed medical diagnosis shown capable one major problems experiments assessment treatment face imperfect standard diagnosis involves purely probabilistic inference fully specified networks causal analysis involves partially specified networks links given causal interpretation domain variables unknown system presented paper provides research community believe first time unbiased assessment average treatment effect offer system practical tool used full data available insufficient queries interest research program research primary trial results parts journal medical association january
foundation structural equation models give causal interpretation abstract assumptions underlying statistical estimation different character causal assumptions structural equation models differences years lack mathematical capable distinguishing causal relationships recent advances graphical methods provide formal differences impact practice
incremental class learning approach application handwritten digit recognition incremental class learning icl provides feasible framework development learning systems instead learning complex problem icl focuses learning incrementally one time using results prior learning subsequent learning combining solutions appropriate manner respect multiclass classification problems icl approach presented paper follows initially system focuses one category learns category tries identify compact subset features nodes hidden layers crucial recognition category system crucial nodes features weights result features subsequent learning features available subsequent learning serve parts weight structures build recognize categories categories learned set features gradually learning new category requires less effort learning new category may involve combining existing features appropriate manner approach sharing learned features among number categories also wellknown interference problem present results applying icl approach handwritten digit recognition problem based spatiotemporal representation patterns
speculative speculation profile variations code performance presence execution scheduling methods trace scheduling scheduling use speculation extract parallelism programs methods predict important execution paths current scheduling scope using execution frequency estimation speculation applied important execution paths possibly cost performance along paths therefore speed output code sensitive ability accurately predict important execution paths prior work area utilized speculative yield function fisher coupled dependence instruction priority among execution paths scheduling scope technique provides stability performance attention needs paths directly address problem prediction runtime behavior work presented paper extends speculative yield dependence heuristic explicitly minimize penalty paths instructions along path since execution time path determined number cycles paths scheduling scope heuristic attempts eliminate speculation paths control speculation makes performance much less sensitive actual path taken run time proposed method strong emphasis achieving minimal thus speculative used paper presents speculative heuristic shows controls scheduler stability ieee published proceedings annual international symposium december personal use material however permission material purposes creating new collective works lists reuse component work works must obtained ieee ieee service center box
efficient inference bayes networks combinatorial optimization problem number exact algorithms developed perform probabilistic inference bayesian belief networks recent years techniques used algorithms closely related network structures easy understand implement paper consider problem combinatorial optimization point view state efficient probabilistic inference belief network problem finding optimal factoring given set probability distributions viewpoint previously developed algorithms seen alternate factoring strategies paper define combinatorial optimization problem optimal factoring problem discuss application problem belief networks show optimal factoring provides insight key elements efficient probabilistic inference demonstrate simple easily implemented algorithms excellent performance
networks develop teaching input backpropagation learning hinton williams useful research tool number features decide learned describe number simulations neural networks generate teaching input networks generate teaching input network input connection weights evolved using form genetic algorithm results evolved capacity behave efficiently environment learn behave efficiently analysis networks evolve learn shows interesting results
probabilistic evaluation counterfactual queries appear national conference artificial intelligence seattle august technical report abstract evaluation counterfactual queries true true important fault diagnosis planning determination present formalism uses probabilistic causal networks evaluate ones belief counterfactual true true query interpreted external action forces true consistent analysis formalism offers concrete world approach properly common understanding causal influences deals uncertainties inherent world machine representation
policy analysis structural models evaluation counterfactual queries true true important fault diagnosis planning determination policy analysis present method evaluating underlying causal model represented structural models nonlinear generalization simultaneous equations models commonly used social sciences new method provides coherent means evaluating policies involving control variables prior policy influenced variables system
malicious membership queries exceptions
theory refinement bayesian networks theory refinement task updating domain theory light new cases done automatically expert problem theory refinement uncertainty reviewed context bayesian statistics theory belief revision problem reduced incremental learning task follows learning system initially partial theory supplied domain expert maintains internal representation alternative theories able domain expert able incrementally refined data algorithms refinement bayesian networks presented illustrate partial theory alternative theory representation etc algorithms incremental variant batch learning algorithms literature work batch incremental mode
behavior due individual energy extracting abilities emergence behavior populations neural networks studied energy extracting ability included property artificial life simulations organisms living environment fitness score interpreted combination organisms behavior ability extract energy potential sources distributed environment energy extracting ability viewed evolvable organisms particular organisms mechanisms extracting energy environment therefore fixed simulations fixed evolvable energy extracting abilities show energy extracting mechanism sensory behavior organisms may results suggest populations organisms evolve due individual energy extracting abilities
modelbased learning application robot navigation
theory paper consider problem theory given domain theory whose components possibly set labeled training examples domain concept theory problem revise components theory resulting theory correctly training examples theory thus type theory revision revisions made individual components theory paper determine classes logical domain theories theory problem tractable consider propositional firstorder domain theories show theory problem equivalent determining information contained theory stable regardless revisions might performed theory show determining stability tractable input theory satisfies two conditions revisions theory component monotonic effects classification examples theory components act independently classification examples theory also show concepts introduced used determine particular theory algorithms
adapting control strategies autonomous agents paper studies balance evolutionary design human expertise order best design autonomous agents learn specific tasks genetic algorithm designs control circuits learn simple behaviors given control strategies simple behaviors genetic algorithm designs circuit simple behaviors perform navigation task keywords genetic algorithms computational design autonomous agents robotics
role trainer reinforcement learning paper propose incremental approach development autonomous agents discuss issues characteristics reinforcement programs define trainer particular kind present set results obtained running experiments trainer provides guidance autonomous robot
aspects building block hypothesis genetic programming paper carefully formulate schema theorem genetic programming using schema definition accounts variable length nature representation manner similar early research use interpretations schema theorem obtain building block definition state classical building block hypothesis searches combining building blocks report approach several reasons difficult find support combination building blocks solely interpretation schema theorem even support empirically whether building blocks always exist partial solutions consistently average fitness also account search behavior
analytical framework local feedforward networks although feedforward neural networks suited function approximation applications networks experience problems learning desired function one problem interference occurs learning one area input space causes another area networks less interference spatially local networks understand properties theoretical framework consisting measure interference measure network localization developed incorporates network weights architecture also learning algorithm using framework analyze sigmoidal multilayer perceptron mlp networks employ learning algorithm address familiar sigmoidal networks inherently demonstrating given sufficiently large number parameters sigmoidal made arbitrarily local ability represent continuous function compact domain
analyzing social network structures iterated choice university department computer sciences technical report abstract iterated choice extension iterated evolution allows choose game individual behaviors behavioral population structures emerge report examine one particular environment document social network methods used identify population behaviors found within complex adaptive system contrast standard homogeneous population also found populations mixed strategies within environment particular social networks interesting populations evolution examined
statistical mechanics nonlinear financial markets applications optimized trading paradigm statistical mechanics financial markets using nonlinear algorithms first published mathematical modelling fit multivariate financial markets using adaptive simulated annealing global optimization algorithm perform maximum likelihood defined path multivariate conditional probabilities canonical thereby derived used technical indicators recursive optimization process tune trading rules trading rules used outofsample data demonstrate model illustrate markets likely efficient
scaling reinforcement learning algorithms learning variable temporal resolution models close connection reinforcement learning algorithms dynamic programming algorithms research within machine learning community yet despite increased theoretical understanding algorithms applicable simple tasks paper use abstract framework connection dynamic programming discuss scaling issues researchers focus learning agents learn solve multiple structured tasks environment propose learning abstract environment models abstract actions represent achieving particular state models variable temporal resolution models different parts state space abstract actions different number time steps definitions abstract actions learned incrementally using repeated experience solving tasks prove certain conditions solutions new tasks found using experience abstract actions alone
decision trees graphs topdown pruning describe supervised learning algorithm uses mutual information build decision tree tree readonce decision graph merging nodes level tree domains appropriate decision trees performance approximately c45 number nodes much smaller merging phase decision tree provides new way dealing problem new pruning mechanism works starting root pruning mechanism suited finding recovering splits irrelevant features may happen tree construction
statistical mechanics interactions eeg relations approach explicitly formulated local global theory investigate determine source information processing nature basis statistical theory interactions success properties capacity scales interactions consistent theory deriving similar relations scales eeg activity received march project supported entirely personal contributions physical studies institute university california san physical studies institute agency account institute pure applied physical sciences
weakly learning dnf characterizing statistical query learning using fourier analysis present new results positive negative problem learning disjunctive normal form dnf expressions first prove algorithm due used weakly learn dnf using membership queries polynomial time respect uniform distribution inputs first positive result learning dnf expressions polynomial time nontrivial formal model learning provides contrast results proved efficiently learnable model given certain plausible assumptions also present efficient learning algorithms various models dnf negative results turn attention recently introduced statistical query model learning model restricted version popular probably approximately correct pac model every class known efficiently learnable pac model fact learnable statistical query model give general characterization complexity statistical query learning terms number functions concept class upper lower bounds number statistical queries required learning input distribution obtain dnf expressions decision trees even weakly learnable research sponsored part laboratory systems center air force advanced research projects agency grant number support also sponsored national science foundation grant also supported part nsf national young grant views conclusions contained document authors interpreted necessarily representing policies either expressed implied laboratory states government nsf respect uniform input distribution polynomial time statistical query model result informationtheoretic therefore rely assumptions demonstrates simple modification existing algorithms computational learning theory literature learning various restricted forms dnf decision trees random examples also several algorithms proposed experimental machine learning id3 algorithm decision trees variants will solve general problem unifying tool results fourier analysis finite class boolean functions
models temporally planning abstract planning learning multiple levels temporal abstraction key problem artificial intelligence paper summarize approach problem based mathematical framework markov decision processes reinforcement learning current modelbased reinforcement learning based models represent actions lunch object paper generalizes prior work temporally abstract models sutton extends prediction setting include actions control planning introduce general form temporally abstract model model establish planning learning relationship bellman equations paper summarizes theoretical framework models illustrates potential need hierarchical abstract planning fundamental problem see hinton modelbased reinforcement learning offers possible solution problem integrating planning realtime learning decisionmaking williams sutton press however current modelbased reinforcement learning based models represent actions modeling actions requires ability handle different levels temporal abstraction new approach modeling multiple time scales introduced sutton based prior work sutton approach enables models environment different temporal scales producing temporally abstract models however work concerned predicting environment paper summarizes planning task
architecture goaldriven explanation complex changing environments explanation must dynamic goaldriven process paper discusses evolving system implementing novel model explanation generation goaldriven interactive explanation models explanation goaldriven multistrategy process reasoning action describe preliminary implementation model system generates explanations internal use support plan generation execution
sites needed accurately large evolutionary trees dimacs technical report
robust stabilization general asymptotically systems shown recently asymptotically system means certain type feedback feedback laws constructed work robust respect errors perturbations system dynamics however may highly sensitive errors measurement state vector paper addresses shows design dynamic hybrid controller preserving robustness external perturbations error also robust respect measurement error new design relies upon controller incorporates internal model system driven previously constructed feedback
universal construction theorem nonlinear stabilization report abstract note presents explicit proof theorem due states existence smooth function implies smooth result extended rational cases proof uses universal formula given algebraic function lie derivatives formula solution simple equation
improving software memory reuse analysis
stage scheduling technique reduce register requirements modulo schedule modulo scheduling efficient technique exploiting instruction level parallelism variety loops resulting high performance code increased register requirements present set low computational complexity heuristics reduce register requirements given modulo schedule operations cycles measurements benchmark suite loops perfect shows best heuristic achieves average decrease register requirements obtained optimal stage scheduler
minimizing register requirements modulo schedule via optimum stage scheduling modulo scheduling efficient technique exploiting instruction level parallelism variety loops resulting high performance code increased register requirements present approach schedules loop operations minimum register requirements given modulo table method determines optimal register requirements machines finite resources general dependence graphs measurements benchmark suite loops perfect show register requirements decrease average applying optimal stage scheduler modulo scheduler
array processor rap software architecture design implementation software array processor rap high performance parallel computer involved development three hardware running realtime operating system rap now runs based system using flexible set tools provided rap user primary emphasis improving efficiency artificial neural network algorithms done providing library assembly language routines use compilation objectoriented rap interface provided allows incorporate rap computational server applications program built provides interactive style rap manipulation
robust feature selection algorithms selecting set features optimal given task problem plays important role wide variety contexts including pattern recognition adaptive control machine learning experience traditional feature selection algorithms domain machine learning lead computational efficiency paper describes alternate approach feature selection uses genetic algorithms primary search component results presented suggest genetic algorithms used increase robustness feature selection algorithms without significant decrease computational efficiency
growing neural networks
minimizing register requirements software paper address following software problem given loop machine architecture fixed number processor resources function units one construct schedule runs given architecture maximum possible iteration rate minimizing number main contributions paper first demonstrate problem described simple mathematical formulation precise optimization objectives linear scheduling framework mathematical formulation provides clear permits one overall solution space schedules different sets secondly show precise mathematical formulation solution make significant performance difference evaluated performance method three leading heuristic methods scheduling modified list scheduling experimental results show method described paper performed significantly better methods
learning decision making approach comparative study machine learning inference laboratory paper concerns issue best form learning representing using knowledge decision making proposed answer knowledge learned represented declarative form needed decision making efficiently procedural form specific decision making situation approach combines advantages declarative representation facilitates learning incremental knowledge modification procedural representation facilitates use knowledge decision making approach also allows one determine decision structures may avoid attributes difficult measure given situation experimental system demonstrated decision structures obtained via declarative often higher predictive accuracy also simpler learned directly facts
effects occams razor evolving neural nets several evolutionary algorithms make use hierarchical representations variable size rather linear strings fixed length variable complexity structures provides additional representational power may application domain evolutionary algorithms price however search space solutions may grow arbitrarily large size paper study effects structural complexity solutions generalization performance analyzing fitness landscape neural networks analysis suggests smaller networks achieve average better generalization accuracy larger ones thus usefulness occams razor simple method implementing occams razor principle described shown effective ing generalization accuracy without limiting learning capacity
mlc tutorial machine learning library classes
incremental interactive algorithm regular grammar inference present interactive algorithms learning regular grammars positive examples membership queries complete set strings language corresponding unknown regular grammar implicitly specifies lattice version space represents space candidate grammars containing unknown grammar lattice efficiently using membership queries identify unknown grammar using implicit representation version space form two sets correspond respectively set specific general grammars consistent set positive examples provided queries teacher given time present provably correct incremental version algorithm complete set positive samples necessarily available learner beginning learning learner constructs lattice grammars based strings provided start performs candidate elimination membership queries additional examples become available learner incrementally updates lattice candidate elimination set positive samples provided teacher complete set unknown grammar algorithm identifying unknown grammar
position paper workshop evolutionary computation variable size representation fitness causes argue based upon numbers representations given length increase representation length inherent using fixed evaluation function discrete variable length representation two examples including use prices theorem examples confirm solutions grow size caused fitness based selection
adaptation constant utility nonstationary environments environments vary time present fundamental problem adaptive systems although worst case effective adaptation forms environmental variability provide adaptive opportunities consider broad class nonstationary environments combine variable result function invariant utility function demonstrate via simulation adaptive strategy employing evolution learning tolerate much higher rate environmental variation strategy suggest many cases stability previously assumed constant utility nonstationary environment may fact powerful viewpoint
neural competitive maps reactive adaptive navigation recently introduced neural network reactive avoidance based model classical conditioning article describe success model implemented two real autonomous robots results show promise selforganizing neural networks domain intelligent robotics
evolutionary approach combinatorial optimization problems paper reports application genetic algorithms probabilistic search algorithms based model evolution npcomplete combinatorial optimization problems particular subset sum maximum cut minimum task problems considered except fitness function changes genetic algorithm required order achieve results high quality even problem instances size used paper constrained problems subset sum minimum task constraints taken account incorporating penalty term fitness function even large instances highly multimodal optimization problems iterated application genetic algorithm observed find global optimum within number runs genetic algorithm samples fraction search space results quite encouraging
efficient highlevel parallel programming neural networks systems analysis modelling programming language designed dynamic neural network learning algorithms provides flexibility generalpurpose languages expressive allows much programs particular algorithms change network topology dynamically constructive algorithms pruning algorithms contrast languages programs efficient code parallel machines without changes source program thus providing easy start using parallel article analyzes circumstances approach useful one presents description language constructs reports performance results symmetric concludes many cases good basis neural learning algorithm research parallel machines
university design strategies evolutionary robotics
genetic local search approach quadratic assignment problem genetic algorithms local search heuristics promising approach solution combinatorial optimization problems paper genetic local search approach quadratic assignment problem presented new genetic operators approach described performance tested various instances containing results indicate proposed algorithm able high quality solutions relatively short time limit largest known instance new best solution found
hard genetic programming simulated annealing hill climbing performance shown problem programming artificial follow santa used example program search space analysis solutions shows many characteristics often coded programs small fraction total search space random sampling many multiple split many local global optima suggests difficult hill climbing algorithms analysis program search space terms fixed length schema suggests highly simplest solutions large building blocks must average fitness cases show solutions using fixed representation small building blocks average fitness suggest problem difficult genetic algorithms random sampling program search space suggests average density global optima changes program size density networks points fitness grows approximately linearly program length part cause
machine learning research four current directions machine learning research making great progress many directions article summarizes four directions discusses current open problems four directions improving classification accuracy learning ensembles classifiers methods scaling supervised learning algorithms reinforcement learning learning complex stochastic models
extensions fills algorithm perfect simulation fills algorithm perfect simulation attractive finite state space models unbiased user presented terms stochastic recursive sequences extended two ways discrete markov random fields two coding sets like distribution lattice treated monotone systems particular partial ordering states used fills algorithm applies directly combining fills sampling leads version algorithm works general discrete specified models extensions types models briefly discussed
pac adaptive control linear systems consider special case reinforcement learning environment described linear system states environment actions agent perform represented real vectors system dynamic given linear equation stochastic component problem equivalent socalled linear quadratic problem studied optimal adaptive control literature propose learning algorithm problem analyze pac learning framework unlike algorithms adaptive control literature algorithm explores environment learn accurate model system faster show control law produced algorithm high probability value close optimal policy relative magnitude initial state system time taken algorithm polynomial dimension statespace dimension ratio constant
empirical analysis benefit decision tree size biases function results reported empirically show benefit decision tree size biases function concept distribution first shown concept distribution complexity number internal nodes smallest decision tree consistent example space benefit minimum size maximum size decision tree biases second policy described defines learner given knowledge complexity distribution concepts third explanations distribution concepts seen practice minimum size decision tree bias given evaluated empirically
collective memory search collective memory search exploiting information center exploration
analysis sound musical machine means higher order statistical features paper describe sound classification method seems applicable broad domain stationary machine made non method based matching higher order signals generalizes earlier results classification musical higher order statistics efficient matched filter presented results show good sound classification statistics comparison spectral matching methods also discussed
generating declarative language bias topdown ilp algorithms many algorithms inductive logic programming ilp put user declarative bias defined rather fashion address issue developed method generating declarative language bias topdown ilp systems highlevel key feature approach distinction user level expert level language bias expert provides abstract user relationship given database obtain declarative language bias suggested languages allow compact abstract specifications declarative language bias topdown ilp systems using schemata verified several properties algorithm generates schemata applied successfully chemical domains consequence propose use approach generate declarative language bias
behavior near zero distribution smoothing parameter estimates
learning approximate control rules high utility one difficult problems area explanation based learning utility problem learning many rules low utility lead performance paper introduces two new techniques improving utility learned rules first technique combine ebl inductive learning techniques learn better set control rules second technique use inductive techniques learn approximate control rules two techniques synthesized algorithm called approximating abductive explanation based learning shown improve substantially standard ebl several domains
search techniques program discovery address problem program discovery defined genetic programming combining hierarchical crossover operator two traditional single point search algorithms simulated annealing stochastic iterated hill climbing solved problems processing fewer candidate solutions greater probability success genetic programming also enhanced genetic programming simple idea hill climbing individuals fixed interval generations
application clausal discovery temporal databases applications consider databases static objects however many databases inherently temporal store evolution object time thus regularities dynamics databases discovered current state might depend way previous states end preprocessing data needed aimed extracting relationships connected temporal nature data will make available discovery algorithm predicate logic language ilp methods together recent advances makes adequate task
first four years abstract summary progress plans
exponential convergence discrete approximations paper consider time method approximating given distribution using diffusion log find conditions diffusion converges exponentially quickly one dimension essentially distributions exponential form exponential convergence occurs consider conditions discrete approximations diffusion converge first show even diffusion converges naive need consider version algorithm find conditions also converges exponential rate perhaps surprisingly even version need converge exponentially fast even diffusion briefly discuss form algorithm practice avoid difficulties forms
automatic programming agents learn mental models create simple plans action essential component intelligent agent ability encode store utilize information environment traditional approaches program induction focused evolving functional reactive programs paper presents approach automatic generation agents discover information environment encode information later use create simple plans utilizing stored mental models approach agents computer programs shared memory programs representation scheme evolved using genetic programming problem gold collection used demonstrate approach one part program makes map world memory part uses map find gold results indicate approach evolve programs store simple representations environments use representations produce simple plans introduction
reasoning time probability
models reinforcement learning reinforcement learning used predict rewards also predict states learn model worlds dynamics models defined different levels temporal abstraction models models focus predicting will happen rather certain event will take place based models define abstract actions enable planning efficient way various levels abstraction
smoothing spline models correlated random errors
design optimization criteria multiple sequence alignment dimacs technical report january
errorcorrecting output coding bias variance previous research shown technique called errorcorrecting output coding dramatically improve classification accuracy supervised learning algorithms learn classify data points one classes paper presents investigation technique works particularly employed decisiontree learning algorithms shows method like form voting reduce variance learning algorithm methods simply combine multiple runs learning correct errors caused bias learning algorithm experiments show bias correction ability relies c45
simultaneous evolution programs control structures simultaneous evolution programs control
algorithm mixtures factor technical report may revised abstract factor analysis statistical method modeling covariance structure high dimensional data using small number latent variables extended allowing different local factor models different regions input space results model performs clustering dimensionality reduction thought reduced dimension mixture gaussians present exact expectationmaximization algorithm fitting parameters mixture factor
modeling dynamic receptive field changes primary visual cortex using inhibitory learning position size shape visual receptive field primary visual cortical neurons change dynamically response artificial conditioning exin learning rules used model dynamic changes exin model compared adaptation model model role lateral inhibitory learning rules exin simulations done lateral inhibitory learning conditioning exin model without feedforward learning produces expansion initially inside region increased without changes activation exin model without feedforward learning consistent data adaptation model model comparison exin models suggests experiments determine role feedforward excitatory lateral inhibitory learning producing dynamic changes conditioning
bottomup induction logic programs one recursive clause paper present bottomup algorithm called induce logic programs examples method induce programs base clause one recursive clause small number examples based analysis examples first generates path structure expression stream values predicates concept path structure originally introduced used paper introduce concepts extension difference path structure recursive clauses expressed difference path structure extension paper presents algorithm shows experimental results obtained method
advances neural information processing systems gaussian processes regression bayesian analysis neural networks difficult simple prior weights implies complex prior distribution functions paper investigate use gaussian process priors functions permit predictive bayesian analysis fixed values hyperparameters carried exactly using matrix operations two methods using optimization averaging via hybrid monte carlo hyperparameters tested number challenging problems produced excellent results
explaining basis knowledge base refinement explanations play key role detection techniques paper show role limited detection also used automated knowledge base refinement introduce refinement procedure takes small number refinement rules rather test cases explanations constructed attempt cause causes detected process returns rule revisions consistency caused one handled time improves efficiency refinement process
facts necessary requirements artificial evolution complex behaviour paper sets conceptual framework artificial evolution complex behaviour autonomous agents recurrent dynamical neural networks similar used genetic algorithm employs variable length capable evolving arbitrary levels behavioural complexity furthermore simple restrictions encoding scheme develop may guaranteed increase fitness requires increase behavioural complexity will evolve order process design alternative however time periods involved must acceptable final part paper general ways encoding scheme may modified speed process experiments reported different categories scheme tested conclusions promising type encoding scheme able evolutionary robotics
unsupervised neural network control mobile robot noise stability recently introduced neural network mobile robot controller learns forward inverse differential robot unsupervised cycle initial learning phase controller move robot arbitrary stationary moving target noise forms changes robots addition forward map allows robot reach targets absence sensory feedback controller also able adapt response longterm changes robots change article review architecture describe simplified algorithmic implementation present new quantitative results performance noisefree noisy conditions compare performance task performance alternative controller describe preliminary results hardware implementation mobile robot
perfect sampling recurrent markov chains develop algorithm simulating perfect random samples invariant measure recurrent markov chain method uses coupling embedded times works effectively finite chains stochastically monotone chains even continuous spaces paths may upper lower processes examples show naive approaches constructing bounding processes may considerably biased algorithm simplified certain cases make easier run give explicit analytic bounds coupling times stochastically monotone case
exact simulation using markov chains reports gives review new exact simulation algorithms using markov chains first part covers discrete case consider two different algorithms coupling past technique fills sampler algorithms tested model without external field second part covers continuous state spaces present several algorithms developed based coupling past discuss applicability methods bayesian analysis problem failure rates
robust convergence nonlinear algorithms identification
specialization social conditions shared environments behaviors populations artificial neural networks studied genetic algorithm used simulate evolution processes thereby develop neural network control systems exhibit behaviors according fitness formula evolvable fitness formulae evaluation measure let free evolve obtain coevolution expressed behavior individual evolvable fitness formula use evolvable fitness formulae work dynamic fitness landscape opposed work traditionally applies static fitness landscapes role competition specialization studied individuals social conditions shared environment directly find competition act provide population populations organisms individual evolvable fitness formulae
good contextual semantics influences program structure using designed primitive sets investigate relationship expression mechanisms size density genetic program trees evolutionary process show contextual semantics influence location code program detail analyze dynamics discuss impact findings descriptions genetic programming
predicting conditional probability distributions connectionist approach traditional prediction techniques mean probability distribution single point multimodal processes instead predicting mean probability distribution important predict full distribution article presents new connectionist method predict conditional probability distribution response input main idea transform problem regression classification problem conditional probability distribution network perform direct predictions iterated predictions task specific time series problems compare method fuzzy logic discuss important differences also demonstrate architecture two time series first benchmark series used santa competition deterministic chaotic system second time series markov process exhibits structure two time scales network produces multimodal predictions series compare predictions network predictor find conditional probability network likely model
stable ilp exploring added background knowledge present stable ilp concept machine learning reasoning stable models give meaning logic programs containing negative stable ilp employ stable models represent current state specified possibly negative rules state background knowledge topdown ilp learner present framework implementation system one realization stable ilp
decisiontheoretic foundations defaults recent years considerable effort understanding default reasoning effort question entailment conclusions defaults surprisingly works formally examine general role defaults argue role necessary order understand defaults suggest concrete role defaults defaults decisionmaking process allowing make fast approximately optimal decisions certain possible states order approach examine decision making framework decision theory use probability utility measure impact possible states decisionmaking process default states small impact according measure choice measures show resulting defaults satisfies desired properties defaults namely reasoning finally compare approach decisiontheoretic defaults show combined form attractive framework reasoning decisions make numerous assumptions car will start road will will traffic etc many assumptions given sufficient evidence humans naturally state defaults conclusions default information hence defaults seem play important part reasoning use however need formal understanding defaults represent conclusions problem default conclusions great deal attention many researchers attempt find contextfree patterns default reasoning research shows much done approach claim however utility approach limited gain better understanding defaults need understand situations state default main thesis investigation defaults role behavior reasoning agent role allow examine default appropriate terms implications agents overall performance paper suggest particular role defaults show role allows provide semantics defaults course claim role defaults play many applications end result reasoning choice actions usually choice optimal much uncertainty state world effects actions allow possibilities suggest one role defaults lies simplifying decisionmaking process assumptions reduce space examined possibilities precisely suggest default situations knowledge amounts one particular understood light semantics pearl semantics default given knowledge probability small small probability states gives although probability plays important part decisions claim also examine utility actions example people highly will next also believe default assumption context decision whether life context high outcome even though suggest set given based impact decision view assuming get much examine decisionmaking framework decision theory decision theory represents decision problem using several components set possible states probability measure sets utility function assigns action state numerical value appear
density estimation wavelet thresholding density estimation commonly used test case nonparametric estimation methods explore asymptotic properties estimators based thresholding empirical wavelet coefficients minimax rates convergence studied large range besov function classes range global error measures single wavelet threshold estimator asymptotically minimax within logarithmic terms simultaneously range spaces error measures particular form essential since minimax linear estimators suboptimal polynomial second approach using approximation gaussian noise model metric used acknowledgements thank helpful discussions references work theorems used section work supported part nsf second author like thank
using training set learning input whose value used turn approximate empirical studies show good results achieved however several drawbacks training set learners backpropagation typically slow may require many training set also guarantee given arbitrary training set system will find enough good critical features get reasonable approximation moreover number features exponential number inputs becomes computationally expensive finally interesting positive theoretical results suggests difficulty learning without sufficient priori knowledge goal learning systems generalize generalization commonly based set critical features system available training set learners typically extract critical features random set examples approach attractive exponential number features propose extend system priori knowledge form advantages augmented system speedup improved generalization greater paper presents learning algorithm main features include distributed implementation bounded learning execution times ability handle correct incorrect results simulations realworld data demonstrate promise paper presents learning intended overcome weaknesses training set augmented small set pair called example example inputs set special value input whose value effect value output use special value therefore pair containing inputs represents many examples product sizes input domains inputs introduction
incremental learning model reasoning
efficient metric heterogeneous inductive learning applications attributevalue language many inductive learning problems expressed classical attributevalue language order learn generalize learning systems often rely measure similarity current knowledge base new information attributevalue language defines heterogeneous multidimensional input space attributes nominal others linear defining similarity two points input spaces non trivial discuss two representative homogeneous metrics show examples limited domains address issues raised design heterogeneous metric inductive learning systems particular discuss need normalization impact values propose heterogeneous metric evaluate empirically simplified version
learning model sequences generated switching distributions study efficient algorithms solving following problem call switching distributions learning problem sequence finite generated following way sequence runs run generated independent random distribution element set distributions learning algorithm given sequence goal find approximations distributions give approximate segmentation sequence runs give efficient algorithm solving problem show conditions algorithm guaranteed work high probability
connectionist architecture inherent connectionist networks adequate higher level cognitive activities natural language interpretation generalize way appropriate given regularities domain identified important pattern regularities domains called several attempts made show connectionist networks generalize regularities satisfaction address challenge paper implications connectionist solutions variable binding problem based work argue network must generalize information learns one variable binding variable show temporal synchrony variable binding inherently generalizes way thereby show temporal synchrony variable binding connectionist architecture accounts important step showing adequate architecture higher level cognition
heuristic improved genetic university technical report may submitted information processing may
using distance metric genetic programs understand genetic operators describe distance metric called distance syntactic difference two genetic programs context one specific problem bit use metric analyze amount new material introduced different crossover operators difference among best individuals population difference among best individuals population relationships data run performance sufficiently interesting investigation use distance
impact external dependency genetic programming control data dependencies among impact behavioural consistency genetic programming solutions behavioural consistency turn ability genetic programming identify appropriate present results modelling dependency parameterized problem exhibits internal external dependency levels change incorporated larger find key difference full external dependency longer time solution identification lower likelihood success shown increased difficulty identifying correct
parallel island model genetic algorithm multiprocessor scheduling problem paper compare performance serial parallel island model genetic algorithm solving multiprocessor scheduling problem show results using fixed problems using using found addition providing speedup use parallel processing parallel island model finds better quality solutions serial
neural programming internal reinforcement policy important reason artificial neural networks anns machine learning community backpropagation procedure gives anns locally optimal change procedure addition framework understanding ann learning performance genetic programming also successful evolutionary learning technique provides powerful parameterized primitive constructs unlike anns though principled procedure changing parts learned system based current performance paper introduces neural programming connectionist representation evolving programs maintains benefits connectionist model neural programming allows regression procedure evolutionary learning system describe general method feedback mechanism neural programming internal reinforcement introduce internal reinforcement procedure use experiment
topdown induction clustering trees approach clustering presented adapts basic topdown induction decision trees method towards clustering aim employs principles instance based learning resulting methodology implemented induction clustering trees system first order clustering system employs first order logical decision tree representation inductive logic programming system various experiments presented propositional domains
genetic programming draft march available via site
evolutionary learning crossover operator abstract
improved center point selection probabilistic neural networks probabilistic neural networks typically learn quickly many neural network models success variety applications however basic form tend large number hidden nodes one common solution problem keep subset original training data building network paper presents algorithm called reduced probabilistic neural network seeks choose subset available instances use center points nodes network algorithm tends points removing nodes instances regions input space highly homogeneous experiments datasets better average generalization accuracy two models requiring average less number nodes
realtime interactive neuroevolution standard neuroevolution population networks evolved task network best solves task found network fixed used solve future instances problem networks evolved way handle realtime interaction hard evolve solution time cope effectively possible environments might arise future possible ways may interact paper proposes evolving feedforward neural networks online create agents improve performance realtime interaction approach demonstrated game world individuals play humans evolution individuals learn varying appropriately taking account goals initial evaluation offline population allowed evolve online performance improves considerably population adapts novel situations changing strategies opponent game also improves performance situations already seen offline training paper will describe implementation online evolution shows practical method performance offline evolution alone
predicting system artificial neural networks methods results abstract feedforward artificial neural network ann procedure predicting utility present resulting predictions two test problems given great energy predictor first building data analysis prediction competition key approach method ffi test determining relevant inputs multilayer perceptron methods briefly reviewed together comments alternative schemes like fitting use recurrent networks
experimental analysis schema creation propagation genetic programming paper first review main results theory schemata genetic programming new schema theory based new definition schema study creation propagation new form schemata real runs standard crossover crossover selection finally discuss results light schema theorem
radial basis functions process control radial basis function neural networks provide attractive method high dimensional nonparametric estimation use nonlinear control faster train conventional feedforward networks sigmoidal activation networks backpropagation nets provide model structure better suited adaptive control article gives brief survey use introduces new statistical interpretation radial basis functions new method estimating parameters using algorithm new statistical interpretation allows provide confidence limits predictions made using networks
new schema theorem genetic programming crossover point mutation review main results obtained theory schemata genetic programming strengths weaknesses propose new simpler definition concept schema original concept schema genetic algorithms gas along new form crossover crossover point mutation concept schema used derive improved schema theorem describes propagation schemata one generation next discuss result show schema theorem natural schema theorem
optimal asymptotic identification bounded paper investigates intrinsic limitation worstcase identification systems using data corrupted bounded unknown known belong given model set done analyzing optimal worstcase asymptotic error performing experiments using bounded inputs estimating using identification algorithm first shown conditions model set identification algorithm asymptotically optimal input characterization optimal asymptotic error function inputs also obtained results hold error metric norm second general results applied three specific identification problems identification stable systems norm identification stable rational systems norm identification rational systems gap metric problems general characterization optimal asymptotic error used find nearoptimal inputs minimize error
connectionist architecture learning parse present connectionist architecture demonstrate learn syntactic text architecture represent syntactic learn generalizations syntactic thereby addressing sparse data problems previous connectionist architectures apply simple synchrony networks mapping sequences word parse trees training samples networks achieve precision recall approaches statistical methods task
evolutionary computation air traffic control planning air traffic control involved realtime planning aircraft trajectories heavily constrained optimization problem planning aircraft required way points choice proper representation realworld problem nontrivial propose two level representation one level evolutionary operators work derived level calculations furthermore show specific choice fitness function important finding good solutions large problem instances use hybrid approach sense use knowledge air traffic control using number heuristics built prototype planning tool flexible tool generating planning low cost number aircraft
approach problem network design using genetic algorithms
tutorial algorithm application parameter estimation gaussian describe parameter estimation problem expectationmaximization algorithm used solution first describe abstract form algorithm often given literature develop parameter estimation procedure two applications finding parameters mixture gaussian densities finding parameters hidden markov model hmm algorithm discrete gaussian mixture observation models derive update equations fairly explicit detail prove convergence properties try rather mathematical
evolving optimal neural networks using genetic algorithms occams razor genetic algorithms used neural networks two main ways optimize network architecture train weights fixed architecture previous work focuses one two options paper investigates alternative evolutionary approach called genetic programming architecture weights optimized simultaneously genotype network represented tree whose depth width dynamically adapted particular application specifically defined genetic operators weights trained hillclimbing search new fitness function proposed principle occams razor makes optimal tradeoff error fitting ability network simulation results two benchmark problems complexity suggest method finds minimal size networks data experiments noisy data show using occams razor improves generalization performance also convergence speed evolution published complex systems
artificial neural network computations synthetic perceptron fully single designed efficient execution artificial neural network algorithms first implementation will technology rate prototype system designed within will connections per second pattern classification around connection updates per second running popular error backpropagation training algorithm represents speedup around two orders magnitude algorithms interest earlier system produced group array processor rap used commercial compared rap multiprocessor similar performance represents order magnitude reduction cost problems arithmetic satisfactory international computer science institute center berkeley
steps towards form parallel distributed genetic programming genetic programming method program discovery consisting special kind genetic algorithm capable operating nonlinear parse trees representing programs run programs paper describes parallel distributed genetic programming new form genetic programming suitable development parallel programs based representation parallel programs crossover mutation operators guarantee syntactic paper describes operators reports preliminary results obtained paradigm
using generative models handwritten digit recognition
fitness structure acquisition genetic programming define fitness structure genetic programming mapping program fitness values paper shows various fitness structures problem independent relate acquisition rate acquisition found directly correlated fitness structure whether structure uniform linear exponential understanding fitness structure provides partial insight complicated relationship fitness function outcome genetic search
rapid learning circuits via longterm argued events situations memory requires rapid formation neural circuits binding errors binding matches formation circuits binding matches modeled associative learning mechanisms rapid formation circuits binding errors difficult explain given behavior circuit must formed response binding particular pattern input subsequent formation must response binding pattern led formation plausible account formation circuits computational model described demonstrates pattern activity representing event lead rapid formation circuits detecting binding errors result longterm within structures whose architecture similar hippocampal formation neural structure known critical memory model exhibits high memory capacity robust limited amounts cell loss model also offers alternate interpretation functional role region formation memories predicts nature memory result damage various regions hippocampal formation
learning using markov models project
specialization populations artificial neural networks specialization populations artificial neural networks studied organisms fixed evolvable fitness formulae isolated shared environments behaviors compared evolvable fitness formula specifies evaluation measure let free evolve obtain coevolution expressed behavior individual evolvable fitness formula isolated environment behavior organisms fixed fitness formula behavior organisms individual evolvable fitness formulae population analysis shows almost organisms population isolated environment converge towards behavioral strategy find competition act provide population populations organisms shared environment
connectionist objectoriented network simulator users minimize learning curve using objectoriented library constructing training utilizing connectionist networks library contains object classes needed simulator small amount added source code examples included size experimental ann programs greatly reduced using objectoriented library time programs easier evolve library includes database network behavior training procedures user designed run efficiently data parallel computers rap efficiency parallel computers primary goals several secondary design goals allow heterogeneous algorithms training procedures trained together within constraints attempt maximize variety artificial neural net work algorithms supported
finding design knowledge systems laboratory march report
discovery symbolic neural networks parallel distributed genetic programming technical report august abstract genetic programming method program discovery consisting special kind genetic algorithm capable operating parse trees representing programs run programs paper describes parallel distributed genetic programming new form genetic programming suitable development parallel programs symbolic neural processing elements combined free natural way based representation parallel programs crossover mutation operators guarantee syntactic paper describes operators reports results obtained problem
routing optical interconnection networks neural network solution much interest using implement computer interconnection networks however little discussion routing already used paper neural network routing methodology proposed generate control bits optical interconnection network though present optical implementation methodology illustrate control optical interconnection network may used communication shared memory distributed computing systems routing methodology makes use artificial neural network ann functions parallel computer generating neural network routing scheme may applied electrical optical interconnection networks however since ann implemented using routing approach especially optical computing environment parallel nature ann computation may make routing scheme faster conventional routing approaches especially furthermore neural network routing scheme results shown generating stage
fast parallel neural network training parallel system straightforward extension used speech recognition research order high performance artificial neural network training without requiring changes user ann library modified run report present algorithms used code analyse communication computation requirements resulting performance model yields better understanding system speedups potential experimental results actual training runs model demonstrate achieved performance levels
genetic algorithm allocation distributed database system paper explore distributed database allocation problem intractable also discuss genetic algorithms used successfully solve combinatorial problems experimental results show far superior greedy heuristic obtaining optimal near optimal allocation problem various data sets
gene biological development neural networks exploratory model
ilp description learning problem towards general definition data mining ilp proc annual workshop special interest group machine learning research report abstract task discovering interesting regularities large sets data data mining knowledge discovery recently increased interest machine learning general inductive logic programming ilp particular however widely accepted definition task concept learning examples ilp definitions data mining task proposed recently paper examine socalled semantics definitions show property data mining learning task task makes perfect sense without assumption therefore introduce define generalized definition data mining task called ilp description learning problem discuss properties relation traditional concept learning prediction learning problem since characterization entirely level models definition applies independently chosen hypothesis language
predictive control interconnection networks using neural networks interconnection networks limited significant control latency used large multiprocessor systems latency time required analyze current traffic network establish required paths goal latency minimize effect control paper introduce technique performs latency learning patterns communication traffic using information need communication paths hence network provides required communication paths path made study communication patterns memory parallel program used input time neural network perform online training prediction predicted communication patterns used interconnection network controller provides memory based experiments neural network able learn highly communication patterns thus able predict allocation communication paths resulting reduction communication latency
performance online learning methods predicting multiprocessor memory access patterns technical report institute advanced computer studies university college abstract shared memory require interconnection networks control unit however often time primarily due control latency amount time control unit takes decide desired new configuration reduce control latency prediction unit added controller job reduce control configuration time major component control latency three different online prediction techniques tested learn predict memory access patterns three typical parallel processing applications relaxation algorithm matrix fast fourier transform predictions used routing control algorithm reduce control latency provide needed memory access paths three prediction techniques used tested markov predictor linear predictor time neural network predictor expected different predictors performed best different applications however produced best overall results
simulation based bayesian nonparametric regression methods
genetic algorithm task distributed system paper explore distributed task problem intractable also discuss genetic algorithms used successfully solve combinatorial problems experimental results show far superior greedy heuristic obtaining optimal near optimal task problem various data sets
consistency posterior distributions neural networks paper show posterior distribution feedforward neural networks asymptotically consistent paper extends earlier results universal approximation properties neural networks bayesian setting proof consistency problem density estimation problem uses bounds entropy show posterior consistent result back regression setting show consistency setting number hidden nodes growing sample size case number hidden nodes treated parameter thus provide theoretical using neural networks nonparametric regression bayesian framework
influence diagrams
interactive planning architecture case paper describes interactive planning system developed inside intelligent decision support system aimed supporting operator planning initial planning architecture integration casebased reasoning techniques constraint reasoning techniques exploited mainly performing temporal reasoning temporal metric information temporal reasoning plays central role supporting interactive functions provided user performing two basic steps planning process plan adaptation resource scheduling first prototype integrated situation assessment resource allocation currently tested
comparison pruning methods relational concept learning two standard methods dealing noise concept learning methods efficient methods typically accurate much slower generate specific concept description first variety pruning methods including two new methods try combine integrate order achieve accuracy efficiency verified test series chess position classification task
topdown pruning relational learning pruning effective method dealing noise machine learning recently pruning algorithms particular reduced error pruning also interest field inductive logic programming however shown methods inefficient time generating clauses explain noisy examples subsequently pruning clauses introduce new method searches good theories topdown fashion get better starting point pruning algorithm experiments show approach significantly lower complexity task without predictive accuracy
updates queries probabilistic networks traditional databases commonly support efficient query update procedures operate time size database goal paper take first step toward dynamic reasoning probabilistic databases comparable efficiency propose dynamic data structure supports efficient algorithms updating connected bayesian networks conventional algorithm new evidence time queries time size network propose algorithm preprocessing phase allows answer queries time olog olog time per evidence usefulness processing time applications requiring near realtime response large probabilistic databases briefly discuss potential application dynamic probabilistic reasoning computational biology
localized partial evaluation belief networks network often however application will need information every node network will need exact probabilities present localized partial evaluation propagation algorithm computes interval bounds marginal probability specified query node examining subset nodes entire network parts network far away node much impact value property able produce better solutions intervals given time consider network
cooperative bayesian casebased reasoning solving multiagent planning tasks describe integrated problem solving architecture named bayesian networks casebased reasoning cbr work multiagent planning tasks includes dynamic tasks paper simulated soccer example bayesian networks used characterize action selection whereas casebased approach used determine implement actions paper two contributions first survey casebased bayesian approaches perspective popular cbr task decomposition framework thus explaining types allows explain unique aspects proposed integration second demonstrate bayesian nets used provide environmental context thus feature selection information casebased reasoner
genetic algorithm optimization neural networks
techniques reducing superior building blocks genetic algorithms
efficient construction networks learned representations general specific relationships machine learning systems often represent concepts rules sets attributevalue pairs many learning algorithms generalize concept representations removing adding pairs thus concepts created general specific relationships paper presents algorithms concepts network based general specific relationships since concept access related concepts quickly resulting structure allows increased efficiency learning reasoning time complexity one set learning models improves log olog number nodes using general specific structure
convergence analysis canonical genetic algorithms paper analyzes convergence properties canonical genetic algorithm mutation crossover proportional reproduction applied static optimization problems proved means homogeneous finite markov chain analysis will never converge global optimum regardless crossover operator objective function variants always maintain best solution population either selection shown converge global optimum due property underlying original results discussed respect schema theorem
case retrieval nets applied large case bases article presents experimental results obtained applying case retrieval net approach large case bases obtained results suggest successfully handle case bases larger considered reports
machine learning application real world database
representing preferences decisiontheoretic preferences specify relative possible outcomes alternative plans order express general patterns preference domain require language refer directly preferences classes outcomes individuals present basic concepts theory meaning generic facilitate incremental capture exploitation automated reasoning systems semantics comparisons individuals comparisons classes things equal means contextual equivalence relations among individuals vary context application discuss implications theory represent ing preference information
genes baldwin effect learning evolution simulated population baldwin effect first proposed suggests course evolutionary change influenced learned behavior existence effect still topic paper clear evidence presented plasticity level produce directed changes level research earlier experimental work done others hinton amount plasticity learned behavior shown crucial size baldwin effect either little much effect significantly reduced finally learnable case made many generations will become easier population whole learn plasticity will increase transition driven population one driven learning importance baldwin effect decreases
casebased reactive navigation casebased method online selection adaptation reactive control parameters article presents new line research investigating online learning mechanisms autonomous intelligent agents discuss casebased method dynamic selection modification behavior system casebased reasoning module designed addition traditional reactive control system provides flexible performance novel environments without extensive highlevel reasoning otherwise slow system method implemented casebased reactive robotic system evaluated empirical simulation system several different environments including box environments known reactive control systems general technical report college computing institute technology
generalization controlled examples specific general network supervised inductive learning examples uses ideas neural networks symbolic inductive learning gain benefits methods network built many simple nodes learn important features input space monitor ability features predict output values network avoids exponential nature number features creating specific features example features making general expansion feature another feature outputs empirical evaluation model realworld data shown network provides good generalization performance convergence accomplished within small number training network provides benefits automatically nodes without requiring user parameters network learns incrementally operates parallel fashion paper describes network architecture supervised learning combines techniques used neural networks symbolic machine learning gain advantages approaches supervised learning network given training set containing examples example gives input pattern along corresponding output network produce presented input task network converge representation contains information given training set generalize information network will inputs trained one approach generalization look important features input space feature subset network inputs along associated values feature matched values network inputs part feature equal values inputs given feature inputs part feature value feature predicts output high probability important feature number inputs contained feature order feature determines generality feature feature inputs general feature feature many inputs specific feature monitor possible input features number features exponential number inputs paper proposes specific general network creates specific input features generalizes features one way generalizes combining similar specific features two features similar close input space combining two features inputs common features creates new feature original features new feature general matches points input space defined example section presents overview model later provide detail system network made many simple nodes node contains input feature monitors training node statistics giving discrete conditional probability possible output value given input feature
analytical mean squared error curves temporal difference learning provide analytical expressions changes bias variance table estimators provided various monte carlo temporal difference value estimation algorithms updates trials markov reward processes used expressions develop software analysis tool given complete description markov reward process rapidly yields exact curve curve one get averaging together sample curves infinite number learning trials given problem use analysis tool illustrate classes curve behavior variety example reward processes show although various temporal difference algorithms quite sensitive choice parameters values parameters make similarly generally good
applicability neural network machine learning natural language processing examine inductive inference complex grammar specifically consider task training model classify natural language sentences grammatical thereby kind power provided principles parameters linguistic framework theory investigate following models feedforward neural networks locally recurrent networks williams recurrent networks euclidean simulated annealing decision trees feedforward neural networks network machine learning models included primarily comparison address question neural network distributed nature gradient descent based iterative calculations possess linguistic capability traditionally handled symbolic computation recursive processes initial simulations models partially successful using large temporal window input models trained fashion learn grammar significant degree attempts training recurrent networks small temporal input implemented several techniques aimed improving convergence gradient descent training algorithms discuss theory present empirical study variety models learning algorithms behaviour present attempting learn simpler grammar
parallel gradient distribution optimization parallel version proposed fundamental theorem serial optimization parallel theorem allows parallel processors use simultaneously different algorithm descent gradient algorithm processor perform one many steps serial algorithm gradient objective function assigned independently processors synchronization step performed convex functions consists taking strong convex combination points found processors convex functions best point found processors taken better point fundamental result establish point parallel algorithm stationary case global solution convex case computational testing machines multiprocessor indicate speedup order number processors employed
problem formulation program synthesis program transformation techniques simulation optimization constraint satisfaction research
evolving sensors environments controlled complexity sensors represent crucial link evolutionary forces species relationship environment individuals cognitive abilities behave learn report experiments using new class latent energy environments models define environments carefully controlled complexity allow state bounds random optimal behaviors independent strategies achieving behaviors using analytic basis defining environments use neural networks model individuals steady state genetic algorithm model evolutionary process particular sensors experiments consider two types sensors variants allowed learn learn via error correction internal prediction via reinforcement learning find predictive learning even using larger sophisticated sensors provides advantage learn however reinforcement learning using small number sensors provide significant advantage analysis results points tradeoff genetic robustness sensors learning system
machine learning statistics tutorial machine learning version brief make available machine learning tutorial statistics workshop available pages please questions please also note date recently updated plan make updates bound quickly also lack time project limited finally definition book now date process subject still suggest recent conference proceedings international also papers subject though around yet however written good paper empirical science written several use author incomplete many references may use
bayesian bayesian approach multivariate adaptive regression spline fitting proposed takes form probability distribution space possible models explored using reversible markov chain monte carlo methods generated sample models produced shown good predictive power allows easy interpretation relative importance predictors overall fit
induction logic programs inference regular languages recursive clauses allowed sequences resolution steps initial theory however many allowed sequences resolution steps expressed set one approach problem presented system based earlier technique learning finitestate automata represent allowed sequences resolution steps extends previous technique three ways negative examples considered addition positive examples new strategy performing generalization used iii technique learned automaton logic program included results experiments presented outperforms system using old strategy performing generalization traditional covering technique latter result explained limited hypotheses produced covering also fact covering needs produce correct base clauses recursive definition
perfect simulation specified models discuss ideas producing perfect simulations based coupling past finite state space models naturally extend multivariate distributions infinite state spaces models using gibbs sampling combination methods originally introduced perfect simulation point processes
general classes functions main result paper equivalence asymptotic controllability nonlinear control systems existence continuous functions defined means generalized derivatives manner one obtains complete characterization asymptotic controllability applying principle far wider class systems theorem feedback stabilization existence smooth proof relies theory optimal control techniques introduction paper study systems general form
metric entropy minimax risk classification apply recent results minimax risk density estimation related problem pattern classification notion loss minimize information theoretic measure predict classification future examples given classification previously seen examples give asymptotic characterization minimax risk terms metric entropy properties class distributions might generating examples use results characterize minimax risk special case noisy classification problems terms density
guided crossover new operator genetic algorithm based optimization genetic algorithms gas extensively used different domains means global optimization simple yet reliable manner much better global optima gradient based methods usually converge local optima however gas close optima small number iterations get close optima needs large number iterations whereas gradient based usually get close local optima relatively small number iterations paper describe new crossover operator designed abilities without actually computing without global optimality operator works using guidance members population select direction exploration empirical results two engineering design domains across binary floating point representations demonstrate operator significantly improve steady state error
cellular encoding applied neural networks trained balancing cart fixed track one variant single pole system pole cart position variables supplied inputs network must learn compute problems solved using fixed architecture using new version cellular encoding evolves application specific architecture realvalued weights learning times generalization capabilities compared neural networks developed using methods processing topologies produced cellular encoding simple analyzed architectures hidden units produced single pole two pole problem velocity information supplied input moreover linear solutions display good generalization control problems cellular encoding automatically generate architectures whose complexity structure reflect features problem solve
exact transition probabilities independence metropolis sampler recent result shown compute explicitly markov chain derived special case sampling algorithm known metropolis sampler note show extend result obtain exact transition probabilities done first chain finite state space extended general discrete continuous state space paper concludes implications diagnostic tests convergence markov chain samplers
learning reformulation appropriate iterative design known reformulation improve speed reliability numerical optimization engineering design argue best choice reformulation depends design goal present technique automatically constructing rules map design goal reformulation chosen space possible tested technique domain design reformulation corresponds incorporating constraints search space applied standard algorithm c45 set training data describing constraints active optimal design goal encountered previous design session used rules choose appropriate reformulation set test cases experimental results show using improves speed reliability design optimization competing methods best performance possible
best known bounds weighted matching polynomialtime apply reduction two total sum matching weights becomes comparison node critical node apply reduction node total sum matching weights becomes comparisons two critical nodes sum total weight thus since total involved time reduce total sum matching weights theorem let monotone function bounding time complexity moreover let satisfy constant monotone constants time proof theorem computed time applying get computable time log computational complexity matrices mathematical biology robust model finding optimal evolutionary trees press see also
asymptotic controllability implies feedback stabilization
classification segments model using hybrid neural network report results recognition extracted database current system differs others similar tasks use specific time normalization techniques use detailed biologically motivated input representation speech model implemented detailed high dimensional representation known classified either backpropagation hybrid neural network classifier hybrid network composed biologically motivated unsupervised network supervised backpropagation network approach produces results comparable obtained others without addition time normalization
feasibility study fully autonomous vehicles using decisiontheoretic control final report
application minimization machine learning knowledge discovery paper presents new application machine learning particularly pattern theory analysis various logic synthesis programs conducted laboratory machine learning applications creating robust efficient boolean machine learning minimize decomposed function measure functions help solve practical problems application areas interest pattern theory group especially problems require strongly functions large number variables many functions complexity minimization better small functions worse however much faster run problems variables significant improvements also found analyze cases worse propose new improvements strongly functions
incremental controller networks comparative study two selforganising nonlinear controllers two selforganising controller networks presented study controller network uses spatial clustering approach select controllers gated controller network network performance model controller used achieve controller selection algorithm architecture networks described makes two schemes selforganising different examples control nonlinear systems considered order illustrate behaviour makes clear schemes performing much better single adaptive controller two main advantages possibilities use controller building block network architecture apply modelling purpose however appears serious problems cope nonlinear systems single variable nonlinear behaviour suffer high sensitivity clustering space order main limiting use therefore makes much suitable approach control wide range nonlinear systems
pattern theoretic feature extraction constructive induction paper offers perspective features pattern finding general perspective based robust complexity measure called decomposed function function decomposition algorithm minimizing complexity measure finding associated features results experiments algorithm also
comparison new old algorithms mixture estimation problem investigate problem estimating vector likelihood given sample mixture given densities adapt framework developed supervised learning give simple derivations many standard iterative algorithms like gradient projection framework distance new old vectors used penalty term square distance leads gradient projection update relative entropy new update call gradient update second order expansion relative entropy used update gives usual update experimentally update update outperform algorithm variants also prove polynomial bound rate convergence algorithm
comparison direct modelbased reinforcement learning paper compares direct reinforcement learning explicit model modelbased reinforcement learning simple task find task modelbased approaches support reinforcement learning smaller amounts training data efficient handling changing goals
programming research group learnability model universal representations
comparison fixed floating building block representation genetic algorithm article compares traditional fixed problem representation style genetic algorithm new floating representation building blocks problem fixed specific locations individuals population addition effects noncoding segments representations studied noncoding segments computational model noncoding dna floating building blocks location independence genes fact structures natural genetic systems suggests may provide advantages evolutionary process results show significant difference gas solve problem fixed floating representations gas able maintain diverse population floating representation combination noncoding segments floating building blocks appears take advantage parallel search recombination abilities
improved performance guarantees accurate classifiers extend bounds develop superior probabilistic performance guarantees accurate classifiers original bounds classifier accuracy depend accuracy parameter since accuracy known priori parameter value gives bounds used present method bounds accuracy using old method uses bound improved parameter value bounds show use bounds practice generalize bounds individual classifiers form uniform bounds multiple classifiers
evolving cooperative groups preliminary results abstract multiagent systems require coordination sources distinct expertise perform complex tasks effectively paper use coevolutionary approach using genetic algorithms evolve multiple individuals effectively solve common problem run individual group paper experiment domain requires cooperation two agents used two mechanisms evaluating individual one population pair randomly members population pair members population shared memory containing best pairs found far approaches successful generating optimal behavior patterns however preliminary results exhibit edge shared memory approach
recursive automatic algorithm selection inductive learning technical report august
methods competitive coevolution finding coevolution refers simultaneous evolution two distinct populations coupled fitness landscapes paper consider competitive coevolution fitness individual population based direct competition individuals population competitive coevolution applied three problems small version two new techniques competitive coevolution explored competitive fitness sharing changes way fitness measured shared sampling way chosen testing experiments using show substantial improvement performance methods used preliminary results using coevolution discovery cellular automata rules playing presented
function approximation neural networks local methods bias variance smoothness review use global local methods estimating function mapping samples function containing noise relationship methods examined empirical comparison performed using multilayer perceptron mlp global neural network model single model linear local approximation model following commonly used datasets chaotic time series time series english data speech building energy prediction data dataset find simple local approximation models often outperform mlp criterion size training set dimensionality training set etc used distinguish whether mlp local approximation method will superior however find consider knn density estimates training datasets choose best performing method priori selecting local approximation density large choosing mlp otherwise result hypothesis global mlp model less appropriate characteristics function approximated varies input space discuss results smoothness assumption often made function approximation
fast kohonen net implementation present implementation kohonen selforganizing feature maps vector system implementation supports arbitrary neural map topologies arbitrary neighborhood functions small networks used realworld tasks single board measured run kohonen net classification connections per second speech coding benchmark task performs online kohonen net training connection updates per second represents almost factor improvement compared previously reported implementations asymptotic peak speed system
learning images using dynamic feature binding isolated object image despite fact complex visual scenes contain multiple overlapping objects people perform object recognition accuracy one operation facilitates recognition early segmentation process features objects labeled according object belong current computational systems perform operation based predefined grouping heuristics describe system called learns group features based set examples many cases discovers grouping heuristics similar previously proposed also capability finding structural regularities images grouping performed relaxation network attempts dynamically related features features signal phase one another binding thus represented phase related features training procedure generalization recurrent back propagation units
beyond independence conditions optimality simple bayesian classifier simple bayesian classifier commonly thought assume attributes independent given class surprisingly good performance exhibits many domains contain clear attribute explanation proposed far paper show fact assume attribute independence optimal even assumption wide margin key finding lies distinction classification probability estimation correct classification achieved even probability estimates used contain large errors show region optimality fraction actual one followed derivation several necessary several sufficient conditions optimality example optimal learning arbitrary conjunctions even though independence assumption paper also reports empirical evidence competitive performance domains containing substantial degrees attribute dependence
intelligent search method using inductive logic programming propose method use inductive logic programming give heuristic functions searching goals solve problems method takes solutions problem history search set background knowledge problem large class problems problem described set states set operators solved finding series operators solution series operators initial state final state transformed positive negative examples relation describes operator better others state also give way use relation heuristic function method use logic program background knowledge induce heuristics induced heuristics high paper method applying
generalization local coordinate transformation
dynamic belief networks discrete monitoring describe development monitoring system uses sensor observation data discrete events construct dynamically probabilistic model world model bayesian network incorporating temporal aspects call dynamic belief network used reason uncertainty causes consequences events basic dynamic construction network datadriven however model construction process combines sensor data events provided information agents behaviour knowledge already contained within model control size complexity network means network structure within time interval amount history detail vary time illustrate system example domain monitoring robot vehicles people restricted dynamic environment using sensor data addition presenting generic network structure monitoring domains describe use complex network structures address two specific monitoring problems sensor validation data association problem
power decision tables evaluate power decision tables hypothesis space supervised learning algorithms decision tables one simplest hypothesis spaces possible usually easy understand experimental results show artificial realworld domains containing discrete features algorithm inducing decision tables sometimes outperform algorithms c45 surprisingly performance quite good datasets continuous features indicating many datasets used machine learning either require features features values also describe incremental method performing crossvalidation applicable incremental learning algorithms including using incremental crossvalidation possible given dataset time linear number instances number features number values time incremental crossvalidation independent number chosen hence leaveoneout crossvalidation crossvalidation take time
feature subset selection using method overfitting dynamic search space topology approach feature subset selection search optimal set features made using induction algorithm box estimated future performance algorithm heuristic search statistical methods feature subset selection including forward selection elimination stepwise variants viewed simple hillclimbing techniques space feature subsets utilize search find good feature subset discuss overfitting problems may associated searching many feature subsets introduce operators dynamically change topology search space better utilize information available evaluation feature subsets show operators previous approaches deal relevant irrelevant features improved feature subset selection yields significant improvements realworld datasets using id3 naivebayes induction algorithms
neural network based tracking system constructed tracking system learns track uses real time graphical user inputs auxiliary signals train neural network inputs neural network consist images motion information frame differences images also used provide scale online training phase neural network rapidly input weights depending upon reliability different channels environment adaptation allows system track even objects moving within background
problems graphs paper consider complexity number combinatorial problems namely graphs dna physical mapping graphs perfect phylogeny directed modified feasible register assignment module allocation graphs bounded problems characteristic uniform upper bound tree path width graphs problems exceptions feasible register assignment module allocation edge coloring given part input main results parameterized variant considered problems hard complexity classes also show graphs graphs
parity problem away wellknown certain learning methods perceptron learning algorithm acquire complete parity mappings often learning methods c45 backpropagation incomplete parity mappings failure methods parity mappings may sometimes impossible mappings parity problems mathematical constructs little realworld learning however paper argues shows parity mappings hard learn statistically statistical property expect frequently realworld contexts also shows generalization failure parity mappings occurs even large incomplete mappings used training purposes generalization particularly
chapter empirical comparison stochastic algorithms empirical comparison stochastic algorithms graph several stochastic methods used solving optimization problems examples algorithms include order increasing computational complexity stochastic greedy search methods simulated annealing genetic algorithms investigate methods likely give best performance practice respect computational effort requires study problem empirically selecting set stochastic algorithms varying computational complexity experimentally evaluating method results achieved improves increasing computational time evaluation use graph optimization problem closely related several realworld practical problems get wider perspective achieved results stochastic methods also compared greedy heuristics investigation suggests although genetic algorithms provide good results simpler stochastic algorithms achieve similar performance quickly
sequential importance sampling nonparametric bayes models next generation running title nonparametric bayes two generations gibbs sampling methods models involving dirichlet process first generation namely locations clusters groups parameters essentially become fixed moving two strategies proposed create second generation gibbs samplers integration second stage gibbs sampler cluster locations show strategies easily implemented sequential importance sampler first strategy dramatically improves results case gibbs sampling strategies applicable much wider class models shown provide uniform importance sampling weights lead additional estimators associate professor department statistics state university assistant professor institute statistics decision sciences university assistant professor department statistics university work second author supported part national science foundation grants last author national science foundation grants fellowship
free lunch early stopping show uniform prior hypothesis functions training error early stopping fixed training error training error minimum results increase expected generalization error also show regularization methods equivalent early stopping certain prior early stopping solutions
exact learning dnf formulas malicious membership queries
error stability properties generalized algorithms present unified framework convergence analysis generalized algorithms presence perturbations one principal novel features analysis perturbations need tend zero limit established algorithms certain sense stationary set problem depends magnitude perturbations characterization sets given general case results convex weakly strongly convex problems analysis extends previously known results convergence stability properties gradient methods including incremental parallel modifications first author supported part grant research second author supported part international science foundation grant international science foundation grant foundation fundamental research grant email operations research department computational state university
power system margin prediction using radial basis function networks research partially supported grants national science foundation gas electric research partially supported grants national science foundation john foundation paper will appear proceedings annual power symposium
dynamics coevolutionary learning coevolutionary learning involves adaptive learning agents fitness environment dynamically progress potential solution many problems several recent surprising artificial robot player recently solved two problem difficult neural network benchmark classification problem using genetic programming set koza instead using absolute fitness use relative fitness based competition coverage data set population fitness function driving selection changes rather solutions found method structure suggests open crossover better able discover modular build ing blocks
power team exploration two robots learn directed graphs show two robots learn exactly directed graph nodes expected time polynomial introduce new type sequence two robots helps robots recognize certain nodes present algorithm robots learn graph sequence simultaneously graph unlike previous learning results using sequences algorithm require teacher provide furthermore algorithm use efficiently additional information available nodes also present algorithm robots learn taking random rate random walk graph converges stationary distribution characterized graph algorithm learns expected time polynomial inverse efficient algorithm graphs
action oriented control visualization neural networks introduction technical description
learning boundary queries introduce new model learning membership queries queries near boundary target concept may receive incorrect care responses partial assume distribution examples zero probability boundary region motivation behind model reason incorrect care response examples extremely practice thus learner present several positive results new model show learn two membership queries near boundary may algorithm extension algorithm learns two homogeneous model also describe algorithms learning several monotone dnf formulas
guided unsupervised learning using local multivariate binary processors thank peter helpful comments work work supported network grant human community
gain leads high variability simple model cortical regular understand interval variability visual cortical neurons critical examine dynamics neuronal integration variability synaptic input current previous models focused latter factor match simple model experimentally measured properties cortical regular spiking cells setting parameters set match experimental measurements neuronal gain obtained frequency current resulting model leads intuitive neuronal integration arguments hold spiking regular memory last becomes threshold caused input variance around steady state spiking neurons matched cortical cell steady state behavior highly variable rates wide range inhibitory excitatory inputs
cooperative design web paper describes system cooperative design applications web system provides experts involved procedures means individual arguments preferences order argue selection certain choice supports qualitative reasoning presence information performed set acts call variety procedures propagation information corresponding discussion graph paper also reports integration case based reasoning techniques used current design issues considering previous similar situations similarity measures various items aim estimate variations among designers involved cooperative design
efficient learning typical finite automata random paper describes new efficient algorithms learning deterministic finite automata approach primarily distinguished two features setting model typical finite automaton worstcase model underlying graph automaton along learning model learner provided means experiment machine rather must learn solely observing output behavior random input sequence main contribution paper presenting first efficient algorithms learning nontrivial classes automata entirely learning model online learning model learner predict output next state given next symbol random input sequence goal learner make prediction mistakes possible assuming learner means target machine fixed start state first present efficient algorithm makes expected polynomial number mistakes model next show first algorithm used second algorithm also makes polynomial number mistakes even absence along way prove number combinatorial results randomly labeled automata also show states bits input sequence need random finally discuss extension results model automata used represent distributions binary strings
program search hierarchical variable length representation genetic programming simulated annealing hill climbing paper presents comparison genetic simulated annealing stochastic iterated hill climbing based suite program discovery problems previously three search algorithms employ hierarchical variable length representation programs recent paradigm intuitively obvious adaptive search handle program discovery yet date problem also work
gibbs sampler empirical estimator nearest neighbor random fields given markov chain sampling scheme standard empirical estimator make best use data show construct better estimators restrict attention nearest neighbor random fields gibbs samplers deterministic approach applies sampler uses reversible updating deterministic structure transition distribution sampler exploited construct empirical estimators combined standard empirical estimator reduce asymptotic variance extra computational cost random field spatially homogeneous estimator lead variance reduction performance estimators evaluated simulation study model
modeling evolution motivation order learning improve animals behavior thus direct evolution way baldwin suggested learning mechanism must incorporate evaluation animals actions influence fitness example many circumstances damage animal otherwise reduce fitness tend avoided refer mechanism animal evaluates fitness consequences actions motivation system argue system must evolve along behaviors evaluates describe simulations evolution populations agents number different architectures generating action learning worlds complexity find cases members populations evolve motivation systems accurate enough direct learning increase fitness actions agents perform furthermore motivation systems tend incorporate systematic representations worlds increase behavior generated
automatic words based sparse data relation language traditionally rule sets approaches offer means knowledge automatically problems arise training material sparse sparse data wellknown problem many algorithms present experiments connectionist instancebased decisiontree learning algorithms applied small instancebased learning algorithm yields best generalisation performance algorithms tested perform given even sparse valuable efficient tool automatic written text
reduced machine description scheduling constraints high performance rely accurate modeling machine resources efficiently exploit instruction level parallelism application paper propose reduced machine description results faster detection resource preserving scheduling constraints present original machine description proposed approach reduces machine description automated efficient fashion moreover fully supports process operations arbitrary order reduced descriptions result times faster detection resource require memory storage used original machine descriptions
choice wavelet shrinkage estimate spectrum study problem estimating log spectrum stationary gaussian time series thresholding empirical wavelet coefficients propose use depending sample size wavelet basis resolution level resolution levels propose purpose thresholding level make nearly noisefree possible addition visual point view noisefree character leads attractive theoretical properties wide range smoothness assumptions previous set much smaller properties log
data mining using mlc machine learning library data mining algorithms including machine learning statistical analysis pattern recognition techniques greatly improve understanding data now paper focus classification algorithms review need multiple classification algorithms describe system called mlc designed help choose appropriate classification algorithm given dataset making easy compare utility different algorithms specific dataset interest mlc provides comparisons also provides library classes aid development new algorithms especially hybrid algorithms multistrategy algorithms algorithms generally hard code scratch discuss design issues programs visualization resulting classifiers
reinforcement learning modular neural networks control reinforcement learning methods applied control problems objective optimizing value function time used train single neural networks learn solutions whole tasks jordan shown set expert networks combined via gating network quickly learn tasks decomposed even decomposition learned inspired work modular neural networks learning temporaldifference methods modify reinforcement learning algorithm called qlearning train modular neural network solve control problem resulting algorithm demonstrated classical problem advantage method makes possible deal complex dynamic control problem effectively using task decomposition competitive learning
casebased classification report extends results reported naval air center automatic classification images used novel casebased reasoning systems empirical studies obtain comparative analyses using standard classification algorithms therefore quality results unknown studies also tested several classifiers casebased otherwise machine learning literature comparisons detailed paper next investigated two suggestions future work combining similarity functions alternative case representation finally describe several ways incorporate additional knowledge applying casebased classifiers similar tasks
functions timevarying set stabilization paper shows time varying systems global asymptotic controllability given closed subset state space equivalent existence continuous function respect set
learning adaptation strategies introspective reasoning memory search casebased reasoning systems case adaptation process traditionally controlled static libraries adaptation rules paper proposes method learning adaptation knowledge form adaptation strategies type developed adaptation strategies differ standard adaptation rules encode general memory search procedures finding information needed case adaptation paper focuses issues involved learning memory search procedures form basis new adaptation strategies proposes method small library abstract adaptation rules uses introspective reasoning systems memory organization generate memory search plans needed apply rules search plans original abstract rules form new adaptation strategies future use process allows cbr system learn domain results case adaptation also learn apply cases memory effectively
goaldriven learning fundamental issues symposium report artificial intelligence psychology growing body research supports view learning process psychological experiments show people different goals process information studies show goals strong effects students learn functional arguments machine learning support focusing learner effort annual conference cognitive science symposium together researchers psychology discuss goaldriven learning article presents fundamental points symposium context open questions current research goaldriven learning appears
evaluating neural network predictors present new method inspired bootstrap whose goal determine quality reliability neural network predictor method leads robust forecasting along large amount statistical information performance exploit exhibit method context multivariate time series prediction financial data new york stock exchange turns variation due different splits training crossvalidation test sets significantly larger variation due different network conditions architecture initial weights furthermore method allows probability distribution opposed traditional case just single value time step demonstrate test set includes stock market also compare performance class neural networks linear models
predictions confidence intervals local error present new method obtaining local error estimates confidence predicted value depend input approach problem nonlinear regression maximum likelihood framework demonstrate technique first computer generated data locally varying normally distributed target noise apply data santa time series competition finally extend technique estimate error iterated predictions apply exact competition task gives best performance date
minimax bayes asymptotic minimax sparse wavelet priors precise asymptotic evaluation minimax mean squared error estimation signal gaussian noise signal known priori lie compact space minimax bayes method applied variety global nonparametric estimation settings parameter spaces far example leads theory exact asymptotic minimax estimation norm besov spaces using simple estimators wavelet bases paper outlines features method common several applications particular derive new results exact asymptotic minimax risk weak also class local estimators scale nature method reveals structure asymptotically least distributions thus may simulate least sample paths illustrate estimation signal gaussian noise norm certain besov spaces wavelet bases least priors sparse resulting sample paths different observed setting acknowledgements grateful many david helpful comments work supported part nsf grants grant
exploration training unsupervised projection pursuit neural network graphical demonstrated using unsupervised neural networks three projection pursuit indices compared low dimensional simulated realworld data principal components projection pursuit network
adaptive metropolis algorithm proper choice proposal distribution mcmc methods metropolishastings algorithm known crucial factor convergence algorithm paper introduce adaptive metropolis algorithm gaussian proposal distribution updated along process using full information far due adaptive nature process algorithm establish correct properties also include results numerical tests indicate algorithm traditional metropolishastings algorithms demonstrate provides easy use algorithm practical computation subject classification keywords adaptive mcmc comparison convergence markov chain
priors basis functions regularization radial additive splines previously shown regularization principles lead approximation schemes equivalent networks one layer hidden units called regularization networks particular discussed standard smoothness lead subclass regularization networks wellknown radial basis functions approximation schemes paper show regularization networks much range approximation schemes including many popular general additive models neural networks particular introduce new classes smoothness lead different classes basis functions additive splines product splines obtained appropriate classes smoothness furthermore extension leads radial basis functions rbf basis functions also leads additive models ridge approximation models containing special cases functions forms projection pursuit regression propose use term generalized regularization networks broad class approximation schemes follow extension regularization probabilistic interpretation regularization different classes basis functions correspond different classes prior probabilities approximating function spaces therefore different types smoothness assumptions final part paper show relation activation functions gaussian sigmoidal type considering simple case kernel summary different multilayer networks one hidden layer call generalized regularization networks correspond different classes priors associated smoothness classical regularization principle three broad classes radial basis functions generalize basis functions product splines additive splines generalize schemes type ridge approximation functions perceptrons paper describes research done within center biological computational learning department brain cognitive sciences artificial intelligence laboratory research sponsored grants office naval research grant national science foundation contract asc9217041 includes provided program grant national health contract additional support provided organization atr visual perception research laboratories electric corporation siemens support laboratorys artificial intelligence research provided onr contract supported college massachusetts institute technology massachusetts institute technology
evolution size variable length representations many cases programs increase known increasing structural complexity artificial evolution show specific genetic programming suggest inherent search techniques discrete variable length representations using simple static evaluation functions investigate characteristics three one population based search techniques using novel mutation operator artificial following santa problem solved simulated annealing hill climbing hill climbing population based search using two variants new based mutation operator predicted observed using unbiased mutation simulated annealing hill using length mutation however occurs using population conclude two causes search operators length bias tend sample trees competition within populations longer programs usually accurately
massively parallel casebased reasoning probabilistic similarity metrics propose probabilistic metric case matching case adaptation tasks central approach probability propagation algorithm bayesian reasoning systems allows casebased reasoning system perform theoretically sound probabilistic reasoning probability propagation mechanism actually offers uniform solution case matching case adaptation problems also show algorithm implemented connectionist network efficient massively parallel case retrieval inherent property system argue using kind approach difficult problem case indexing completely avoided topics casebased reasoning michael
evolving artificial neural networks using baldwin effect paper describes simple means genetic search towards optimal neural network architectures improved convergence speed quality final result result theoretically explained baldwin effect implemented just learning process network alone also changing network architecture part learning procedure seen combination two different techniques help ing improving simple genetic search
trees splines survival analysis technical report revised march university washington department statistics seattle washington abstract past years several nonparametric alternatives proportional hazards model appeared literature methods extend techniques known regression analysis analysis survival data paper discuss methods based partition trees polynomial splines analyze two datasets using survival compare strengths weaknesses two methods one strengths model fitting procedure implicit check underlying hazards model also provides explicit model conditional hazards function makes obtain graphical hand methods automatically partition dataset groups cases similar survival history results obtained survival trees often trees splines survival analysis provide data two useful tools analyzing survival data
alternative discretetime operators application nonlinear models
estimating functions probability distributions finite set samples part bayes estimators paper second series two problem estimating function probability distribution finite set samples distribution first paper bayes estimator function probability distribution introduced optimal properties bayes estimator discussed bayes estimators entropy derived current paper analysis first paper extended derivation bayes estimators several functions interest statistics information theory functions mutual information tests independence variance covariance average finding bayes estimators several functions requires extensions analytical techniques developed first paper extensions form main body paper paper extends analysis ways example class potential priors beyond uniform prior assumed first paper particular use dirichlet priors considered
receptive fields vision object recognition many areas visual system organized maps preserve certain degree unit part map normally part visual field receptive field receptive fields probably computational mechanism employed biological information processing systems paper surveys possible computational reasons behind examples solutions problems vision spatial sensory coding object recognition institute appear vision mit press
local hebbian learning use
neural architecture learns multiple transformations spatial representations
combining neural network time series
computing largest fraction missing information algorithm worst address problem computing largest fraction missing information algorithm worst linear function data largest associated operator maximum likelihood estimate important convergence iterative simulation estimate largest fraction missing information available often adequate since accuracy needed instances iteration also gives estimate worst linear function show power method used compute efficient accurate estimates quantities unlike decomposition power method computes largest matrix take advantage good estimate initial value accuracy obtained moreover matrix products needed power method computed need form operator give results studies multivariate normal data showing approach becomes efficient data dimension increases methods use approximation generalpurpose alternative available national health small grant office naval research adrian raftery comments discussion advance research improve paper
hierarchical community experts describe directed acyclic graphical model contains hierarchy linear units mechanism dynamically selecting appropriate subset units model observation nonlinear selection mechanism hierarchy binary units output one linear units connections linear units binary units generative model viewed logistic belief net selects linear model among available linear units show gibbs sampling used learn parameters linear binary units even sampling brief markov chain far equilibrium
note learning multipleinstance examples describe simple reduction problem paclearning multipleinstance examples paclearning random classification noise thus concept classes learnable noise includes concepts learnable usual random noise model plus others parity function learnable multipleinstance examples also describe efficient somewhat involved reduction model results polynomialtime algorithm learning axisparallel rectangles sample complexity roughly factor results
hierarchical learning procedural abstraction mechanisms
coordination control structures processes possibilities connectionist networks absence powerful control structures processes coordinate choose among direct interactions combine distinct yet modules large connectionist networks probably one important reasons networks yet handling difficult tasks complex object recognition description complex problemsolving planning paper examine built large numbers relatively simple units given ability handle problems typical networks artificial intelligence programs along types programs always handled using extremely precisely central control coordination synchronization switching etc point several mechanisms central control sort already built hidden often ways examine kinds control mechanisms found computers programs development cellular function system evolution social especially might use particularly suggestions found oscillators local sources complex partial global effects slow electrical developmental program development communication coordination within among living cells working system evolutionary processes operate large populations organisms great variety partially competing partially controls found small groups larger systems rich control typically control complex interactions many local sources explore several different kinds plausible control mechanisms might incorporated assess potential benefits respect cost
comparison dynamic distance drug activity prediction drug activity prediction handwritten character recognition features extracted describe training example depend pose location orientation etc example handwritten character recognition one best techniques addressing problem distance method introduce new also addresses problem dynamic iteratively learns neural network examples effort maximize predicted output values new models trained new computed models converge paper compares dynamic distance method task predicting biological activity compounds crossvalidation
improving integrating casebased reasoning heuristic search analyse behaviour propose revise architecture design problem show problem solving method solve possible cases available domain knowledge investigate problem show limitation caused restricted search regime employed method method improved acquiring additional domain knowledge therefore propose alternative design problem solver integrates casebased reasoning heuristic search techniques limitations exhibited propose revise architecture maintaining level efficiency describe four algorithms casebased design exploit general properties parametric design tasks application specific heuristic knowledge
properties genetic representations neural architectures genetic algorithms related evolutionary techniques offer promising approach automatically exploring design space neural architectures artificial intelligence cognitive modeling central process evolutionary design neural architectures choice representation scheme used encode neural architecture form gene string genotype genotype corresponding neural architecture representation scheme used class neural architectures evolvable system also determines efficiency complexity evolutionary design procedure whole paper identifies discusses set properties used characterize different representations used design select representations necessary properties particular classes applications
parallel language neural algorithms language reference tutorial load balancing even neural networks idea achieve goals lies programming model programs connections nodes graph neural network objects algorithms based parallel local computations nodes connections communication along connections plus reduction operations report describes design resulting language definition discusses detail tutorial example program
issues goaldriven explanation reasoner explains surprising events internal use key motivation explaining perform learning will facilitate goals human use range strategies build explanations including internal reasoning external information search effect choices explanations however standard models explanation rely use single fixed build explanations paper argues explanation modeled goaldriven learning process information discusses issues involved developing active multistrategy process goaldriven explanation
abduction experience goals model abductive explanation
neural network model visual selforganizing model connected orientation maps primary visual cortex used study psychological phenomenon known selforganizing processes responsible longterm development map lateral connections shown result short time scales model allows observing large numbers neurons connections simultaneously making possible relate phenomena events difficult experimentally results give computational support idea direct arise adaptive lateral interactions feature long also suggest indirect effects result synaptic resources process model thus provides unified computational explanation selforganization direct indirect primary visual cortex
factor graphs algorithms factor graph graph global function several variables factors product local functions factor graphs many graphical models including bayesian networks markov random fields graphs describe general algorithm computing global function distributed corresponding factor graph wide variety algorithms developed artificial intelligence statistics signal processing digital derived specific instances general algorithm including belief propagation belief revision algorithms fast fourier transform algorithm algorithm iterative algorithm
automatic programming robot controller analog electrical circuit implement genetic programming automatic programming technique evolves computer programs solve approximately solve problems paper presents two examples genetic programming creates computer program controlling robot robot moves specified point minimal time first approach genetic programming evolves computer program composed ordinary
reasoning portions precedents paper argues task matching casebased reasoning often improved comparing new cases portions precedents example presented illustrates combining portions multiple precedents permit new cases new cases compared entire precedents system uses portions precedents analysis domain law described examples analysis combine reasoning steps multiple precedents presented
model projection action designing autonomous agents deal issues involving time space tradeoff made guaranteed one hand flexibility propose model action probabilistic reasoning decision analytic evaluation use control architecture model suited tasks require reasoning interaction behaviors events fixed temporal horizon decisions continuously problem plans new information becomes available paper particularly interested tradeoffs required guarantee fixed time reasoning nondeterministic relationships exploiting approximate decision making processes able trade accuracy predictions speed decision making order improve expected per dynamic situations
parallel programming model dynamic neural networks programming model allows compiler built using compilation techniques also applied parallel machines paper presents main ideas techniques used results obtained various
approximating value trees structured dynamic programming propose examine method approximate dynamic programming markov decision processes based structured problem representations assume mdp represented using dynamic bayesian network construct value functions using decision trees function representation size representation within acceptable limits pruning value trees leaves represent possible values thus approximating value functions produced optimization propose method detecting convergence prove errors bounds resulting approximately optimal value functions policies describe preliminary mental results
evolving programs register machine code majority commercial computers today register machines von type developed method evolve programs register machine described implementation enables use program constructs arithmetic operators large memory automatic decomposition conditional constructs loop structures functions string list functions function set system use register machine language allows work level binary machine code without steps von machine programs data memory genetic operators thus directly binary machine code memory genetic operators written modify individuals binary representation result execution speed enhancement times compared implementation times compared implementation use binary machine code compact coding one per node individual resulting evolved programs incorporated conventional software development environment low memory requirements significant speed enhancement technique use applying genetic programming new application areas research domains
exploratory learning game paper considers importance exploration programs learn playing central question whether learning program play move offers best present game play move best providing useful information future games approach addressing question developed using probability theory implemented two different learning methods initial experiments game suggest program takes exploration account learn better opponent program
framework combining symbolic neural learning rule extraction neural networks algorithm technical report computer sciences department university abstract article describes approach combining symbolic connectionist approaches machine learning framework presented research several groups reviewed respect framework first stage involves symbolic knowledge neural networks second addresses refinement prior knowledge neural representation third concerns extraction refined symbolic knowledge experimental results open research issues discussed version paper will appear machine learning
analysis sentences embedded clauses distributed neural network model called processing sentences recursive relative clauses described model based separating tasks input word sequence clauses representations keeping track recursive different modules system needs trained basic sentence constructs generalizes new instances familiar relative clause structures novel structures exhibits plausible memory depth center increases memory earlier performance semantic constraints ability process structure largely due central network monitors controls execution entire system way contrast earlier systems modeled controlled highlevel process rather one based automatic responses
predictive memorybased reinforcement learning approach adaptive traffic control paper propose memorybased qlearning algorithm called predictive adaptive traffic control attempt address two problems encountered namely routing policies low network load learn new optimal policies load conditions unlike memorybased reinforcement learning algorithms memory used keep past experiences increase learning speed best experiences learned predicting traffic effectiveness verified various network topologies traffic conditions simulation results show superior
online adaptation signal dual reinforcement learning several researchers demonstrated neural networks trained nonlinear signal distortion digital systems networks however require original signal version known therefore trained offline adapt changing channel characteristics paper novel dual reinforcement learning approach proposed adapt online system performing assuming channel characteristics directions two end communication channel using output determine reinforcement using common series model simulate channel system shown successfully learn significantly higher might expected actual channel
connectionist modeling assumes noisefree inputs assumption often paper introduces idea clearning simultaneously data learning underlying structure step viewed topdown processing model modifies data learning step viewed bottomup processing data modifies model clearning used conjunction standard pruning paper discusses statistical foundation clearning gives interpretation terms model describes obtain point predictions conditional densities output shows resulting model used discover properties data otherwise ratio inputs paper uses clearning predict exchange rates noisy time series problem wellknown benchmark performances outofsample test period clearning obtains return significantly better otherwise identical network final network remaining weights initial weights inputs hidden units robust overfitting small network also interpretation
connectionist modeling assumes noisefree inputs assumption often paper introduces idea clearning simultaneously data learning underlying structure step viewed topdown processing model modifies data learning step viewed bottomup processing data modifies model clearning used conjunction standard pruning paper discusses statistical foundation clearning gives interpretation terms model describes obtain point predictions conditional densities output shows resulting model used discover properties data otherwise ratio inputs paper uses clearning predict exchange rates noisy time series problem wellknown benchmark performances outofsample test period clearning obtains return significantly better otherwise identical network final network remaining weights initial weights inputs hidden units robust overfitting small network also interpretation
learning less data experiments lifelong robot learning
wavelet thresholding via bayesian approach discuss bayesian formalism gives rise type wavelet threshold estimation nonparametric regression prior distribution wavelet coefficients unknown response function designed capture wavelet expansion common applications prior specified posterior yields thresholding procedure prior model underlying function give functions specific besov space establish relation hyperparameters prior model parameters besov spaces within prior will fall relation gives insight meaning besov space parameters moreover established relation makes possible principle incorporate prior knowledge functions properties prior model wavelet coefficients however prior knowledge functions properties might hard propose standard prior hyperparameters works examples several simulated examples used illustrate method comparisons made thresholding methods also present application data set collected study
choice basis approximation
fast algorithm computation perfect perfect phylogeny problem classical problem computational evolutionary biology set described set qualitative recent years problem shown npcomplete general different fixed parameter versions solved polynomial time particular developed algorithm perfect phylogeny problem species defined since commonly character data drawn molecular sequences length sequences thus large thus develop algorithms run efficiently large values paper make additional observations structure problem produce algorithm problem runs time also show possible efficiently build structure implicitly represents set perfect randomly sample set
adaptive probabilistic networks belief networks probabilistic networks neural networks two forms network representations used development intelligent systems field artificial intelligence belief networks provide representation general probability distributions set random variables facilitate exact calculation impact evidence interest neural networks represent parameterized algebraic combinations nonlinear activation functions found use models real neural systems function approximators simple training algorithms furthermore simple local nature neural network training algorithms provides certain biological plausibility allows massively parallel implementation paper show similar local learning algorithms derived belief networks learning algorithms operate using information directly available normal inferential processes networks main belief networks competing neural networks tasks precise local probabilistic interpretation belief networks also allows partially constructed humans allows results learning easily understood allows contribute rational decisionmaking way
parallel learning algorithm bayesian inference networks present new parallel algorithm learning bayesian inference networks data learning algorithm exploits properties score metric distributed asynchronous adaptive search technique called fault dynamic load balancing features scales demonstrate effectiveness approach empirically several experiments using order machines specifically show distributed algorithm provide optimal solutions larger problems good solutions bayesian networks variables
convergence algorithm gibbs sampler summary article investigate relationship two popular algorithms algorithm gibbs sampler show approximate rate convergence gibbs sampler gaussian approximation equal corresponding type algorithm helps implementing either algorithms improvement strategies one algorithm directly particular running algorithm know approximately many iterations needed convergence gibbs sampler also obtain result conditions algorithm used finding maximum likelihood estimates slower converge corresponding gibbs sampler bayesian inference uses proper prior distributions illustrate results number realistic examples based generalized linear mixed models
classification using feature extraction based analysis bcm theory sound classification demonstrated using novel application wavelet decomposition feature extraction using bcm unsupervised network different feature extraction methods different wavelet representations studied system achieves classification performance even tested different locations used training improved results suggest nonlinear feature extraction wavelet representations outperforms different linear choices basis functions
errorcorrecting output codes general method improving multiclass inductive learning programs multiclass learning problems involve finding definition unknown function whose range discrete set containing values classes definition acquired studying large training examples form existing approaches problem include direct application multiclass algorithms decisiontree algorithms id3 cart application binary concept learning algorithms learn individual binary functions classes application binary concept learning algorithms distributed output codes employed sejnowski system paper compares three approaches new technique errorcorrecting codes employed distributed output representation show output representations improve performance id3 task backpropagation task results demonstrate errorcorrecting output codes provide generalpurpose method improving performance inductive learning programs multiclass problems
hypotheses found inverse entailment extended abstract paper give theorem inductive inference rule inverse entailment proposed main result hypothesis clause derived example background theory inverse entailment relative sense theory clausal theory example clause neither implied derived hypothesis clause always order prove result give declarative semantics arbitrary consistent clausal theories show originally introduced complete procedural semantics shown extension theorem also show every hypothesis derived generalization proposed must sense moreover show generalization obtained inverse entailment giving restriction
structured reversal simulation dynamic probabilistic networks present algorithm reversal bayesian networks treestructured conditional probability tables consider advantages especially simulation dynamic probabilistic networks particular method allows one produce nodes involved reversal exploit regularities conditional distributions argue approach associated reversal plays important role evidence integration used restrict sampling variables also provide algorithm dynamic state variables forward simulation algorithm exploits structured network determine fashion conditions variable need sampled
inductive constraint logic novel approach learning first order logic formulae positive negative examples presented whereas present inductive logic programming systems employ examples true false facts clauses view examples interpretations true false target theory viewpoint allows inductive logic programming paradigm classical attribute value learning sense latter special case former property able adapt cn2 type algorithms order enable learning full first order formulae however whereas classical learning techniques concept representations disjunctive normal form will use clausal representation corresponds normal form forms constraint positive examples representation also role positive negative examples heuristics algorithm resulting theory incorporated system named icl inductive constraint logic
solving multipleinstance problem axisparallel rectangles multiple instance problem arises tasks training examples single example object may many alternative feature vectors instances describe yet one feature vectors may responsible observed classification object paper describes compares three kinds algorithms learn axisparallel rectangles solve multipleinstance problem algorithms multiple instance problem perform poorly algorithm directly multiple instance problem attempting identify feature vectors responsible observed classifications performs best giving correct predictions prediction task paper also illustrates use artificial data compare algorithms
efficient function constraint classification learning new class data structures called described structures useful efficiently implementing number neural network related operations empirical comparison radial basis functions presented robot arm mapping learning task applications density estimation classification constraint representation learning also
automatic definition modular neural networks
category control navigation planning preference presentation exploiting model uncertainty estimates dynamic model learning combined dynamic programming shown effective learning control continuous state dynamic systems simplest method assumes learned model correct applies dynamic programming many approximators provide uncertainty estimates fit exploited paper addresses case system must failures learning propose new algorithm adapted dual control literature use bayesian locally weighted regression models stochastic dynamic programming common reinforcement learning assumption exploration paper addresses case system exploration algorithm illustrated dimensional simulated control problem
multiclass problems discretization icl extended abstract handling multiclass problems real numbers important practical applications machine learning problems attributevalue learners address problems rule ilp systems ilp systems handle real numbers mostly trying real values applicable thus running efficiency overfitting problems paper discusses recent extensions icl address problems icl inductive constraint logic ilp system learns first order logic formulae positive negative examples main icl view examples seen interpretations true false clausal target theory first argue icl used learning theory disjunctive normal form dnf possible solution handling two classes given based ideas cn2 finally show problems continuous values adapting discretization techniques attribute value learners
learning assembly operation connectionist reinforcement technique paper presents learning controller capable increasing speed operations without increasing force level aim find better relationship measured forces controlled velocity without using complicated human generated model followed connectionist approach two learning phases distinguished first learning controller trained supervised way suboptimal task frame controller reinforcement learning phase follows controller consists two networks policy network exploration network online robotic exploration plays crucial role obtaining better policy architecture extended third network reinforcement network learning controller implemented force simulator contrast related work experiments simulated degrees freedom performance task measured time force level fact better performance obtained way demonstrates importance learning techniques robotic assembly tasks paper presents approach simulation results keywords robotic assembly artificial neural networks reinforcement learning
robust sound localization application auditory perception system robot
causal inference indirect experiments indirect experiments studies randomized control randomized subjects rather receive treatment programs purpose paper attention experimental researchers simple mathematical results enable assess indirect experiments strength causal influences operate among variables interest results despite encouraging indirect experimentation yield significant sometimes accurate information impact program population whole particular individuals program
identification frequency response measurements paper problem system identification investigated case given frequency response data necessarily uniformly grid frequencies large class convergent identification algorithms derived particular algorithm examined explicit worst case error bounds norm derived discretetime systems examples provided illustrate application algorithms
scheduling parallelism raw machine advances technology will enable within next unfortunately architectures modern exploit advances achieving high level parallelism reasonable speed requires processor resources already architecture raw takes position space resources instruction register memory twodimensional fully compiler compilation parallelism ilp machines requires spatial instruction scheduling traditional temporal instruction scheduling paper describes techniques used raw compiler handle issues preliminary results compiler sequential programs written indicate raw approach exploiting ilp achieve speedups number processors applications parallelism raw architecture attempts provide performance least comparable provided scaling existing architecture achieve orders magnitude improvement performance applications large amount parallelism paper offers positive results direction
manipulation learning process hand
integrating bias search bias neural network learning use previously learned knowledge learning shown reduce number examples required good generalization increase robustness noise examples various means using learned knowledge domain guide learning domain two underlying classes methods use previous knowledge learner bias use previous knowledge learner search bias show methods fact exploit domain knowledge shown presenting combined approach learner combined approach seen outperform individual methods conditions accurate previously learned domain knowledge available irrelevant features domain representation
analog neural nets gaussian common noise distributions recognize arbitrary regular languages consider recurrent analog neural nets output subject gaussian noise common noise distribution large set show many regular languages networks type give precise characterization languages result implies constraints possibilities constructing recurrent analog neural nets robust realistic types analog noise hand present method constructing feedforward analog neural nets robust analog noise type
modifying network architectures revision paper describes system revising probabilistic rule bases symbolic rules connectionist network trained via connectionist techniques uses modified version backpropagation refine factors rule base uses heuristic add new rules work currently way finding improved techniques modifying network architectures include adding hidden units using algorithm case made via comparison fully connected connectionist techniques keeping rule base close original possible adding new input units needed
distance induction first order logic used classification via process experiments paper supervised induction distance examples described horn clauses constrained clauses approaches approach defining small set complex discriminant hypotheses hypotheses serve new concepts used initial examples embedded space natural distance examples thus naturally follows
using temporaldifference reinforcement learning improve decisiontheoretic diagnosis probability theory represents uncertainties behave need utility theory assigns values usefulness different states decision theory concerns optimal rational decisions many methods probability modeling learning utility decision models use reinforcement learning find optimal sequence questions diagnosis situation maintaining high accuracy automated diagnosis domain used demonstrate temporaldifference learning improve diagnosis database results better reported previous methods
issues integration data mining data visualization simple bayesian classifier simple bayesian classifier sometimes called naivebayes built based conditional independence model attribute given class model previously shown surprisingly robust obvious independence assumption accurate classification models even clear conditional dependencies serve excellent tool initial exploratory data analysis coupled makes structure comprehensible describe visual representation model successfully implemented describe requirements visualization design decisions made satisfy
evolution neural networks sequential decision tasks
simulation reduced precision arithmetic digital neural networks using rap machine paper describes recent work development computer architectures efficient execution artificial neural network algorithms earlier system array processor rap multiprocessor based commercial interconnection scheme used rap simulate variable precision arithmetic guide design higher performance based rap system critical role study experiment much larger networks otherwise possible study shows backpropagation training algorithms require precision specifically weight values output values sufficient achieve training classification results comparable floating point although results frame classification continuous speech expect will extend many connectionist calculations used results part design single reduced precision arithmetic permits use multiple units per processor also reduced precision make efficient use valuable arithmetic applications represents order magnitude reduction cost systems based
evolving networks using genetic algorithm connectionist learning artificial life studies science complexity eds vol use genetic algorithms back propagation training feedforward neural networks ieee international conference neural networks vol fast genetic selection features neural network classifiers ieee transactions neural networks vol automatic design cellular neural networks means genetic algorithms finding feature third ieee international workshop cellular neural networks applications ieee new efficient reinforcement learning evolution machine learning vol genetic algorithms van new york algorithm selective pressure proceedings third conference genetic algorithms morgan san van hinton neural network simulator department computer science university toronto toronto
new roles machine learning design design computing new roles research machine learning design use development techniques solve simple problems effort important early stages development field scale address real design problems since existing techniques based simplifying assumptions hold real design particular address dependence context multiple often design paper analyzes present situation number views subsequently paper offers alternative approach whose goal advance use machine learning design practice approach partially integrated modeling system called use machine learning presented open research issues
automatic smoothing spline projection pursuit automatic smoothing spline projection pursuit standard algorithm estimates smooth functions using nonparametric algorithm constructs model max linear combinations back simpler model size max max specified user paper discusses alternative algorithm smooth functions estimated using smoothing splines direction coefficients amount smoothing direction number terms max determined optimize single generalized crossvalidation measure
learning refining algorithm paper suggest mechanism improves significantly performance topdown inductive logic programming ilp learning system improvement achieved cost giving system extra information difficult formulate information appears form algorithm incomplete somewhat representation computation related particular example describe give details learning algorithm exploits information contained experiments carried implemented system demonstrated usefulness method potential future applications
architecture iterative learning recursive definitions paper concerned problem inducing recursive horn clauses small sets training examples method iterative bootstrap induction presented first step system generates simple clauses properties required definition properties represent generalizations positive examples simulating effect larger number examples properties used subsequently induce required recursive definitions paper describes method together series experiments results support thesis iterative bootstrap induction indeed effective technique general use ilp
automatic design cellular neural networks means genetic algorithms finding feature paper examine use genetic algorithms optimize cellular neural network architectures application hand character recognition aim evolve optimal feature order aid conventional classifier network generalize across different end performance function genetic encoding feature presented experiment described optimal feature indeed found genetic algorithm interested application cellular neural networks computer vision genetic algorithms gas serve optimize design cellular neural networks although design global architecture system still done human insight propose specific system best optimized using one optimization method gas good candidate optimization role suited problems objective function complex function many parameters specific problem investigate one character recognition specifically like use find optimal feature used recognition digits
bootstrap evaluation effect data splitting financial time series article problems commonly used technique splitting available data training validation test sets held fixed strong conclusions static splits shows potential variability across splits using bootstrap method compare uncertainty solution data splitting neural network specific uncertainties parameter choice number hidden units etc present two results data new york stock exchange first variation due different significantly larger variation due different network conditions result implies important model ensemble models estimated one specific split data second split neural network solution early stopping close linear model significant extracted
routing reinforcement learning estimating shortest paths dynamic graphs
early stopping validation used detect overfitting supervised training neural network training convergence avoid overfitting early stopping exact criterion used early stopping however usually chosen fashion training describes select stopping criterion systematic fashion either learning procedures improving generalization important particular situation empirical investigation multilayer perceptrons shows exists tradeoff training time generalization given training runs using different problems different network architectures conclude slower stopping criteria allow small improvements generalization average cost much training time factor longer average
learning population hypotheses introduce new formal model learning algorithm must combine collection potentially poor statistically independent hypothesis functions order approximate unknown target function arbitrarily motivation includes question make optimal use multiple independent runs learning algorithm settings many hypotheses obtained distributed population identical learning agents
markov chain monte carlo practice discussion markov chain monte carlo mcmc methods make possible use flexible bayesian models otherwise computationally infeasible recent years great variety applications described literature applied new methods may several questions concerns however much effort expertise needed design use markov chain sampler much confidence one answers mcmc produces use mcmc affect process joint statistical august mcmc users discussed issues various trade paper discussion purpose offer advice guidance users mcmc users topics include building confidence simulation results methods convergence estimating standard errors identification models good mcmc algorithms exist current state software development
proceedings international workshop neural networks realvalued neural network machine learning laboratory computer science department young university email abstract many neural network models must trained finding set realvalued weights yield high accuracy training set learning models require weights input attributes yield high leaveoneout classification accuracy order avoid problems associated irrelevant attributes high dimensionality addition variety general problems set real values must found maximize evaluation function paper presents algorithm schemata search realvalued weight space find set weights real values yield high values given evaluation function algorithm called realvalued schemata search uses statistical technique determine search space paper details approach gives initial empirical results
estimating square root density via supported large body nonparametric statistical literature devoted density estimation given paper addresses problem univariate density estimation novel way approach class called projection estimators introduced basis used basis supported family among others applied density estimation local nature wavelet functions makes wavelet estimator superior projection estimators use classical bases fourier etc instead estimating unknown density directly estimate square root density enables control norm density estimate however approach one needs density calculate sample wavelet coefficients describe datadriven procedure determining maximum number levels wavelet density estimator coefficients selected levels make estimator
control selective visual attention modeling intermediate higher vision processes require selection subset available sensory information processing usually selection implemented form spatially region visual field socalled focus attention visual scene dependent input state subject present model control focus attention based map mechanism expected model biological vision also essential understanding complex scenes machine vision
mutual information bayesian measure independence abstract problem hypothesis testing examined historical bayesian points view case sampling underlying joint probability distribution hypotheses tested independence dependence underlying distribution exact results bayesian method provided asymptotic bayesian results historical method quantities compared historical method quantities interpreted terms clearly defined bayesian quantities asymptotic bayesian test relies upon mutual information problems hypothesis testing arise situations observed data produced unknown process question process observed data arise hypothesis testing problem point view sampling several fixed hypotheses tested given measures test quality found directly likelihood amounts sampling likelihood specific hypothesis set possible parameter vectors parameter vector completely sampling distribution simple hypothesis hypothesis set contains one parameter vector composite hypothesis occurs hypothesis set single parameter vector generally test procedure true hypothesis gives largest test value although notion procedure specific may refer method choosing hypothesis given test values since interest quantify quality test level significance generated level probability chosen hypothesis test procedure incorrect hypothesis choice made significance generated using sampling distribution likelihood simple hypotheses level significance found using single parameter value hypothesis test applied case composite hypothesis size test found given probability ranging parameter vectors hypothesis set chosen
guide literature learning probabilistic networks data literature review discusses different methods general learning bayesian networks data includes overlapping work general probabilistic networks connections drawn statistical neural network uncertainty different methodological bayesian description length classical statistics basic concepts learning bayesian networks introduced methods reviewed methods discussed learning parameters probabilistic network learning structure learning hidden variables presentation avoids formal definitions theorems literature instead illustrates key concepts simplified examples
building classifiers using bayesian networks recent work supervised learning shown surprisingly simple bayesian classifier strong assumptions independence among features called naive bayes competitive state classifiers c45 fact question whether classifier less assumptions perform even better paper examine evaluate approaches inducing classifiers data based recent results theory learning bayesian networks bayesian networks representations probability distributions generalize naive bayes classifier explicitly represent independence among approaches single method call tree augmented naive bayes outperforms naive bayes yet time maintains computational simplicity search involved robustness characteristic naive bayes experimentally tested approaches using benchmark problems repository compared c45 naive bayes feature selection methods
learning belief networks presence missing values hidden variables recent years works learning probabilistic belief networks current state methods shown successful two learning scenarios learning network structure parameters complete data learning parameters fixed network incomplete presence missing values hidden variables however method yet demonstrated effectively learn network structure incomplete data paper propose new method learning network structure incomplete data method based extension expectationmaximization algorithm model selection problems performs search best structure inside procedure prove convergence algorithm adapt learning belief networks describe learn networks two scenarios data contains missing values presence hidden variables provide experimental results show effectiveness procedure scenarios
static data association prior density
systematic description greedy algorithms cost sensitive generalisation paper defines class problems involving combinations induction cost framework presented systematically describes problems involve construction decision trees rules accuracy measurement misclassification costs present new algorithms shows framework used greedy algorithms constructing trees rules framework covers number existing algorithms moreover framework also used define algorithm configurations new expressed evaluation functions
integrating inductive learning prior knowledge reasoning
learning reason introduce new framework study reasoning learning order reason approach developed views learning part inference process suggests learning reasoning studied together learning reason framework combines world used known learning models reasoning task performance criterion suitable framework intelligent agent given access learning interface also given period interact interface construct representation world reasoning performance measured period agent presented queries query language relevant world answer whether implies approach overcome main computational difficulties traditional treatment reasoning separation world since agent world constructing knowledge representation choose representation useful task hand moreover now make explicit dependence reasoning performance environment agent show previous results learning theory reasoning fit framework illustrate usefulness learning reason approach new results possible traditional setting first give learning reason algorithms classes propositional languages efficient reasoning algorithms represented traditional knowledge base second exhibit learning reason algorithm class propositional languages known learnable traditional sense earlier version paper appears proceedings national conference artificial intelligence supported grant nsf grant supported nsf grant author present addresses division applied sciences university cambridge email department computer science university email permission make digital hard copies part work personal use without provided copies made distributed direct commercial advantage copies show first initial display along full components work others must credit otherwise lists use component work works requires prior specific permission andor may new york
approximate reasoning many problems reduce evaluating probability propositional expression true paper show problem computationally intractable even surprisingly restricted cases even approximation probability consider various methods used approximate reasoning computing degree belief bayesian belief networks reasoning techniques constraint satisfaction knowledge compilation use approximation avoid computational difficulties reduce problems propositional domain prove assignments propositional languages intractable even horn monotone formulae even size clauses number variables extremely limited case deductive reasoning horn theories theories binary clauses distinguished existence linear time satisfiability algorithms even surprising show even approximating number assignments approximating approximate reasoning intractable restricted theories also identify restricted classes propositional formulae efficient algorithms assignments given preliminary version paper appeared proceedings international joint conference artificial intelligence supported nsf grants
extended kalman filter recurrent neural network training pruning recently extended kalman filter based training demonstrated effective neural network training however conjunction pruning methods weight decay optimal brain damage yet studied paper will method training propose pruning method based results obtained training combined training pruning method applied time series prediction problem
induction musical structure describe recent extensions framework automatic generation programs previously used genetic programming techniques produce programs satisfy critical criteria paper describe new work use connectionist techniques automatically induce musical structure show resulting neural networks used genetic programming system argue framework potentially support induction structural features music present initial results produced using neural hybrid discuss directions future work
combining inductive learning prior knowledge reasoning much effort devoted understanding learning reasoning artificial intelligence however models attempt integrate two complementary processes rather body research machine learning often focusing inductive learning examples quite isolated work reasoning artificial intelligence though two processes may different much ability reason domain knowledge often based rules domain must learned ability reason often used acquire new knowledge learn paper introduces incremental learning algorithm attempts combine inductive learning prior knowledge reasoning many important characteristics useful combination including incremental selforganizing learning learning inherent capabilities low order polynomial complexity paper describes gives simulation results several applications discusses characteristics detail
toward trainer paper appeared machine learning abstract paper demonstrates nature training learning play perfect information board games considers different kinds competitive training impact trainer error appropriate metrics performance measurement ways metrics applied results suggest teaching program leading repeatedly restricted paths high quality ones variations appear realworld experience results also demonstrate variety introduced training random choice program training may important situations results argue broad variety training experience play many levels variety may either inherent game introduced training practice training expert guidance knowledgebased shown particularly effective learning competition
heuristic approach discovery machine learning negative effect naturally significant complex domain graph simple domain line earlier complex domain means learning useful weight greater simple domain complex domain optimality requirement macro complex domain becomes purpose research described paper identify parameters effects deductive learning perform experiments systematically order understand nature effects goal paper demonstrate methodology performing parametric experimental study deductive learning example include study two parameters point scale used search carried problem solving time learning time showed optimal solutions benefit macro learning strategy comes search utility increases also demonstrated deductive learners learn offline solving training problems sensitive type search used learning showed general optimizing search best learning generates increase quality solutions regardless search method used problem solving also improves efficiency problem require high level optimality using optimizing search increase learning resources fact results described surprising goal parametric study necessarily find results obtain results sometimes even previously known controlled experimental environment work described part research plan currently process extensive experimentation parameters described also others also test validity conclusions study tests several commonly known search problems systematic experimentation will help research community better understand process deductive learning will serve experimental methodology used machine learning research
frame problem bayesian network action representations examine number techniques representing actions stochastic effects using bayesian networks influence diagrams compare techniques according specification size representation required complete specification dynamics particular system particular attention role relationships precisely characterize two components frame problem bayes nets stochastic actions propose several ways deal problems compare solutions solution frame problem situation result set techniques permit specification compact representation probabilistic system dynamics comparable size representation explicit frame
learning queries highly noisy case task case given function mapping inputs finite field consider task list degree agree fraction ffi input space give randomized algorithm solving task box runs time polynomial special case solve problem case running time algorithm bounded polynomial exponential algorithm generalizes previously known algorithm due solves
experimentation better perfect guidance many problems correspond classical control task determining appropriate control action take given sequence observations one standard approach learning control rules called behavior involves perfect operator operate trying behavior experimental learning approach contrast learner first initial policy tries policy performs learner modify produce new policy paper discusses relative effectiveness two approaches especially presence perceptual showing particular experimental learner often learn effectively one
neural models hierarchies present connectionist method representing images explicitly addresses hierarchical nature data viewpoint sensitive cells cortex ideas hierarchical descriptions based resulting model makes critical use bottomup topdown pathways analysis synthesis illustrate model simple example representing information faces
cognition paper discusses role evolution cognitive systems define information individuals generations means experiments presented use genetic programming systems include special mechanisms transmission information systems evolve computer programs perform cognitive tasks including mathematical function mapping action selection virtual world data show presence mechanisms clear impact correct programs implications results may cognitive science briefly discussed
transformation system interactive reformulation design optimization strategies numerical design optimization algorithms highly sensitive particular formulation optimization problems given formulation search space objective function constraints will generally large impact optimization process quality resulting design furthermore best formulation will vary one application domain another one problem another within given application domain unfortunately design may know best formulation advance attempting set run design optimization process order problem developed software environment supports interactive formulation testing reformulation design optimization strategies system represents optimization strategies terms graphs strategies implemented transformations graphs system permits user generate search space design optimization strategies experimentally evaluate performance test problems order find strategy suitable application domain system implemented domain independent fashion tested domain design
planning incremental dynamic programming paper presents basic results ideas dynamic programming relate directly concerns planning form theoretical basis incremental planning methods used integrated architecture incremental planning methods based continually updating evaluation function mapping reactive system actions generated reactive system thus involve minimal incremental planning process guarantees actions evaluation function will extensive search required methods suited stochastic tasks tasks complete accurate model available tasks large implement mapping table methods must used capabilities significant limitation approach
design evaluation rule induction algorithm technical report
cbr document retrieval project paper reports project document retrieval industrial setting objective provide tool helps finding documents related given query answers frequently questions databases cbr approach used develop running system currently practical evaluation
many queries needed learn investigate query complexity exact learning membership proper equivalence query model give complete characterization concept classes learnable polynomial number polynomial queries model give applications characterization including results learning natural subclass dnf formulas learning membership queries alone query complexity previously used prove lower bounds time complexity exact learning show new relationship query complexity time complexity exact learning class exactly properly learnable polynomial query complexity learnable polynomial time particular show class exactly learnable learnable using oracle
evaluation case study evaluating casebased system paper presents case study evaluating casebased system describes evaluation system combination rulebased casebased reasoning three sets experiments run set exploratory measurements profile systems operation comparison systems set studies modified various parts system contribution lessons learned experiments cbr evaluation methodology cbr theory discussed work may whole part commercial purpose permission whole part without research purposes provided whole partial copies include following permission electric research laboratories cambridge massachusetts authors individual contributions work applicable portions reproduction purpose require electric research laboratories
tight performance bounds greedy policies based imperfect value functions consider given value function states markov decision problem might result applying reinforcement learning algorithm value function corresponding optimal value function states will natural call bellman residual value function specifies state obtained lookahead along best action state using given value function evaluate states paper derives tight bound far optimal discounted return greedy policy based given value function will function maximum norm magnitude bellman residual corresponding result also obtained value functions defined pairs used qlearning one significant application results problems function approximator used learn value function training approximator based trying minimize bellman residual across states pairs control based use resulting value function result provides link objectives function approximator training quality resulting control
canonical distortion measure vector quantization function approximation measure quality set vector quantization points means measuring distance random point quantization required common metrics euclidean metrics mathematically simple inappropriate comparing natural signals speech images paper shown environment functions input space induces canonical distortion measure canonical shown optimizing reconstruction error respect gives rise optimal constant approximations functions environment calculated closed form several different function classes algorithm training neural networks implement presented along experimental results
optimized theory revision module theory revision systems typically use set transformations given initial theory new theory whose empirical accuracy given set labeled training instances local maximum process compares accuracy current theory neighbors goal determining neighbor highest accuracy obvious simply evaluates individual neighbor theory instance expensive evaluate single theory single instance great many training instances number neighbors approach slow present alternative system employs quickly computes accuracy transformed theory inside reasoning effects transformation compare performance naive system realworld theories obtained expert system find runs times faster accuracy paper also discusses source power keywords theory revision efficient algorithm hillclimbing system multiple related version paper many helpful comments report
density rate estimation right data using wavelet methods paper describes wavelet method estimation density rate functions randomly right data nonparametric approach assuming density rate specific parametric form method based time number intervals number events within interval number events survival function observations separately time via linear wavelet rate function estimators obtained taking ratio prove estimators possess global mean square consistency obtain best possible asymptotic convergence rate also asymptotically normally distributed also describe simulation experiments show estimators reasonably reliable practice method illustrated two real examples first uses survival time data patients primary without second concerned times wavelet estimate flexibility provides new interesting interpretation
expert casebased reasoner learning adapt prior cases experience plays important role development human expertise one computational model experience expertise provided research casebased reasoning examines stored cases traces specific prior problemsolving episodes retrieved facilitate new problemsolving much progress made methods relevant cases casebased reasoning wide acceptance technology developing intelligent systems cognitive model human reasoning process however one important aspect casebased reasoning remains poorly understood process retrieved cases adapted fit new situations difficulty encoding effective adaptation rules hand widely serious development fully autonomous casebased reasoning systems consequently important question casebased reasoning systems might learn improve expertise case adaptation present framework acquiring expertise using combination general adaptation rules introspective reasoning casebased reasoning case adaptation task
computer evolution objects evolutionary design computers
modular neural network models strong evidence face processing brain localized face recognition occurring brain damage visual object difficulty recognizing kinds complex objects indicates face object recognition may partially independent neural mechanisms chapter use computational models show face processing specialization underlying visual object relatively simple competitive selection mechanism development neural resources tasks best performing developing need perform classification identification faces early low visual inspired arguments factors like bias visual system develop specialized face processor experiments mixtures experts modeling paradigm provide preliminary computational theory accounts face object processing present two feedforward computational models visual processing models selection mechanism gating network competition modules attempting classify input stimuli model modules simple unbiased classifiers competition sufficient achieve enough specialization one module models face recognition object recognition module models object recognition face recognition model however bias modules providing one low spatial frequency information high spatial frequency information case models task classification faces classification objects low spatial frequency network shows even stronger specialization faces combination tasks inputs shows strong specialization take results support idea face processing module arise natural consequence developmental environment without specified
robust learning missing data
relational knowledge discovery databases paper indicate possible applications ilp similar techniques knowledge discovery field discuss several methods adapting relational database systems proposed methods range pure ilp based techniques ilp show easy adapt way
incremental pruning simple fast exact algorithm partially observable markov decision processes exact algorithms general pomdps use form dynamic programming convex representation one value function transformed another examine variations incremental pruning approach solving problem compare earlier algorithms theoretical empirical find incremental pruning efficient algorithm solving pomdps
similar classifiers error bounds improve error bounds based analysis classes sets similar classifiers apply new error bounds separating artificial neural networks key words machine learning learning theory generalization vapnikchervonenkis separating neural networks
gene structure prediction linguistic methods higherorder structure genes features biological sequences described means formal grammars grammars used generalpurpose detect structures means syntactic pattern recognition describe grammar genes measures effective current connectionist combinatorial algorithms predicting gene structures sequence database parameters grammar rules optimized several different species mixing experiments performed determine degree species specificity relative importance compositional syntactic components gene prediction
learning specialization face recognition effect spatial frequency face recognition occurring brain damage visual object difficulty recognizing kinds complex objects indicates face object recognition may partially independent mechanisms brain result competitive learning mechanism development neural resources tasks best performing studies normal performance face object recognition tasks seem indicate face recognition primarily involving low spatial frequency information present stimulus relatively large whereas object recognition primarily involving analysis objects parts using local high spatial frequency information feedforward computational model visual processing two modules classify input stimuli one module receives low spatial frequency information receives high spatial frequency information module shows strong specialization face recognition combined face classification task series experiments shows discrimination necessary distinguishing members class faces relies heavily low spatial frequencies present stimulus
combining exploratory projection pursuit projection pursuit regression application neural networks present novel classification regression method combines exploratory projection pursuit unsupervised training projection pursuit regression supervised training yield new family penalty terms improved generalization properties demonstrated real world problems
objective function formulation bcm theory visual cortical plasticity statistical connections stability conditions paper present objective function formulation bcm theory visual cortical plasticity permits demonstrate connection unsupervised bcm learning procedure various statistical methods particular projection pursuit formulation provides general method stability analysis fixed points theory enables analyze behavior evolution network various visual conditions also allows comparison many existing unsupervised methods model shown successful various applications object recognition thus possibly highly significant result biological neuron performing sophisticated statistical procedure
face recognition using hybrid neural network system automatic face recognition presented consists several steps automatic detection eyes followed spatial normalization images classification images carried hybrid supervised unsupervised neural network two methods reducing overfitting common problem high dimensional classification schemes presented superiority combination demonstrated
statistical basis using radial basis functions process control radial basis function rbf neural networks offer attractive equation form use modelbased control approximate highly nonlinear yet suited linear adaptive control show mixtures gaussians allows application many statistical tools including algorithm parameter estimation resulting models give uncertainty estimates beyond region training data available
modeling optimization tasks cases framework modeling optimization task domains domains neither systems human experts possess exact model optimization users model optimality optimizes solution iterative revision using casebased reasoning task structure analysis creating initial model optimization task generic found analysis specialized case feature descriptions application problems extensive experimentation job shop scheduling problems shown improve model cases
balancing may explain cortical spiking five related factors identified enable single model neurons random synaptic input similar seen cortical suggest cortical neurons may operate parameter regime synaptic intrinsic detailed correlations inputs please comments reference paper technical report institute neural computation san
genetic encoding strategies neural networks application genetic algorithms neural network optimization produced active field research paper proposes classification encoding strategies also gives critical analysis idea evolving artificial neural networks genetic algorithms based powerful evolution human brain mechanism developed highest form intelligence known scratch inspired great deal research activities instance increasing amount research reports journal papers published topic generating growing field researchers variety different techniques encode neural networks increasing complexity young field driven mostly small research groups paper will attempt analyse structure already performed work point shortcomings approaches current state development
object recognition using unsupervised bcm network usefulness distinguishing features propose object recognition scheme based method feature extraction level images corresponds recent statistical theory called projection pursuit derived biologically motivated feature extracting neuron evaluate performance method use set detailed psychophysical object recognition experiments
nonlinear wavelet shrinkage bayes rules bayes factors wavelet method proposed work simple efficient way data wavelet coefficients proposed several optimality criteria asymptotic minimax crossvalidation criteria paper wavelet shrinkage natural properties bayesian models data proposed performance methods tested standard test functions key words discrete wavelet transform thresholding bayes model subject classification
reliable models data clearning paper introduces idea clearning simultaneously data learning underlying structure step viewed topdown processing model modifies data learning step viewed bottomup processing data modifies model statistical foundation proposed method maximum likelihood perspective apply clearning hard problem benchmark performances known prediction exchange rates difficult test period clearning conjunction pruning yields return outofsample significantly better otherwise identical network trained without network inputs hidden units weights inputs hidden units resulting final architectures obtained clearning pruning overfitting even noisy problems since data allow simpler model competitive performance clearning gives insight data show estimate overall ratio input variable show error estimates pattern used detect remove replace missing corrupted data values clearning used nonlinear regression classification problem
performance enhancement decision graphs
approach contextsensitive correction large class problems natural language require characterization linguistic context two characteristic properties problems feature space high dimensionality target concepts depend small subset features space conditions multiplicative algorithms shown good theoretical properties work reported present algorithm combining variants voting apply problem class contextsensitive correction task errors happen result valid words causal evaluate algorithm comparing method representing state task find run full set features achieves significantly higher able achieve either condition compared systems literature exhibits highest performance several aspects architecture contribute superiority primary factor able learn better linear learns run test set drawn different training set drawn better able adapt using strategy will present combines supervised learning training set unsupervised learning noisy test set
geometric hybrid markov chains various notions geometric markov chains general state spaces exist paper review certain relations implications among apply results collection chains commonly used markov chain monte carlo simulation algorithms socalled hybrid chains prove certain conditions hybrid chain will geometric constituent parts acknowledgements thank number useful comments regarding spectral theory central limit theorems thank gibbs peter helpful discussions thank many excellent suggestions
polynomialtime algorithm phylogeny problem number character states fixed dimacs technical report march
methodology strategy optimization uncertainty extended twodimensional problem
avoiding overfitting locally matching noise level data gating network discovers trying future behavior realworld system two key problems process regime switching overfitting model particularly serious noisy processes shows gated experts point solutions problems architecture also called experts mixture experts consists nonlinear gating network several nonlinear competing experts expert learns conditional mean usual expert also adaptive width gating network learns assign probability expert depends input article first discusses assumptions underlying architecture derives weight update rules evaluates performance gated experts comparison single networks networks two outputs one predicting mean one local error article also investigates ability gated experts discover characterize underlying results significantly less overfitting compared single nets two reasons subsets potential inputs given experts gating network less dimensionality experts learn match local noise levels thus learning article focuses architecture overfitting problem applications problem data santa competition given application realworld problem predicting given much data support
learning bayesian prototype trees simulated annealing given set samples unknown probability distribution study problem constructing good bayesian network model probability distribution question task viewed search problem goal find maximal probability network model given data work make attempt learn arbitrarily complex bayesian network structures since resulting models practical purposes due exponential amount time required reasoning task instead restrict special class simple treestructured bayesian networks called bayesian prototype trees polynomial time algorithm bayesian reasoning exists show probability given bayesian prototype tree model evaluated given data evaluation criterion used stochastic simulated annealing algorithm searching model space simulated annealing algorithm provably finds maximal probability model provided sufficient amount time used
technique searching beyond failure many applications decision support planning scheduling etc one needs express requirements partially satisfied order express requirements propose technique called intuitively kind dual program globally fails find solution new execution program point state forward computation tree search technique applied constraint logic programming obtaining powerful extension useful properties original scheme report successful practical application evolutionary training constrained neural networks
fails local search topology local search algorithms combinatorial search problems frequently sequence states impossible improve value objective function moves regions called moves time local search analyze characterize three different classes randomly generated boolean satisfiability problems identify several interesting features impact performance local search algorithms show local minima tend small may large also show local minima without large number clauses systematically searching may computationally expensive local minimum large show called tend much larger minima states local search use show solutions global minima randomly generated problem instances form clusters behave similarly local minima several local search algorithms explain performance light results finally discuss strategies creating next generation local search algorithms
solving temporal binding problem neural theory constructing updating object visual objects parts correctly identified integrated neural network theory proposed seeks explain human visual system together visual properties space time multiple objects problem known temporal binding problem proposed theory based upon neural mechanisms construct update object representations interactions serial mechanism location selection grouping mechanisms associative memory structure together object form spatial information working model presented provides unified quantitative explanation results psychophysical experiments object review object integration tracking
behavior genetic algorithms using expected values bit products consider two methods genetic algorithms first method based expected values bit products second method expected values products proportional selection mutation uniform crossover applications obtain results stable points fitness schemata
evolutionary approach time constrained routing problems routing problems important class planning problems usually many different constraints optimization criteria involved difficult find general methods solving routing problems propose evolutionary solver planning problems instance solver tested specific routing problem time constraints performance evolutionary solver compared biased random solver biased solver results show evolutionary solver performs significantly better two
cooperative casebased reasoning investigating possible modes cooperation among homogeneous agents learning capabilities paper will focused agents learn solve problems using casebased reasoning cbr will present two modes cooperation among distributed casebased reasoning collective casebased reasoning illustrate modes application different cbr agents able techniques protein approach taken extend representation language used cbr agents knowledge modeling framework designed integrate learning methods based decomposition principle extension present allows communication cooperation among agents implemented means three basic constructs references method evaluation mobile methods
casebased probability factoring bayesian belief networks bayesian network inference formulated combinatorial optimization problem concerning computation optimal factoring distribution represented net since determination optimal factoring computationally hard problem heuristic greedy strategies able find approximations optimal factoring usually present paper investigate alternative approach based combination genetic algorithms casebased reasoning cbr show use genetic algorithms improve quality computed factoring case static strategy used computation combination cbr still provide advantages case dynamic strategies preliminary results different kinds nets reported
symbolic complexity analysis connectionist algorithms machines paper attempts determine computation communication requirements connectionist algorithms running machine strategy involves key connectionist algorithms highlevel objectoriented language extracting running times analyzing determine algorithms space time complexity results presented various implementations backpropagation algorithm
adaptive lookahead planning problem finding good initial plans solved use present new adaptive connectionist planning method interaction environment world model constructed using backpropagation learning algorithm planner constructs lookahead plan iteratively using model predict future future reinforcement derive suboptimal plans thus determining good actions directly knowledge model network level done gradient descent action space
proceedings conference uncertainty artificial seattle technical report abstract evaluation counterfactual queries true true important fault diagnosis planning determination paper present methods computing probabilities queries using formulation proposed pearl query interpreted external action forces true prior probability available causal mechanisms domain counterfactual probabilities evaluated precisely however causal knowledge specified conditional probabilities bounds computed paper develops techniques evaluating bounds demonstrates use two applications determination treatment studies subjects may choose treatment determination
bayesian networks
logistic response projection pursuit
efficient superscalar performance boosting goal superscalar processor design increase performance exploitation parallelism ilp previous studies shown speculative execution required high instruction per cycle rates applications general toward supporting speculative execution complicated processors performance though just high rate also depends upon instruction cycle time boosting technique supports general speculative execution simpler processors boosting labels speculative instructions control dependence information control dependence constraints instruction scheduling still providing full dependence information hardware incorporated boosting global scheduling algorithm exploits ilp without instruction program use algorithm estimates boosting hardware involved evaluate much speculative execution support necessary achieve good performance find superscalar processor using minimal implementation boosting easily reach performance much complex superscalar processor
minimum feature set problem paper appeared neural networks
decisiontheoretic casebased reasoning technical report
clustering learning tasks selective transfer knowledge research sponsored part national science foundation award laboratory systems center air force advanced research projects agency grant number views conclusions contained document author interpreted necessarily representing policies either expressed implied nsf laboratory states government
utility classification problem investigate application classification techniques utility decision problem two sets parameters must generally probabilities prior conditional probabilities model change user user utility models thus necessary utility model separately new user long particularly outcome space large two common approaches utility function first base determination users utility function solely qualitative preferences second makes assumptions form utility function take different approach attempt identify new users utility function based classification relative database previously collected utility functions identifying clusters utility functions minimize appropriate distance measure identified clusters develop classification scheme requires many fewer simpler full utility robust utility based solely preferences tested algorithm small database utility functions diagnosis domain results quite promising
ensemble learning hidden markov models standard method training hidden markov models optimizes point estimate model parameters estimate viewed maximum posterior probability density model parameters may overfitting contains parameter uncertainty also maximum may posterior probability distribution paper study method optimize ensemble approximates entire posterior probability distribution ensemble learning algorithm requires traditional training algorithm hidden markov models expectationmaximization algorithm known algorithm maximum likelihood method simple modification maximum likelihood method viewed maximizing posterior probability density model parameters recently hinton van developed technique known ensemble learning see also review whereas maximum methods optimize point estimate parameters ensemble learning ensemble optimized approximates entire posterior probability distribution parameters objective function optimized variational free energy measures relative entropy approximating ensemble true distribution paper derive test ensemble learning algorithm hidden markov models building resources traditional algorithm
neural model serial processing visual tracking quantitative model provided psychophysical data tracking multiple visual elements tracking model employs mechanism constructing updating object representations model neural activations construct update internal representations objects changes synaptic weights correspondence problem items memory elements visual input combination topdown prediction signals bottomup grouping processes simulations model image sequences used tracking experiments show reported results consistent serial tracking mechanism based psychophysical findings addition simulations show observed effects perceptual grouping tracking accuracy may result interactions predictions object location motion grouping processes involved solving motion correspondence problem
data value prediction methods performance
adaptive wavelet control nonlinear systems paper considers design analysis adaptive wavelet control algorithms uncertain nonlinear dynamical systems lyapunov synthesis approach used develop adaptive control scheme based wavelet network models stability results obtained key assumption system uncertainty satisfies matching condition localization properties adaptive networks discussed formal definitions interference localization measures proposed
temporal differences efficient implementation reinforcement learning temporal difference methods class methods learning predictions prediction problems parameterized factor currently important application methods temporal credit assignment reinforcement learning known reinforcement learning algorithms qlearning may viewed instances learning paper examines issues efficient general implementation arbitrary use reinforcement learning algorithms optimizing discounted sum rewards traditional approach based traces argued suffer lack generality temporal differences procedure proposed alternative indeed approximates requires little computation per action used arbitrary function representation methods idea derived fairly simple new probably far encouraging experimental results presented suggesting using procedure allows one obtain significant learning speedup essentially cost usual td0 learning
toward learning systems integrate different strategies representations
incremental polynomial controller networks two selforganising nonlinear controllers
mining causes cancer machine learning experiments various levels detail paper presents methodological point view first results project scientific data mining analyze data derived program longterm research study performed national institute environmental health sciences database contains detailed descriptions tests performed compounds animals different species chemical structures described level terms various relevant structural properties goal paper investigate effects various levels detail amounts information resulting hypotheses apply relational propositional machine learning algorithms learning problems formulated regression classification tasks addition experiments conducted two learning problems different levels detail experiments indicate additional information necessarily improves accuracy number potential made algorithm relational regression abstract details contained relations database
efficient implementation gaussian processes neural networks bayesian inference provide useful framework within solve regression problems however means bayesian analysis neural networks difficult paper investigate method regression using gaussian process priors allows exact bayesian analysis using matrix discuss method detail will also detail range mathematical numerical techniques useful applying gaussian processes general problems including efficient approximate matrix methods developed
please prototype learning system using genetic algorithms prototypes proposed representation concepts used effectively humans developing computational schemes generating prototypes examples however proved difficult problem present novel genetic algorithm based prototype learning system please constructing appropriate prototypes classified training instances constructing set prototypes possible classes class new input instance determined nearest prototype instance attributes assumed ordinal nature prototypes represented sets pairs genetic algorithm used evolve number prototypes per class positions input space present experimental results series artificial problems varying complexity please performs several nearest neighbor classification algorithms problem set analysis strengths weaknesses initial version system need additional operators operators substantially improves performance system particularly difficult problems
worstcase identification nonlinear memory systems paper problem asymptotic identification memory systems presence bounded noise studied experiment worstcase error characterized terms worstcase uncertainty set optimal inputs minimize uncertainty studied characterized finally convergent algorithm require knowledge noise upper bound algorithm based data spline functions shown suited identification presence bounded noise basis functions
combining connectionist symbolic learning refine rule bases paper describes system revising probabilistic knowledge bases combines connectionist symbolic learning methods uses modified version backpropagation refine factors probabilistic rule base uses heuristic add new rules results refining three actual expert knowledge bases demonstrate combined approach generally performs better previous methods
choosing learning strategies achieve learning goals open world applications number techniques may potentially apply given learning situation research presented illustrates complexity involved automatically choosing appropriate technique multistrategy learning system also step toward general computational solution selection problem approach selection separate planning problem set goals case ordinary therefore management pursuit learning goals becomes central issue learning similar problems associated traditional planning systems paper explores issues problems possible solutions framework examples presented multistrategy learning system called
financial markets present empirical evidence considering stochastic process requiring generalization standard model constant use previous development statistical mechanics financial markets model issues
plausibility measures users guide examine new approach modeling uncertainty based plausibility measures plausibility measure just event plausibility element partially ordered set approach easily seen generalize approaches modeling uncertainty probability measures belief functions possibility measures lack structure plausibility measure makes easy add structure needed basis examine required ensure plausibility measure certain properties interest gives insight essential features properties question allowing prove general results apply many approaches reasoning uncertainty plausibility measures already proved useful analyzing default reasoning paper examine algebraic properties use probability theory understanding properties will essential plausibility measures used practice representation tool
temporal abstractions preprocessing monitoring time series paper describe number intelligent data analysis techniques analyze data monitoring patients particular show combination temporal abstractions statistical probabilistic techniques may applied derive useful patients behaviour certain monitoring period finally describe intelligent data analysis methods may used index past cases perform casebased database past cases
framework multipleinstance learning multipleinstance learning variation supervised learning task learn concept given positive negative instances may contain many instances labeled positive even one instances within concept labeled negative instances negative describe new general framework called diverse density solving multipleinstance learning problems apply framework learn simple description series images containing stock selection problem drug activity prediction problem
generalized approximate cross validation smoothing splines data
language series complexity hypothesis spaces ilp restrictions number depth variables defined language series widely used ilp expected produce considerable reduction size hypothesis space paper show generally case lower bounds present lead intractable hypothesis spaces except domains argue parameters chosen bias shift operations propose alternative approaches resulting desired reduction hypothesis space allowing natural integration shift bias
role learning paper discussion relationship learning analysis learning carried argued knowledge sometimes negative value series experiments involving program learns state spaces described shown knowledge acquired negative value even though correct acquired solving similar problems shown value knowledge depends known random sometimes lead substantial improvements performance research knowledge acquisition take possibility knowledge may sometimes view taken learning complementary processes construct maintain useful representations experience
sparse image codes using efficient coding framework apply general technique learning bases problem finding efficient image codes bases learned algorithm localized oriented consistent earlier results obtained using related methods show learned bases structure higher degrees produce greater sampling density position orientation scale efficient coding framework provides method comparing different bases calculating probability given observed data measuring entropy basis function coefficients compared complete fourier wavelet bases learned bases much better coding efficiency demonstrate improvement representation learned bases showing superior performance image missing
turing neural nets paper shows existence finite neural network made sigmoidal neurons universal turing machine composed less evolving processors linearly connections required
genetic programming estimates complexity paper problem complexity related binary strings propose genetic programming approach consists evolving population programs optimal program generates given string evolutionary approach overcome intractable space time difficulties occurring methods perform approximation complexity function experimental results quite significant also show interesting computational strategies proving effectiveness implemented technique
exploiting data inductive inference algorithms learners work effectively training data contain completely specified labeled samples many diagnostic tasks however data will include values attributes model process values attributes learner remove values critical attributes learner paper instead focuses remove attribute values values needed classify instance given values attributes first model demonstrate useful showing certain classes seem hard learn general pac model decision trees trivial learn setting even learned manner robust classification noise also discuss model extended deal theory revision modifying existing decision tree complex attributes correspond combinations attributes include values required values hypothesis classes dnf formulae paper already accepted currently review journal another conference will submitted review period extended version paper appeared working notes fall symposium relevance new november authors helpful comments
casebased approach reactive control autonomous robots propose casebased method selecting behavior sets addition traditional reactive robotic control systems new system case based reactive robotic system provides flexible performance novel environments standard hard problem reactive systems box additionally designed manner intended close pure reactive control possible higher level reasoning memory functions minimum result new reasoning significantly slow system pure reactive
growing simpler decision trees facilitate knowledge discovery using machine learning techniques knowledge discovery output comprehensible human important predictive accuracy introduce new algorithm improves comprehensibility decision trees standard c45 without reducing accuracy using genetic search select set input features c45 allowed use build tree test wide variety realworld datasets show trees significantly smaller reference significantly fewer features trees c45 without using statistical significance tests show trees either accurate original c45 trees ten datasets tested
using bayesian networks incorporating probabilistic priori knowledge boltzmann machines present method automatically determining structure connection weights boltzmann machine corresponding given bayesian network representation probability distribution set discrete variables resulting boltzmann machine structure implemented efficiently massively parallel hardware since structure two separate clusters nodes one cluster updated simultaneously updating process boltzmann machine approximates gibbs sampling process original bayesian network sense boltzmann machine converges final state gibbs sampler mapping bayesian network boltzmann machine seen method incorporating probabilistic priori information neural network architecture trained existing learning algorithms
linear dependencies represented chain graphs graphical modelling identifying independence bayesian simple algorithm construct consistent extension partially oriented graph computer science department university also technical report cognitive systems laboratory pearl association graphs causal interpretation cognitive systems laboratory technical report november pearl graphs npcomplete technical report cognitive systems laboratory
learning accurate representation system default theory different answers certain queries identify theory set related theories produces single response query total ordering defaults goal identify theory optimal expected accuracy natural distribution queries domain two obvious first expected accuracy theory depends query distribution usually known second task identifying optimal theory even given distribution information intractable paper presents method problems using set samples estimate unknown distribution hillclimbing local optimum particular given error confidence parameters ffi produces theory whose expected accuracy probability least
mdl learning probabilistic neural networks discrete problem domains given problem casebased reasoning cbr system will search case memory use stored cases find solution possibly modifying retrieved cases adapt required input specifications discrete domains cbr reasoning based rigorous bayesian probability propagation algorithm bayesian cbr system implemented probabilistic feedforward neural network one layers representing cases paper introduce minimum description length mdl based learning algorithm obtain proper network structure associated conditional probabilities algorithm together resulting neural network implementation provide massively parallel architecture solving efficiency casebased reasoning
nonlinear trading models ratio maximization working paper school new york university decision financial engineering proceedings international conference neural networks markets world scientific many trading strategies based price prediction financial markets typically interested performance ratio rather price predictions paper introduces approach generates nonlinear strategy explicitly ratio expressed neural network model whose output position size iterative parameter update rules derived compared alternative approaches resulting trading strategy evaluated analyzed data real world data daily index trading based ratio maximization compares favorably optimization probability matching optimization results show goal optimizing outofsample achieved nonlinear approach
analysis designed simulated evolution randomized adaptive greedy search using evolutionary algorithms offers powerful approach automated design neural network architectures variety tasks artificial intelligence robotics paper present results evolutionary design robotic robot given task boxes boxes analysis evolved networks show evolution exploits design constraints properties environment produce network structures high fitness conclude brief summary related ongoing research examining environment evolutionary processes determining structure function resulting neural architectures
sequential procedure within evolutionary algorithm coloring problems graphs
defining combining symmetric similarity measures paper present framework definition similarity measures using functions show strengths particularly combining similarity measures investigate particular framework sets used represent objects degrees similarity paper suggesting findings
approach preference decision making investigate solution configuration problems preference function outcomes unknown specified aim system personal computer will optimal given user goal project develop algorithms generate feasible configuration preference queries user order minimize number complexity preference queries user algorithm reasons users preferences taking account constraints set feasible configurations assume user structure preferences particular way natural many settings exploited optimization process also address preliminary fashion tradeoffs computational effort solution problem degree interaction user
combination supervised unsupervised learning reducing overall error measure classifier
abstract input examples basis performance failure powerful bias learning systems definition learning bias however typically restricted bias provided input language hypothesis language preference criteria competing concept hypotheses bias taken context basis provides preference one concept change another paradigm processing indeed provides bias bias exhibited selection examples input stream examples failure successful performance show degrees freedom less learning learning learning constraint also definition failure provide novel taxonomy failure causes illustrate interaction multistrategy learning system called
gamma mlp using multiple temporal resolutions improved classification previously introduced gamma mlp defined mlp usual synaptic weights gamma filters associated gain terms layers paper apply gamma mlp larger scale speech recognition problem analyze operation network investigate gamma mlp perform better alternatives gamma mlp capable employing multiple temporal resolutions temporal resolution defined per number parameters freedom number variables per unit time gamma memory equal gamma memory parameter detailed paper multiple temporal resolutions may certain problems different resolutions may optimal extracting different features input data problem paper gamma mlp observed use large range temporal resolutions comparison networks typically use single temporal resolution motivation gamma mlp related dimensionality ability gamma mlp trade temporal resolution memory depth therefore increase memory depth without increasing dimensionality network iir mlp general version gamma mlp however iir mlp performs poorly problem paper investigation suggests error surface gamma mlp suitable gradient descent training error surface iir mlp
fast nonlinear dimension reduction present fast algorithm nonlinear dimension reduction algorithm builds local linear model data merging pca clustering based new distortion measure experiments speech image data indicate local linear algorithm produces lower distortion built five layer networks local linear algorithm also order magnitude faster train
nondeterministic human genes
negative observations concerning approximations spaces generated shifts functions approximation shifts basis function considered different methods translates compared argued note superior localization processes employ original translates
optimum decision rule pattern recognition
identification protein coding regions dna molecular cellular developmental biology keywords gene
stationary wavelet transform statistical applications wide potential use statistical contexts discrete wavelet transform reviewed using filter useful subsequently paper stationary wavelet transform sequences stage described two different approaches construction inverse stationary wavelet transform set application stationary wavelet transform exploratory statistical method discussed together potential use nonparametric regression method local spectral density estimation developed involves extensions wavelet context standard time series ideas spectrum technique illustrated application data sets
neural model cortical representation distance
users decision table classifiers users commonly use analyze understand data online analytical processing provides users added flexibility data around different attributes multidimensional machine learning researchers however hypothesis spaces users hyperplanes perceptrons neural networks bayesian networks decision trees nearest neighbors etc paper use decision table classifiers easy users understand describe several variants algorithms learning decision tables compare performance describe visualization mechanism implemented performance decision tables comparable known algorithms yet resulting classifiers use fewer attributes comprehensible
analysis quality monitors using hierarchical time series models management department considerably collection assessment data specific levels quality care resulting time series quality monitors provide information relevant evaluating patterns variability quality care time across care areas compare assess differences across management group developed various models evaluating patterns dependencies combining data across system paper provides brief overview resulting models summary examples three monitor time series discussion data modelling inference issues work introduces new models multivariate time series framework combines hierarchical models population time series structure allow measure associated hierarchical model parameters study components models describe patterns across population relationships among several monitors time series components describe patterns variability time effects relationships across quality monitors additional model components aspects variability quality monitor outcomes care areas discuss model assessment residual analysis mcmc algorithms developed fit models will interest related applications areas
vector system application large problems backpropagation training report development system neural network signal processing applications designed implemented vector processor conventional present performance comparisons commercial neural network backpropagation training system demonstrates significant speedups extensively hand optimization code running
challenge revising theory pure rulebased program will return set answers query will return answer set even rules however program includes prolog cut operators return different answers rules also many reasoning systems return first answer found query first answers depend rule order even pure rulebased systems theory revision algorithm revised whose expected accuracy distribution queries optimal therefore consider modifying order rules paper first shows polynomial number training labeled queries query coupled correct answer provides distribution information necessary identify optimal ordering however task determining ordering optimal given information intractable even trivial situations even query perfect theory rule base propositional also prove task even polynomial time algorithm produce ordering theory whose accuracy within optimal also prove similar results related tasks determining contexts optimal ordering optimal set rules add optimal priority values set defaults
four challenges computational model identifying open research issues field necessary step progress field paper describes four open research problems computational models reasoning relating case representation use modeling selection construction arguments based pairwise case comparison arguments modeling process purposes policies principles used case similarity assessment extending applicability precedents tasks classification
noisy time series prediction using symbolic representation recurrent neural network grammatical inference financial forecasting example signal processing problem challenging due small sample sizes high noise neural networks successful number signal processing applications discuss fundamental limitations inherent difficulties using neural networks processing high noise small sample size signals introduce new intelligent signal processing method addresses difficulties method uses symbolic representation selforganizing map grammatical inference recurrent neural networks apply method prediction daily exchange rates addressing difficulties overfitting priori class probabilities find significant experiments covering different exchange rates method correctly predicts direction change next error rate error rate reduces around examples system low confidence prediction symbolic representation extraction symbolic knowledge recurrent neural networks form deterministic finite state automata automata explain operation system often relatively simple rules related known behavior following mean reversal extracted
dynamic automatic model selection technical report abstract problem learn examples studied history machine learning many successful learning algorithms developed problem received less attention select algorithm use given learning task ability chosen algorithm induce good generalization depends appropriate model class underlying algorithm given task define algorithms model class representation language uses express generalization examples supervised learning algorithms differ underlying model class search good generalization given characterization surprising algorithms find better generalizations tasks therefore order find best generalization task automated learning system must search appropriate model class addition searching best generalization within chosen class thesis proposal investigates issues involved selection appropriate model class presented approach two firstly approach combines different model classes form model combination decision tree allows best representation found learning task secondly model class appropriate determined dynamically using set heuristic rules explicit rule conditions particular model class appropriate done next addition describing approach proposal describes approach will evaluated order demonstrate efficient effective method automatic model selection
competition models development connections development system involves many cases interactions local scale rather execution fully specified genetic problem discover nature interactions factors depend developing example competitive interactions play important role examine possible types competition formal
rule induction instancebased learning unified approach paper presents new approach inductive learning combines aspects instancebased learning rule induction single simple algorithm rise system searches rules fashion starting one rule per training example avoids difficulties approaches evaluating proposed induction step globally efficient procedure equivalent accuracy rule set whole every training example classification performed using strategy reduces generalizations instances extensive empirical study shows rise consistently achieves higher paradigms cn2 also outperforms decisiontree learner c45 test domains
learning one research machine learning focused scenarios learner faces single isolated learning task lifelong learning framework assumes instead learner related learning tasks lifetime providing opportunity transfer knowledge paper studies lifelong learning context binary classification presents approach knowledge via learned model invariances domain results learning recognize objects images demonstrate superior generalization capabilities invariances learned used bias subsequent learning research sponsored part national science foundation award laboratory systems center air force advanced research projects agency grant number views conclusions contained document authors interpreted necessarily representing policies either expressed implied nsf laboratory states government
predicate invention learning positive examples previous bias shift approaches predicate invention applicable learning positive examples complete hypothesis found given language negative examples required determine whether new predicates one approach problem presented system predicate invention guided sequences input clauses positive negative examples general theory contrast searches minimal finitestate automaton generate positive negative sequences uses technique inducing hidden markov models positive sequences enables system new predicates without negative examples another advantage using induction technique allows incremental learning experimental results presented comparing positive learning framework comparing original induction technique new version produces deterministic hidden markov models results show predicate invention may indeed necessary possible learning positive examples keep induced model deterministic
recent ideas utility probability distribution reference
paclearning recursive logic programs efficient algorithms present algorithms learn certain classes recursive logic programs polynomial time equivalence queries particular show single recursive clause learnable programs consisting one learnable recursive clause one clause also learnable additional oracle assumed results imply classes although classes learnable recursive programs constrained shown paper general generalizing either class natural way leads computationally difficult learning problem thus taken together paper paper boundary efficient learnability recursive logic programs
smoothing spline anova application historical global temperature data
lookahead discretization ilp present evaluate two methods improving performance ilp systems one discretization numerical attributes based text adapted extended way cope aspects discretization occur relational learning problems literals occur second technique lookahead wellknown problem ilp learner always assess quality refinement without will without refinement lattice present simple method lookahead used kind lookahead interesting discretization lookahead techniques evaluated experimentally results show techniques improve quality induced theory computational costs acceptable
filtering via simulation auxiliary filters paper analyses recently suggested approach filtering time series suggest algorithm robust two reasons design use discrete support represent updating prior distribution problems paper believe largely solved first problem reduced order magnitude second addition introduce idea filter allows perform online bayesian calculations parameters index models maximum likelihood estimation new methods illustrated using stochastic model time series model
induction determinations paper suggest determinations representation knowledge easy understand briefly review determinations use prediction involves simple matching process describe algorithm uses feature selection construct determinations training data augmented process produce simpler structures report experiments show reduces complexity loss accuracy discuss relation work outline directions future studies
recurrent neural networks learn natural language grammars recurrent neural networks able recurrent neural networks complex parametric dynamic systems exhibit wide range different behavior consider task grammatical inference recurrent neural networks specifically consider task classifying natural language sentences grammatical recurrent neural network made exhibit kind power provided principles parameters linguistic framework government binding theory attempt train network without learned components assumed produce data consider recurrent neural network possess linguistic capability investigate properties williams recurrent networks locally recurrent networks setting show
improve forecasting working paper school new york university journal computational intelligence special issue improving generalization nonlinear financial forecasting models abstract predictive models financial data often based large number plausible inputs potentially combined yield conditional expectation target daily return paper introduces new architecture task output side predict dynamical variables first derivatives different time subsequently combined interaction output layer form several estimates variable interest estimates yield final prediction independently idea input side propose new internal preprocessing layer connected matrix positive weights layer functions weights adapt input learn input apply two ideas real world example daily predictions stock index index compare results network single output new six layer architecture stable training due two facts information back outputs input constraint predicting first second derivatives focuses learning relevant variables dynamics architectures compared training perspective squared errors robust errors trading perspective returns correct ratio
regression shrinkage selection via propose new method estimation linear models minimizes residual sum subject sum absolute value coefficients less constant nature constraint tends produce coefficients exactly zero hence gives models simulation studies suggest properties subset selection ridge regression produces models like subset selection exhibits stability ridge regression also interesting relationship recent work adaptive function estimation idea quite general applied variety statistical models extensions generalized regression models models briefly described
improved heterogeneous distance functions instancebased learning techniques typically handle continuous linear input values often handle nominal input attributes appropriately value difference metric designed find reasonable distance values nominal attribute values largely continuous attributes requiring discretization map continuous values nominal values paper proposes three new heterogeneous distance functions called heterogeneous value difference metric value difference metric value difference metric new distance functions designed handle applications nominal attributes continuous attributes experiments applications new distance metrics achieve higher classification accuracy average three previous distance functions datasets nominal continuous attributes
coding segments genetic programming research utility noncoding segments introns shown evolution solutions domains building blocks crossover consider genetic programming system noncoding segments removed population leads convergence since remove naturally occurring noncoding segments away feature coding segments place modified population method significantly improves learning rate domain considered also show method applied domains
recognizing handwritten digit strings using modular spatiotemporal connectionist networks
evolution iteration genetic programming solution many paper introduces new operation restricted iteration creation automatically genetic programming extends genetic algorithm task automatic programming early work genetic programming demonstrated possible evolve sequence steps single branch main program book genetic programming programming computers means natural selection koza describes extension genetic algorithm genetic population consists computer programs primitive functions see also koza basic form genetic programming single branch evolved genetic programming demonstrated capability discover sequence length content steps sufficient produce satisfactory solution several problems including many problems used years machine learning artificial intelligence applying genetic programming problem user must perform five major steps namely identifying inputs programs identifying primitive functions operations contained programs creating fitness measure evaluating given program solving problem hand choosing certain control parameters population size number generations run determining criterion method result typically individual populations produced run creates restricted
stability chaos two neuron system center studies statistical mechanics added system explore effects stability fixed points system two neuron system one two terms added shown exhibit chaos chaos lyapunov power phase space
method incremental learning application computer detection machine learning paper describes incremental learning method based inductive learning system method maintains representative set past training examples used together new examples appropriately modify currently held hypotheses incremental learning feedback environment user method useful applications involving intelligent agents changing environment active vision dynamic study method applied problem computer detection symbolic learned computer systems users experiments proposed method significant gains terms learning time memory requirements slightly lower predictive accuracy higher concept complexity compared batch learning examples given
adaptation pruning fuzzy inference estimation
empirical studies genetic algorithm noncoding segments genetic algorithm problem solving method process natural selection interested studying specific aspect effect noncoding segments performance noncoding segments segments bits individual provide contribution positive negative fitness individual previous research noncoding segments suggests including structures may improve performance understanding improvement occurs will help use full potential article discuss hypotheses noncoding segments describe results experiments experiments may separated two categories testing program problems previous related studies testing new hypotheses effect noncoding segments
casebased acquisition user preferences solution improvement domains developed approach acquire complicated user optimization criteria use guide
computational modeling spatial attention
concept learning flexible weighting previously introduced model named exploits highly flexible weighting scheme simulations showed records faster learning rates higher asymptotic several artificial categorization tasks models limited abilities input spaces paper extends previous work describes experimental results suggest human subjects also highly flexible schemes particular model provides significantly better models less flexibility hypothesize humans weight attributes depending items location input space need flexible models many theories human concept learning concepts represented prototypes prototype models represent concepts best example central concept new item category relatively similar prototype prototype models relatively great deal information people use concept learning number concept variability features correlations features particular used concept learning
testing generalized linear model hypothesis versus smooth alternatives
ilp noise fixed example size bayesian approach current inductive logic programming systems limited handling noise employ greedy covering approach constructing hypothesis one clause time approach also causes difficulty learning recursive predicates additionally many current systems implicit expectation positive negative examples reflect concept instance space framework learning noisy data fixed example size presented bayesian heuristic finding hypothesis general framework derived approach evaluates hypothesis whole rather one clause time heuristic theoretical properties incorporated ilp system experimental results show noise better foil able learn recursive definitions noisy data systems perform also capable learning positive data also negative data
direction neural networks present general formulation network stochastic units formulation extension boltzmann machine units binary take values range measure appropriate many domains representing values direction phases state unit boltzmann machine described complex variable phase component specifies direction weights also complex variables associate quadratic energy function corresponding probability configuration conditional distribution units stochastic state version gaussian probability distribution known von distribution approximation stochastic phase component units state represents mean direction magnitude component specifies degree associated direction combination value provides additional representational power unit present proof dynamics cause convergence free energy minimum finally describe learning algorithm simulations demonstrate ability learn interesting mappings appear neural networks
end line model known damage right brain cause patients side space condition known represents collection related spatial characterized failure free vision explore stimuli side space recent studies using simple task line conventional diagnostic test proved surprisingly respect spatial involved line patient line paper patients generally far right center extensive studies line conducted length orientation position simulated pattern results using existing computational model visual perception selective attention called already used model data related earlier work damage
models parallel adaptive logic paper proposed architecture adaptive parallel logic asocs adaptive selforganizing concurrent system asocs approach based adaptive network composed many simple computing elements operate parallel asynchronous fashion problem specification given system presenting rules form boolean conjunctions rules added incrementally system adapts changing adaptation data processing form two separate phases operation processing system acts parallel hardware circuit adaptation process distributed computing elements efficiently exploits parallelism adaptation done selforganizing fashion takes place time linear depth network paper summarizes overall asocs concept three specific architectures
genetic algorithms automated tuning fuzzy controllers application describe design tuning controller velocity profile system requires following trajectory rather fixed fuzzy controller tracking velocity profile providing smooth within speed limits use genetic algorithm tune fuzzy controllers performance parameters scaling factors membership functions sequential order significance show approach results controller superior designed one computational effort makes possible automated tuning variety different configurations power configuration
combination rulebased casebased reasoning
neural network patient monitoring algorithm classifier compared baseline algorithm six major databases consisting trained initial records tested additional records algorithm found reduce number errors one channel factor number errors factor conclude patient adaptation provides significant advance classifying normal patient monitoring
comparison systems paper presents experiment comparing new system existing systems three commercial systems bell two variants system two humans works combining rulebased casebased reasoning based idea much easier improve rulebased system adding casebased reasoning tuning rules deal every exception experiment described used set rules adapted case library components required relatively little knowledge engineering found perform almost level commercial systems significantly better two versions work may whole part commercial purpose permission whole part without research purposes provided whole partial copies include following permission electric research laboratories cambridge massachusetts authors individual contributions work applicable portions reproduction purpose require electric research laboratories
predicting ordinal classes ilp paper devoted problem learning predict ordinal ordered discrete classes ilp setting start relational regression algorithm named structural regression trees study various ways firstorder learner ordinal classification tasks combinations algorithm variants several data preprocessing methods compared two ilp benchmark data sets relative strengths weaknesses strategies study tradeoff optimal categorical classification accuracy rate minimum error preliminary results indicate promising towards algorithms combine aspects classification regression relational learning
learning text categorization learning problems text processing domain often map text space whose dimensions measured features text words three characteristic properties domain high dimensionality learned concepts instances feature space high variation number active features instance work study three learning algorithms typical task nature text categorization argue algorithms documents learning linear feature space properties make domain show performance achieved modify algorithms better address specific characteristics domain particular demonstrate variation document length either feature weights using negative weights positive effect applying threshold range training alternatives considering feature frequency benefits features training overall present algorithm variation performs significantly better algorithm tested task using similar feature set
efficient implementation sigmoidal neural nets temporal coding noisy spiking neurons show networks relatively realistic mathematical models biological neurons principle simulate arbitrary feedforward sigmoidal neural nets way previously considered new approach based temporal coding single respectively neurons rather traditional interpretation analog variables terms rates resulting new simulation substantially faster hence consistent experimental results maximal speed information processing cortical neural systems consequence show networks noisy spiking neurons universal approximators sense approximate temporal coding given continuous function several variables result holds fairly large class schemes coding analog variables times spiking neurons new proposal possible organization computations networks spiking neurons systems interesting consequences type learning rules needed explain selforganization networks finally fast implementation sigmoidal neural nets via temporal coding points possible new ways implementing feedforward recurrent sigmoidal neural nets stream
monte carlo approach bayesian regression modeling framework functional response model regression model feedforward neural network estimator nonlinear response function constructed set functional units parameters defining functional units estimated using bayesian approach sample representing bayesian posterior distribution obtained applying markov chain monte carlo procedure namely combination gibbs metropolishastings algorithms method described radial basis function estimators response function general proposed approach suitable finding values parameters complicated parameter space illustrate method numerical examples
information processing pathways model
feature selection means feature weighting approach selecting set features optimal given classification task one central problems machine learning address problem using flexible robust filter technique based feature weighting approach computes binary feature weights therefore solution feature selection sense also gives detailed information feature relevance continuous weights moreover user one several potentially optimal feature subsets important feature selection algorithms since gives flexibility use even complex classifiers application combined approach applied number artificial real world data sets used radial basis function networks examine impact feature subsets classifier accuracy complexity
theoretical models learning learn machine learn biased way typically bias supplied hand example choice appropriate set features however learning machine embedded within environment related tasks learn bias learning sufficiently many tasks environment paper two models bias learning learning learn introduced main theoretical results presented first model model based empirical process theory second hierarchical bayes model
comparison cellular encoding direct encoding genetic neural networks paper compares efficiency two encoding schemes artificial neural networks optimized evolutionary algorithms direct encoding weights priori fixed neural network architecture cellular encoding weights architecture neural network previous studies direct encoding cellular encoding used create neural networks balancing cart fixed track controller cart right cases velocity information pole cart provided input cases network must learn balance single pole without velocity information study behavior systems suggests possible balance single pole velocity information input without learning compute velocity new fitness function introduced forces neural network compute velocity using new fitness function tuning syntactic constraints used cellular encoding achieve speedup previous study solve difficult problem balancing two information velocity provided input
digital neural networks applications requiring parallelism symbolic environments given research models labeled networks models made many simple nodes highly computation takes place data nodes network present models proposed nodes based simple analog functions inputs weights total transformed arbitrary function node learning systems accomplished weights input paper discusses use digital boolean nodes primitive building block connectionist systems digital nodes naturally new paradigms mechanisms learning processing connectionist networks digital nodes used basic building block class models called asocs adaptive selforganizing concurrent systems models combine parallelism ability adapt selforganizing fashion basic features standard neural network learning algorithms proposed using digital nodes compared latter mechanisms lead improved efficiency many applications
focusing construction selection abductive hypotheses many abductive understanding systems explain novel situations process needs beyond generating plausible explanation event explained paper examines relationship standard models abductive understanding casebased explanation model casebased explanation construction selection abductive hypotheses focused specific explanations prior episodes criteria current information needs casebased method inspired observations human explanation events understanding paper focuses methods contributions problems building good explanations domains identify five central issues compare issues addressed traditional casebased explanation models discuss using casebased approach facilitate generation plausible useful explanations domains complex
probability estimation via errorcorrecting output coding previous research shown technique called errorcorrecting output coding dramatically improve classification accuracy supervised learning algorithms learn classify data points one classes paper will extend technique also provide class probability information method supervised learning problem large number supervised learning problems combining results evaluations underlying supervised learning algorithms assumed provide probability estimates problem computing class probabilities formulated system linear equations least methods applied solve equations accuracy reliability probability estimates demonstrated
generalizing learning
towards reactive paper propose reactive able changing situations will explain reinforcement learning used improve control strategy take problem derive solution analytically enables investigate relation parameters resulting approximations will also demonstrate reactive changing situations
simulating quadratic dynamical systems preliminary version quadratic dynamical systems whose definition extends markov chains used model phenomena variety fields like statistical physics natural evolution systems also play role genetic algorithms class heuristics hard analyze recently important step study showing technical assumptions systems converge stationary distribution similar theorems markov chains wellknown show however following sampling problem given initial distribution produce random sample generation result hold restricted classes simple initial distributions thus suggesting complicated markov chains
nonlinear neuron dynamics statistical mechanics complex systems technique using forces nonlinear oscillators analyzed oscillators examined effective neurons model neural networks method shown valid several different circumstances verified analysis power spectrum force energy transfer system
role activity synaptic competition extended version dual constraint model motor presented includes activity dependent independent competition supported wide range recent evidence indicates strong relationship synaptic survival computational model molecular level predictions match developmental behaviour real synapses
learning using group representations extended abstract consider problem learning functions fixed distribution algorithm learns boolean function time polynomial norm fourier transform function show special case general class learning algorithms achieved extending ideas using representations finite groups introduce new classes functions learned using generalized algorithm
bayesian model averaging standard statistical practice model uncertainty data typically select model class models selected model generated data approach uncertainty model selection leading inferences decisions one bayesian model averaging provides coherent mechanism model uncertainty several methods implementing recently discuss methods present number examples examples provides improved outofsample predictive performance also provide currently available software
explanation assist refinement knowledgebased systems
integrated system machine learning discovery programs research
computational environment design design associate computational environment design engine aircraft may used either design new aircraft design new adapt existing aircraft may new developed computer university designers general electric aircraft general electric research development project two principal goals provide useful engineering tool design explore fundamental research issues arise application automated design optimization methods realistic engineering problems
evolutionary heuristic minimum cover problem
new modes generalization perceptual learning learning many visual perceptual tasks motion discrimination shown specific stimulus new stimuli require scratch specificity found many different tasks supports hypothesis perceptual learning takes place early visual cortical areas contrast using novel paradigm motion discrimination learning shown specific found generalization trained subjects discriminate directions moving verified learning transfer trained direction new one however tracking subjects performance across time new direction found rate learning moreover task easy stimulus subjects briefly discriminate easy stimulus new direction generalized difficult stimulus direction generalization brief practice thus learning motion discrimination always generalizes new stimuli learning various forms learning rate indirect transfer direct transfer results challenge existing theories perceptual learning suggest complex learning takes place multiple levels learning biological systems great importance cognitive learning problem solving generalizes analogous problems appear acquire perceptual skills gradually specifically human subjects generalize perceptual discrimination solve similar problems different attributes example discrimination task described subject trained discriminate motion directions ffi ffi use discriminate ffi ffi specificity supports hypothesis perceptual learning neuronal modifications cortical areas visual area contrast previous results specificity will show three experiments learning motion discrimination always generalizes task easy generalizes directions training
learning evolving concepts using approach machine learning inference laboratory paper addresses problem learning evolving concepts concepts whose meaning gradually evolves time solving problem important many applications example building intelligent agents users search active vision automatically updating acquiring users networks requirements learning architecture supporting applications include ability incrementally modify concept definitions accommodate new information fast learning recognition rates low memory needs concept descriptions address requirements propose learning architecture based logic methodology algorithm method uses approach means step learning system current concept descriptions selected representative examples past experience developed method experimentally applied problem computer system detection results show significant advantages method learning speed memory requirements decreases predictive accuracy concept simplicity compared traditional learning training examples provided
toward simulated evolution iteration use simulated evolution search genetic programming automatic synthesis small iterative programs integer register machine addition instruction arithmetic operator show genetic programming produce exact general routines necessary iterative control structures primitive instructions program representation virtual register machine arbitrary control flow evolution strategy furthermore restrict synthesis control structure place upper bound program evaluation time programs fitness distance output produced test case desired output test cases cover finite subset natural numbers yet derived solutions general positive problem simulated evolution crossover operator examines significantly fewer individuals finding solution random search introduction small rate mutation increases number solutions
learning play games experience application artificial neural networks temporal difference learning
interactive genetic programming system music generation automated fitness technical report abstract paper present system interactive system allows users evolve short musical sequences using interactive genetic programming extensions aimed making system fully automated basic works using genetic programming algorithm small set functions creating musical sequences user interface allows user rate individual sequences user interactive technique possible generate runs individuals generations user interactive systems system takes data users run uses train neural network based automatic replace user runs using able make runs generations individuals per generation best run generated general generated user interactive runs
bayesian induction features temporal domains concept induction algorithms process concept instances described terms properties constant time temporal domains instances best described terms properties whose values vary time data engineering called upon temporal domains transform raw data appropriate form concept induction investigate method inducing features suitable classifying finite univariate time series unknown deterministic processes noise supervised setting induce appropriate complexity characterize data class using bayesian model induction principles study evaluate proposed method empirically domain classification problem originally presented cart book compared classification accuracy proposed algorithm accuracy c45 various noise levels feature induction improved classification accuracy noisy situations noise results demonstrate value proposed method presence noise shared classifiers using generative rather models sensitivity model
cbr documents project article present casebased approach flexible query systems two different application areas supports technical diagnosis field system project use cbr system document retrieval industrial setting objective systems knowledge stored less structured documents internal case memory implemented case retrieval net allows handle large case bases efficient retrieval process order provide user access server model combined web interface
automated fitness system
using local trajectory speed global optimization dynamic programming dynamic programming provides methodology develop planners controllers nonlinear systems however general dynamic programming computationally intractable developed procedures allow complex planning control problems solved use second order local trajectory optimization generate locally optimal plans local models value function derivatives maintain global consistency local models value function locally optimal plans actually globally optimal resolution search procedures
parallel transfer task knowledge using dynamic learning rates based measure
limits control flow parallelism paper discusses three techniques useful constraints control flow parallelism control dependence analysis executing multiple control simultaneously speculative execution evaluate techniques using trace simulations find limits parallelism machines employ different combinations techniques three major results first local regions code limited parallelism control dependence analysis useful extracting global parallelism different parts program second superscalar processor limited execute independent regions code higher performance obtained machines machines simultaneously follow multiple control finally without speculative execution allow instructions execute control amounts parallelism obtained programs complex control flow
learning heuristics logic programs applications speedup learning language acquisition paper presents general framework learning heuristics logic programs used improve efficiency accuracy knowledgebased systems expressed logic programs approach combines techniques explanationbased learning recent advances inductive logic programming learn heuristics guide program execution two specific applications framework detailed dynamic optimization prolog programs improving efficiency natural language acquisition improving accuracy area program optimization prototype system able transform intractable specifications polynomialtime algorithms outperforms competing approaches several benchmark speedup domains prototype language acquisition system also described capable automatically acquiring semantic grammars uniformly syntactic semantic constraints parse sentences representations initial experiments show approach able construct accurate generalize novel sentences significantly outperform previous approaches learning mapping based connectionist techniques extensions general framework specific applications plans evaluation also discussed
relative loss bounds multidimensional regression problems study online generalized linear regression multidimensional outputs neural networks multiple output nodes hidden nodes allow final layer transfer functions function need consider linear activations output neurons use distance functions certain kind two completely independent roles deriving analyzing online learning algorithms tasks use one distance function define matching loss function possibly multidimensional transfer function allows generalize earlier results onedimensional multidimensional outputs use another distance function tool measuring progress made online updates shows previously studied algorithms gradient descent gradient fit common framework evaluate performance algorithms using relative loss bounds compare loss online best offline predictor relevant model class thus completely eliminating probabilistic assumptions data
knowledgebased programs robustness automated design systems automated design optimization complex realworld objects principle constructed combining numerical routines existing analysis simulation programs unfortunately analysis codes frequently use automated design may large classes input locally highly sensitive control parameters useful analysis programs must modified reduce eliminate behaviors without desired computation direct modification programs costly implemented highlevel language runtime environment allow strategies incorporated existing analysis programs preserving computational integrity approach relies globally execution programs level functions computation affected problems detected problem handling procedures constructed knowledge base generic problem management strategies show approach effective improving analysis program robustness design optimization performance domain conceptual design engine
sample complexity weakly learning paper study sample complexity weak learning much data must collected unknown distribution order extract small significant advantage prediction show important distinguish learning algorithms output deterministic hypotheses output randomized hypotheses prove weak learning model algorithm using deterministic hypotheses weakly learn class vapnikchervonenkis dimension requires examples contrast randomized hypotheses allowed show examples cases show exists efficient algorithm using deterministic hypotheses weakly learns distribution set size examples thus class symmetric boolean functions variables strong learning sample complexity sample complexity weak learning using deterministic hypotheses sample complexity weak learning using randomized hypotheses next prove existence classes sample size required obtain advantage prediction random essentially equal required obtain arbitrary accuracy finally class small circuits namely parity functions subsets boolean variables prove weak learning sample complexity bound holds even weak learning algorithm allowed replace random sampling membership queries target distribution uniform
convergence results approach abstract expectationmaximization algorithm iterative approach maximum likelihood parameter estimation jordan recently proposed algorithm mixture experts architecture jordan hinton hierarchical mixture experts architecture jordan showed empirically algorithm architectures yields significantly faster convergence gradient current paper provide theoretical analysis algorithm show algorithm variable metric algorithm searching direction positive projection gradient log likelihood also analyze convergence algorithm provide explicit expression convergence rate addition describe technique yields significant speedup simulation experiments report describes research done brain cognitive sciences center biological computational learning artificial intelligence laboratory massachusetts institute technology support provided part grant nsf asc9217041 support laboratorys artificial intelligence research provided part advanced research projects agency defense authors supported grant foundation grant atr human information processing research laboratories grant siemens corporation grant national science foundation grant office naval research nsf grant support intelligent control mit michael jordan nsf young
associative reinforcement learning generate test algorithm agent must learn act world trial error faces reinforcement learning problem quite different standard concept learning although good algorithms exist problem general case often quite inefficient exhibit generalization one strategy find restricted classes action policies learned efficiently paper strategy developing algorithm online search space action mappings expressed boolean formulae algorithm compared existing methods empirical trials shown good performance
integrated diagnostic system combining casebased abductive reasoning aim paper describe system diagnostic architecture combining casebased reasoning abductive reasoning exploiting adaptation solution old episodes order focus reasoning process domain knowledge represented via logical model basic mechanisms based abductive reasoning consistency constraints defined solving complex diagnostic problems involving multiple modelbased component case memory adaptation mechanisms developed order make diagnostic system able exploit past experience solving new cases heuristic function proposed able solutions associated retrieved cases respect adaptation effort needed transform solutions possible solutions current case will discuss preliminary experiments showing validity heuristic solving new case adapting retrieved solution rather solving new problem scratch
learning complex boolean functions algorithms applications commonly used neural network models suited direct digital implementations node needs perform large number operations floating point values ability learn examples generalize restricted networks type indeed networks node implements simple boolean function boolean networks designed way exhibit similar properties two algorithms generate boolean networks examples presented results show algorithms generalize class problems compact boolean network descriptions techniques described general applied tasks known characteristic two examples applications presented image reconstruction handwritten character recognition
control systems robot implementation memorybased learning paper explores issues involved implementing robot learning challenging dynamic task using case study robot use memorybased local model ing approach locally weighted regression represent learned model task performed statistical tests given examine uncertainty model optimize quality deal noisy corrupted data develop exploration algorithm explicitly deals prediction accuracy requirements ing using combination methods optimal control robot achieves fast real time learning task within trials address authors massachusetts institute technology artificial intelligence laboratory department brain cognitive sciences technology square email support provided air force office research siemens support first author provided foundation von support second author provided foundation young award thank ing first version van build ing robot implementing first version learning
adaptation genetic algorithms engineering design optimization genetic algorithms extensively used different domains means global optimization simple yet reliable manner however realistic engineering design optimization domains observed simple classical implementation based binary encoding bit mutation crossover sometimes inefficient reach global optimum using floating point representation alone eliminate problem paper describe way new operators strategies take advantage structure properties engineering design domains empirical results initially domain conceptual design aircraft domain high performance design demonstrate formulated significantly better classical terms efficiency reliability
discovering structure continuous variables using bayesian networks study bayesian networks continuous variables using nonlinear conditional density estimators demonstrate useful structures extracted data set way present sampling techniques belief update based
minimax risk error key words minimax decision theory consider estimating mean vector data norm loss known lie large ratio minimax linear risk minimax risk arbitrarily large obvious exceptions limiting ratio arguments mostly indirect involving reduction univariate bayes minimax problem simple nonlinear threshold rules asymptotically minimax small within bounded factor asymptotic general results basic theory estimation besov spaces
efficient visual search connectionist solution searching objects scenes natural task people extensively studied paper examine task connectionist perspective computational complexity arguments suggest parallel feedforward networks perform task efficiently one difficulty order distinguish target combination features must associated single object often called binding problem requirement presents serious connectionist models visual processing multiple objects present psychophysical experiments suggest people use visual attention get around problem paper describe plausible system uses focus attention mechanism target objects strategy combines topdown bottomup information used minimize search time behavior resulting system matches time behavior people several interesting tasks
inverting implication small training sets present algorithm inducing recursive clauses using inverse implication rather inverse resolution underlying generalization method approach applies class logic programs similar class primitive recursive functions induction performed using small number positive examples need along resolution path algorithm implemented system named matched lists generating terms determine pattern decomposition exhibited target recursive clause theoretical analysis defines class logic programs approach complete described terms characteristic ilp approaches current implementation considerably faster previously reported present evidence demonstrating given randomly selected inputs increasing number positive examples increases accuracy reduces number outputs relate approach similar recent work inducing recursive clauses
behavior linear game game important domain study coevolution robust adaptive behavior behavior nevertheless potential game largely due methodological coevolutionary simulation raised versions game optimal solutions respect solution space lack rigorous metric agent behavior characterize behavior turn coevolutionary dynamics present new formulation rigorous measure agent behavior system dynamics game twodimensional plane onedimensional time step generates bit must simultaneously predict behavior expressed time series employ information theory provide quantitative analysis agent activity version onto component pursuit behavior providing serial channel open world via coevolution results show changes game determine whether affect dynamics
parametric design problem solving aim paper understand involved parametric design problem solving order achieve goal paper identify detail conceptual elements defining parametric design task specification illustrate elements interpreted design process iii formulate generic model parametric design problem solving number problem solving methods terms proposed generic model show enables provide precise account different expressed methods design constructing means design process creative sense design process produces new solution opposed selecting solution predefined set recognizing essential creative elements present design process researchers restrict use term creative design design applications design elements target selected predefined set instance designing new car model normally case design included present previous car designs words always possible process designing new car one components predefined set nevertheless large number realworld applications possible assume target designed terms predefined design elements design process consists design elements way satisfies design requirements constraints approximates typically optimization criterion class design tasks takes configuration design many cases typically problem hand exhibit complex spatial requirements possible solutions common solution possible configuration design problem even modelling target set parameters characterizing design problem solving process values parameters given design requirements constraints optimization criterion assumption true particular task parametric design task application provides wellknown example parametric design task aim paper understand involved parametric design problem solving order achieve goal paper identify detail conceptual elements defining parametric design task specification illustrate elements interpreted design process iii produce generic model parametric design problem solving knowledge level generalizes existing methods parametric design number problem solving methods terms question
distributed reinforcement learning scheme network routing paper describe algorithm routing reinforcement learning method embedded node network local information used node keep accurate statistics routing policies lead minimal routing times simple experiments involving node network learning approach superior routing based shortest paths
biological design modular artificial neural networks thesis
efficient instruction scheduling using finite state automata modern employ sophisticated instruction scheduling techniques number cycles taken execute instruction stream addition instruction scheduler must also ensure hardware resources cycle processor implementation multiple pipelines complex resource restrictions easy task complexity involved reasoning resource hazards one primary factors instruction scheduler performing many transformations example ability code motion instruction already block powerful transformation performed efficiently extend technique detecting resource hazards based finite state automata support efficient implementation transformations essential instruction scheduling beyond basic blocks although similar code transformations supported schemes tables scheme superior terms space time global instruction scheduler used techniques implemented compiler
proposal variable selection model propose new method variable selection estimation proportional hazards model proposal minimizes log partial likelihood subject sum absolute values parameters bounded constant nature constraint tends produce coefficients exactly zero hence gives models method variation proposal designed linear regression context simulations indicate accurate stepwise selection setting
report abstract many current artificial neural network systems serious limitations concerning flexibility scaling reliability order way removing suggest neural network architecture architecture modular structure important element elements called modules perform current level development scope expertise within module system integrates work together handle mapping tasks network complexity limitations way problem decomposition paradigm static dynamic whole system effectively generation interpretation confidence measures every development system two problem domains used test demonstrate various aspects architecture reliability quality measures defined systems answer part time system achieves better quality values single networks larger size handwritten digit problem second third best answers accepted system error test set better best single net also shown system learn handle patterns parity problem demonstrated complexity problems may decomposed automatically system solving networks size smaller single net required even system find solution parity problem networks small size used reliability remains around architecture gives power flexibility higher levels large hybrid system single net system useful information feedback loops reliability answers may less reliable important answers providing weighted alternatives possible generalizations architecture gives best possible service larger system will form part
incorrect answers neural net classifier report number
learning behaviors control simulated car rapid response complexity many environments make difficult tune coordinate reactive behaviors consistency hypothesize complex behaviors decomposed separate behaviors separate networks higher level controller explore issues implemented neural network architecture reactive component two layer control system simulated car varying architecture tested whether separate behaviors leads superior overall performance learning convergence based results modified architecture produce car competitive available solutions
genetic prototype learner supervised classification problems received considerable attention machine learning community propose novel genetic algorithm based prototype learning system please class problems given set prototypes possible classes class input instance determined prototype nearest instance assume ordinal attributes prototypes represented sets pairs genetic algorithm used evolve number prototypes per class positions input space determined corresponding pairs comparisons c45 set artificial problems controlled complexity demonstrate effectiveness system
comparing methods refining paper compares two methods refining uncertain knowledge bases using propositional rules first method implemented system employs neuralnetwork training refine existing rules uses symbolic technique add new rules second method based one used system initially adds complete set potential new rules low allows neuralnetwork training filter rules experimental results indicate former method results significantly faster training produces much simpler refined rule bases slightly greater accuracy
constructing conjunctive tests decision trees paper discusses approach constructing new attributes based decision trees production rules improve concepts learned form decision trees simplifying improving predictive accuracy addition approach distinguish relevant primitive attributes irrelevant primitive attributes
models perceptual learning performance human subjects wide variety early visual processing tasks improves practice networks mathematically framework understanding improvement performance perceptual learning class tasks known visual present article two issues raised recent psychophysical computational findings reported first develop biologically plausible extension model takes account basic features functional architecture early vision second explore various learning modes within framework focus two unsupervised learning rules may involved learning finally report results psychophysical experiments consistent hypothesis may involved perceptual learning
mining model simplicity case study diagnosis proceedings international conference knowledge discovery data mining version paper published association artificial intelligence association artificial intelligence abstract describe results performing data mining challenging medical diagnosis domain domain known difficult little predictive accuracy human machine moreover many researchers argue one simplest approaches naive bayesian classifier optimal comparing performance naive bayesian classifier general bayesian network classifier selective bayesian classifiers just total attributes show simplest models perform least complex models argue simple models like selective naive bayesian classifier will perform complicated models similarly complex domains relatively small data sets thereby question extra necessary induce complex models
spatial representation early vision
studies quality monitor time series system build contributions report describes statistical research development work quality monitor data sets system project covers statistical analysis exploration modelling data several quality monitors primary goals understanding patterns variability time monitor area specific quality monitor measures understanding patterns dependencies sets monitors present discussion basic data structure preliminary data exploration three monitors followed developments several classes formal models identify classes hierarchical random effects time series models relevance modelling single multiple monitor time series basic model features results analyses three monitor data sets single multiple monitor present variety summary inferences graphical discussion includes summary conclusions related two key goals discussions questions comparisons across potential statistical
least absolute shrinkage equivalent quadratic adaptive ridge special form ridge regression balancing quadratic parameter model paper shows equivalence adaptive ridge least absolute shrinkage selection operator equivalence states procedures produce estimate least absolute shrinkage thus viewed particular quadratic observation derive algorithm compute solution finally present series applications type algorithm problems kernel regression additive modeling neural net training
regression noise gaussian process treatment technical report available appear advances neural information processing systems eds jordan abstract gaussian processes provide natural nonparametric prior distributions regression functions paper consider regression problems noise output variance noise depends inputs assume noise smooth function inputs natural model noise variance using second gaussian process addition gaussian process noisefree output value show prior uncertainty parameters controlling processes handled posterior distribution noise rate sampled using markov chain monte carlo methods results synthetic data set give posterior noise variance true variance
importance sampling technical report department statistics university toronto abstract simulated annealing moving tractable distribution distribution interest via sequence intermediate distributions traditionally used method handling isolated modes markov chain samplers shown one use markov chain transitions annealing sequence define importance sampler markov chain aspect allows method perform even highdimensional problems finding good importance sampling distributions otherwise difficult use importance weights estimates found converge correct values number annealing runs increases annealed importance sampling procedure second transitions seen generalization variant sequential importance sampling also related integration methods estimating constants annealed importance sampling attractive isolated modes present estimates constants required may also generally useful since independent sampling allows one problems convergence markov chain samplers
introduction radial basis function networks document introduction radial basis function rbf networks type artificial neural network application problems supervised learning regression classification time series prediction available either
time multilayer networks method tries answer questions like kind input will give desired output possible get desired output special inputoutput constraints will describe two methods inverting connectionist network firstly extend via backpropagation williams recurrent jordan discrete versions continuous networks result input vector corresponding output vector equal target vector except small knowledge attractors may help understand function generalization connectionist systems kind secondly introduce new method proving input combination special constraints input space method works iterative activation values might helpful way properties trained network conclude simulation results three different tasks signal handwritten digit recognition
learning paper study learning algorithms environments changing time unlike previous work interested case changes might rapid direction relatively constant model type change assuming target distribution changing continuously constant rate one distribution another show case use simple weighting scheme estimate error hypothesis using estimate minimize error prediction
multiple regression multiple definition regularisation techniques performances multiple regression crucial plus general selection variables conditions simulations techniques techniques regression selection regression generalisation competition
learning control real robot distributed classifier systems
interaction crossover operator restriction tree depth crossover crossover operator common implementations genetic programming another usually factor form restriction size trees population paper interaction crossover operator restriction tree depth demonstrated max problem involves largest possible value given function sets
expected bound model online reinforcement learning propose model efficient online reinforcement learning based expected bound framework introduced measure performance use expected difference total reward received learning agent received agent start call expected difference agent require levels reasonably fast rate learning show model equivalent pac model offline reinforcement learning introduced particular show offline pac reinforcement learning algorithm transformed efficient online algorithm simple practical way consequence result pac algorithm general finite statespace reinforcement learning problem described transformed polynomial online guaranteed performances
robust single neurons
map protein space automatic hierarchical classification protein sequences investigate space protein sequences combine standard measures similarity associate sequence exhaustive list sequences lists induce weighted directed graph whose sequences weight edge two sequences represents degree similarity graph much fundamental properties sequence space look clusters related proteins graph clusters correspond strongly connected sets two main ideas work interesting among proteins applied order proteins clustering together analysis classification based significant similarities many classes subsequently classes include less significant similarities merging performed via novel two phase algorithm first algorithm identifies groups possibly related clusters based strong connectivity using local global test applied identify strong relationships within groups clusters classification refined process takes place varying statistical significance step algorithm applied classes previous classification obtain next one threshold consequently hierarchical organization proteins obtained resulting classification splits space protein sequences defined groups proteins results show automatically induced sets proteins closely correlated natural biological families families hierarchical organization reveals make known families proteins many interesting relations protein families hierarchical organization proposed may considered first map space protein sequences interactive web site including results analysis constructed now
multistrategy learning theory revision paper presents system learns updates diagnostic knowledge base using domain knowledge set examples knowledge consists causal model domain relationships among basic phenomena body theory describing links abstract concepts possible world knowledge used causal model used examples used problems theory handled allowing system make assumptions reasoning way robust knowledge learned limited complexity limited number examples system works first order logic environment applied real domain
metropolis gibbs states optimal scaling paper investigates behaviour random walk metropolis algorithm high dimensional problems case components target density spatially homogeneous gibbs distribution finite range performance algorithm strongly presence absence phase transition gibbs distribution convergence time approximately linear dimension problems phase transition present related optimal way scale variance proposal distribution order speed convergence algorithm turns involve scaling variance proposal dimension least phase transition free case moreover actual optimal scaling terms overall acceptance rate algorithm value value predicted studies simpler classes target density results proved framework weak convergence result shows algorithm actually like infinite dimensional diffusion process high dimensions introduction discussion results
uniform error bounds paper develops probabilistic bounds outofsample error rates several classifiers using single set data bounds based probabilities outofsample data outofsample data sets bounds apply outofsample data drawn distribution bounds stronger bounds require computation
polynomial time incremental algorithm regular grammar inference
learning dfa simple examples present framework learning dfa simple examples show efficient pac learning dfa possible class distributions restricted simple distributions teacher might choose examples based knowledge target concept answers open research question paper examples drawn uniform distribution known simple distribution approach uses algorithm learning dfa labeled examples particular describe efficient learning algorithm exact learning target dfa high probability bound number states target dfa known advance known show algorithm used efficient pac learning
belief maintenance probabilistic logic
forecasting patients using belief networks
exact bound convergence metropolis chains note present calculation gives exact bound convergence independent metropolis chains finite state space metropolis chain convergence rate markov chain monte carlo
probabilistic reasoning
simple synchrony networks learning parse natural language temporal synchrony variable binding simple synchrony network new connectionist architecture incorporating insights temporal synchrony variable binding simple recurrent networks use means output representations structures learn structures required paper describes associated training algorithm demonstrates generalisation abilities results training
evolutionary method find good architectures artificial neural networks paper deals combination evolutionary algorithms artificial neural networks ann new method presented find good architectures artificial neural networks method based cellular encoding representation scheme genetic programming koza first will shown modified cellular encoding technique able find good architectures even networks help new method secondly possible build architectures modular structures information architectures obtained statistically analyzing data simulation results two real world problems given
learning easier tasks work necessary order determine precisely relationship obtain stronger correlation relationship performance included studying variance members population observing rate convergence respect population evolved unfortunately yet able obtain significant correlation future work plan track genetic diversity considered variance far populations order light underlying mechanism one factor made analysis difficult far use genetic programming space large many redundant solutions neighborhood structure less easily standard genetic algorithm since every reason believe underlying mechanism incremental evolution largely independent genetic programming currently investigating incremental evolution mechanism using genetic algorithms enable better understanding mechanism will scale research effort analyze incremental evolution one transition test cases will involve many open issues regarding optimization transition schedule test cases performed following experiment let fitness value genetic program according evaluation function best population time highest fitness according words best population evolved usual manner using evaluation function generations however generation also evaluated current population using evaluation function value words evolved population using evaluation function every generation also computed fitness best individual population according value using random control parameters evolved population generations using evaluation function note generation identical values compared order better exploit notion domain difficulty
compiler high performance genetic programming genetic programming computationally expensive applications majority time evaluating candidate solutions desirable make individual evaluation efficient possible describe compiler machine code resulting significant speedup individual evaluations standard systems based performance results symbolic regression show execution compiler system comparable alternative systems also demonstrate utility compilation realworld problem image compression somewhat surprising result test domains compilation
max problem genetic programming interaction crossover operator crossover operator common implementations genetic programming another usually factor form restriction size trees population paper interaction crossover operator restriction tree depth demonstrated max problem involves largest possible value given function sets characteristics crossover normal use discussed discovery movement takes place mostly near nodes nodes near root diversity quickly zero near root node tree population create trees via crossover operator mutation operator common discovery trees
functional representation design rationale design rationale design activity alternatives available choices made reasons explanations proposed design intended work describe representation called functional representation used represent devices functions arise functions components propose provide basis causal aspects design rationale briefly discuss use number tasks expect design rationale useful generation diagnostic knowledge design redesign
human face detection visual scenes present neural face detection system connected neural network examines small image whether window contains face system multiple networks improve performance single network use bootstrap algorithm training networks adds false training set training difficult task selecting training examples must chosen entire space images comparisons face detection systems presented system better performance terms detection rates work partially supported grant siemens research department research office grant number office naval research grant number work supported national science foundation graduate fellowship currently supported graduate student fellowship national space space center views conclusions contained document authors interpreted necessarily representing policies either expressed implied
